{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\sysme hue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement textract==1.6.2 (from versions: 0.1.0, 0.2.0, 0.3.0, 0.4.0, 0.5.0, 0.5.1, 1.0.0, 1.1.0, 1.2.0, 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.6.1, 1.6.3, 1.6.4, 1.6.5)\n",
      "ERROR: No matching distribution found for textract==1.6.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q textract==1.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar Error Corection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "# model_name = 'deep-learning-analytics/GrammarCorrector'\n",
    "model_name = 't5_gec_model_02' # model path\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def correct_grammar(input_text,num_return_sequences):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['summary: I enjoy writing articles on AI and I also enjoy writing articles on AI.',\n",
       " 'Summary: I enjoy writing articles on AI and I also enjoy writing articles on AI.',\n",
       " 'summary: I enjoy writing articles on AI and I also enjoyed writing articles on AI.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"summary: I am enjoys, writtings Articles ons AI and I also enjoyed write articling on AI.\"\n",
    "num_return_sequences = 3 # Số lượng câu generate\n",
    "corrected_text = correct_grammar(input_text, num_return_sequences)\n",
    "corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['summary: I enjoy writing articles on AI and I also enjoy writing articles on AI.',\n",
       " 'Summary: I enjoy writing articles on AI and I also enjoy writing articles on AI.',\n",
       " 'summary: I enjoy writing articles on AI and I also enjoyed writing articles on AI.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation and Fact - Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import json\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13876\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import textract\n",
    "\n",
    "# Đọc nội dung của tài liệu PDF\n",
    "text = textract.process('pdf/Report.pdf', encoding='utf-8')\n",
    "\n",
    "# Sử dụng biểu thức chính quy để cắt thành các đoạn văn\n",
    "all_paragraphs = re.split(r'\\s{2,}', text.decode('utf-8'))\n",
    "num_paragraph= len(text)\n",
    "print(num_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_para = []\n",
    "list_para = [para for para in all_paragraphs if len(para.split()) >= 20] # list of paragraphs which have more than 20 words\n",
    "len(list_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize paragraph using t5 model\n",
    "\n",
    "summaries = []\n",
    "for i,paragraph in enumerate(list_para):\n",
    "    input_text = \"summarize: \" + paragraph\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(input_ids, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    summaries.append(summary)\n",
    "    \n",
    "    # if i==3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abstract. This paper aims to develop a system that will help in recommendation of courses for an upcoming semester based on the performance of previous semesters.',\n",
       " \"It has always been a tough choice for the students to choose the courses in different semesters in which there is possibility to score good grades apart from the interest in the course. IIIT-Delhi offers variety of courses with mandatory courses in first 4 semesters (with exception of 2 to 3 electives) and all elective courses from fifth semester onwards. Hence, choosing the courses based on the verbal recommendation from the seniors, instructors and fellowmates becomes a hectic task. For easing this process of course recommendation for an upcoming semester, we have developed a system which deploys simple yet powerful recommendation techniques such as auto-encoders, hybrid matrix factorization and similarity based approaches. It is a GUI based system which takes an input of student's ID (which is stored in the backend database) and semester for which the student wants to get the recommendation. Then, it outputs the top 5 courses that the student can choose based on his/her performance in previous semesters. Also, a confidence score is provided for each recommended course.\",\n",
       " 'The dataset has been acquired from the official IIIT-Delhi academics department for the students of 7 Computer Science passout batches. The dataset consists 739 students and 306 subjects with mapping of each student to the grades for each course the student has taken throughout the duration of their degree. The courses are spread over 8 semesters (also including the courses offered to student with extended semester). The fields in the dataset include Serial Number(SN), Roll Number(anonymized) of the student, Batch/Term Code in which the course was offered, Course Code, Course name, Credit offered for the course, Grade obtained by particular student in the course, SPI (Semester Percentile Index) - Average GPA(Grade Point Average) for the current semester and CPI (Cummulative Percentile Index) - Overall GPA.',\n",
       " 'The data is processed from CSV format to JSON format. Each student is mapped to all the courses with the grade and semester in which the course was offered. If the student has taken the course, then the course key(under the particular student key) will have the corresponding grade obtained in the course and the semester in which the course was offered. Each user and course is mapped to a unique integer ID. Incomplete courses, courses for which leave application was given by the student and courses with grade W(weak) are given grade score 0. Online courses with grade S are given grade score as 10 while with X are given grade score 0. For creation of train and test matrices we included only courses that were offered in first 8 semesters as any semester beyond that signifies the presence of backlog or extended semester. For such cases we took the maximum grade score provided to the student in a particular course including all extended semesters and backlogs. Every course has a list of semesters in which it was offered as the same course can be floated in more than 1 semester. Various split ratios were considered and 5 fold cross validation was done for creating the matrices. Test matrix only considered the student-course pair for courses in 5th, 6th 7th and 8th semesters.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_para[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge into result \n",
    "\n",
    "result = [{\"paragraph\": list_para[i], \"summary\": summaries[i]} for i in range(len(summaries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paragraph': 'Abstract. This paper aims to develop a system that will help in recommendation of courses for an upcoming semester based on the performance of previous semesters.',\n",
       "  'summary': 'this paper aims to develop a system that will help in recommendation of courses for an upcoming semester based on the performance of previous semesters. this paper aims to develop a system that will help in recommendation of courses based on the performance of previous semesters.'},\n",
       " {'paragraph': \"It has always been a tough choice for the students to choose the courses in different semesters in which there is possibility to score good grades apart from the interest in the course. IIIT-Delhi offers variety of courses with mandatory courses in first 4 semesters (with exception of 2 to 3 electives) and all elective courses from fifth semester onwards. Hence, choosing the courses based on the verbal recommendation from the seniors, instructors and fellowmates becomes a hectic task. For easing this process of course recommendation for an upcoming semester, we have developed a system which deploys simple yet powerful recommendation techniques such as auto-encoders, hybrid matrix factorization and similarity based approaches. It is a GUI based system which takes an input of student's ID (which is stored in the backend database) and semester for which the student wants to get the recommendation. Then, it outputs the top 5 courses that the student can choose based on his/her performance in previous semesters. Also, a confidence score is provided for each recommended course.\",\n",
       "  'summary': 'IIIT-Delhi offers variety of courses with mandatory courses in first 4 semesters. all elective courses from fifth semester onwards become a hectic task. it deploys simple yet powerful recommendation techniques such as auto-encoders.'},\n",
       " {'paragraph': 'The dataset has been acquired from the official IIIT-Delhi academics department for the students of 7 Computer Science passout batches. The dataset consists 739 students and 306 subjects with mapping of each student to the grades for each course the student has taken throughout the duration of their degree. The courses are spread over 8 semesters (also including the courses offered to student with extended semester). The fields in the dataset include Serial Number(SN), Roll Number(anonymized) of the student, Batch/Term Code in which the course was offered, Course Code, Course name, Credit offered for the course, Grade obtained by particular student in the course, SPI (Semester Percentile Index) - Average GPA(Grade Point Average) for the current semester and CPI (Cummulative Percentile Index) - Overall GPA.',\n",
       "  'summary': 'the dataset has been acquired from the official IIIT-Delhi academics department for the students of 7 Computer Science passout batches. the dataset consists 739 students and 306 subjects with mapping of each student to the grades for each course the student has taken throughout the duration of their degree.'},\n",
       " {'paragraph': 'The data is processed from CSV format to JSON format. Each student is mapped to all the courses with the grade and semester in which the course was offered. If the student has taken the course, then the course key(under the particular student key) will have the corresponding grade obtained in the course and the semester in which the course was offered. Each user and course is mapped to a unique integer ID. Incomplete courses, courses for which leave application was given by the student and courses with grade W(weak) are given grade score 0. Online courses with grade S are given grade score as 10 while with X are given grade score 0. For creation of train and test matrices we included only courses that were offered in first 8 semesters as any semester beyond that signifies the presence of backlog or extended semester. For such cases we took the maximum grade score provided to the student in a particular course including all extended semesters and backlogs. Every course has a list of semesters in which it was offered as the same course can be floated in more than 1 semester. Various split ratios were considered and 5 fold cross validation was done for creating the matrices. Test matrix only considered the student-course pair for courses in 5th, 6th 7th and 8th semesters.',\n",
       "  'summary': 'the data is processed from CSV format to JSON format. each student is mapped to all the courses with the grade and semester in which the course was offered. if the student has taken the course, then the course key will have the corresponding grade obtained in the course and the semester in which the course was offered.'},\n",
       " {'paragraph': 'Fig. 1: Top 5 elective courses. (CSE535 - Mobile Computing, CSE506 - Data Mining, CSE345 - Foundation of Security, CSE300 - Software Engineering, FIN401 - Foundations of Finance )',\n",
       "  'summary': 'CSE535 - Mobile Computing, CSE506 - Data Mining, CSE345 - Foundation of security, CSE300 - Software Engineering, FIN401 - Foundations of finance. FIN401 - foundations of finance.'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sysme Hue\\Desktop\\LMS\\t5-grammar\n"
     ]
    }
   ],
   "source": [
    "# Clone this responsitory for questions generator\n",
    "\n",
    "# !git clone https://github.com/amontgomerie/question_generator!git clone https://github.com/amontgomerie/question_generator\n",
    "# !pip install -r question_generator/requirements.txt -qq\n",
    "# !python run_qg.py --text_file question_generator/articles/twitter_hack.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.1.0) was trained with spaCy v3.1.0 and may not be 100% compatible with the current version (3.7.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import file from another folder\n",
    "\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, 'question_generator')\n",
    "\n",
    "from questiongenerator import QuestionGenerator\n",
    "qg = QuestionGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating questions...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.1.0) was trained with spaCy v3.1.0 and may not be 100% compatible with the current version (3.7.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating QA pairs...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = result[2][\"summary\"]\n",
    "q  = qg.generate(text, num_questions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'how many students have taken the course?',\n",
       "  'answer': 'the dataset consists 739 students and 306 subjects with mapping of each student to the grades for each course the student has taken throughout the duration of their degree.'},\n",
       " {'question': 'how many students have taken the iitt?',\n",
       "  'answer': 'the dataset has been acquired from the official IIIT-Delhi academics department for the students of 7 Computer Science passout batches.'},\n",
       " {'question': 'how many subjects do students have taken?',\n",
       "  'answer': [{'answer': 'dataset', 'correct': False},\n",
       "   {'answer': '7 Computer Science', 'correct': False},\n",
       "   {'answer': '306', 'correct': True},\n",
       "   {'answer': '739', 'correct': False}]}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset has been acquired from the official IIIT-Delhi academics department for the students of 7 Computer Science passout batches. the dataset consists 739 students and 306 subjects with mapping of each student to the grades for each course the student has taken throughout the duration of their degree.\n",
      "how many students have taken the course?\n",
      "how many students have taken the iitt?\n",
      "how many subjects do students have taken?\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "for i in range(len(q)): print(q[i][\"question\"]) #  7 Computer Science passout batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact -check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng mô hình\n",
    "\n",
    "class BERTClassificationModel(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels):\n",
    "        super(BERTClassificationModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size * 2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = torch.cat((outputs.last_hidden_state[:, 0, :], outputs.last_hidden_state[:, -1, :]), dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might error but thing fine\n",
    "\n",
    "model_path = \"model/bert_classification_model.pth\"\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Tạo mô hình mớia\n",
    "loaded_model = BERTClassificationModel('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Nạp trạng thái từ tệp đã lưu\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải lại tokenizer từ đường dẫn đã lưu\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(\"model/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise_text : The likelihood is 100% \n",
      " hypothesis_text : Maybe the probability is 100%\n",
      "Predicted Label: neutral\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 128\n",
    "def predict_premise_hypothesis(premise_text, hypothesis_text, model, tokenizer):\n",
    "    # Chuẩn bị dữ liệu đầu vào cho mô hình\n",
    "    inputs = tokenizer(premise_text, hypothesis_text, padding=True, truncation=True, max_length=max_seq_length, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Dự đoán\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predicted_label = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "pre = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "# Sử dụng hàm predict_premise_hypothesis để dự đoán\n",
    "premise_text = \"The likelihood is 100%\"\n",
    "hypothesis_text = \"Maybe the probability is 100%\"\n",
    "predicted_label = predict_premise_hypothesis(premise_text, hypothesis_text, loaded_model, loaded_tokenizer)\n",
    "\n",
    "print(\"premise_text :\", premise_text, \"\\n\", \"hypothesis_text :\", hypothesis_text)\n",
    "print(\"Predicted Label:\", pre[predicted_label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise_text : The dataset had acquired for the student of 7 computer science \n",
      " hypothesis_text : the dataset has been acquired from the official IIIT-Delhi academics department for the students of 7 Computer Science passout batches. the dataset consists 739 students and 306 subjects with mapping of each student to the grades for each course the student has taken throughout the duration of their degree.\n",
      "Predicted Label: entailment\n"
     ]
    }
   ],
   "source": [
    "premise_text = \"The dataset had acquired for the student of 7 computer science\"\n",
    "hypothesis_text = result[2][\"summary\"]\n",
    "\n",
    "predicted_label = predict_premise_hypothesis(premise_text, hypothesis_text, loaded_model, loaded_tokenizer)\n",
    "print(\"premise_text :\", premise_text, \"\\n\", \"hypothesis_text :\", hypothesis_text)\n",
    "print(\"Predicted Label:\", pre[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.8.2\n",
    "# !python setup.py install\n",
    "# !pip install --upgrade --force-reinstall scikit-learn numpy\n",
    "# !pip install --upgrade --force-reinstall numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of content\n",
    "----\n",
    "## English Learning Support\n",
    "- [1. CEFR English Level Predictor](#1)\n",
    "  - [1.1. RoBERTa for CEFR English Level Predictor](#1.1)\n",
    "  - [1.2. XGboost for CEFR English Level Predictor](#1.2)\n",
    "- [2. Grammar Error Correction](#2)\n",
    "- [3. Tense Predictions](#3)\n",
    "- [4. Spelling Check](#4)\n",
    "- [5. Identify Error Types](#5)\n",
    "- [6. Structure Prediction Constituency Parser](#6)\n",
    "\n",
    "## Question Generation and Fact-Check\n",
    "- [1. T5 for Summarize Task](#7)\n",
    "- [2. Question Generation](#8)\n",
    "- [3. Fact-Check](#9)\n",
    "\n",
    "## Recommendation System\n",
    "- [1. Association Rule - Apriori](#10)\n",
    "- [2. Similar Courses](#11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English learning support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"1\"></a>\n",
    "\n",
    "### 1. CEFR English Level Predictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "\n",
    "#### 1.1. RoBERTa for CEFR English Level Predictor\n",
    "\n",
    "\n",
    "Get [CEFR_model](https://drive.google.com/drive/folders/1VkdH3IPyoA8KAmpt9YbVgP399KPQvZFV?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "\n",
    "ro_model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"model/cefr/cefr999_model\",\n",
    "    num_labels=6\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ro_model.to(device)\n",
    "ro_tokenizer = AutoTokenizer.from_pretrained(\"model/cefr/cefr999_token\")\n",
    "\n",
    "def predict_english_level(text):\n",
    "    # Encode the text using the tokenizer\n",
    "    inputs = ro_tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].squeeze()\n",
    "    attention_mask = inputs[\"attention_mask\"].squeeze()\n",
    "\n",
    "    # Make a prediction\n",
    "    with torch.no_grad():\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        output = ro_model(input_ids=input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))\n",
    "        predicted_class = output.logits.argmax().item()\n",
    "\n",
    "    # Map the predicted class back to the English level\n",
    "    labels = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "    predicted_level = labels[predicted_class]\n",
    "\n",
    "    return predicted_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted English Level: B1\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "In contemporary era thattourism had been show great  impact on society . I agree this statement and my view here\n",
    "\n",
    "Firsty , tourism has a great impact on everycountry economy beacuse where foriegners have come to see their tourist place they spend money to buy many things . Moreover , some tourism place like a jaipur there is many historical builiding and every year lot of people visit these place to see there \n",
    "\n",
    "Building and when they saw these building buy some tickets . These money spend on building by government .\n",
    "\n",
    "Moreover if they another countries individuals come their country than they expand their culture one place to another place and it is big way exploure our culture .\n",
    "\n",
    "\"\"\"\n",
    "# print(len(text))\n",
    "predicted_level = predict_english_level(text)\n",
    "print(\"Predicted English Level:\", predicted_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "\n",
    "#### 1.2. XGboost for CEFR English Level Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [16:14:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07f6e447eee219473-1\\xgboost\\xgboost-ci-windows\\src\\common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "import pandas as pd\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import joblib\n",
    "\n",
    "model_filename = \"model/cefr/xgboost100.pkl\"\n",
    "\n",
    "xgb_model = joblib.load(model_filename)\n",
    "\n",
    "def avg_words_per_sentence(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_count = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    if len(sentences) == 0:\n",
    "        return 0\n",
    "    return word_count / len(sentences)\n",
    "\n",
    "# Hàm để dự đoán trình độ tiếng Anh dựa trên văn bản đầu vào\n",
    "def predict_english_level_XG(text, xgb_model):\n",
    "\n",
    "    smog_index = textstat.smog_index(text)\n",
    "    automated_readability_index = textstat.automated_readability_index(text)\n",
    "    dale_chall_readability_score = textstat.dale_chall_readability_score(text)\n",
    "    difficult_words = textstat.difficult_words(text)\n",
    "    linsear_write_formula = textstat.linsear_write_formula(text)\n",
    "    gunning_fog = textstat.gunning_fog(text)\n",
    "    szigriszt_pazos = textstat.szigriszt_pazos(text)\n",
    "    gutierrez_polini = textstat.gutierrez_polini(text)\n",
    "    crawford = textstat.crawford(text)\n",
    "    osman = textstat.osman(text)\n",
    "    avg = avg_words_per_sentence(text)\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        \"smog_index\": [smog_index],\n",
    "        \"automated_readability_index\": [automated_readability_index],\n",
    "        \"dale_chall_readability_score\": [dale_chall_readability_score],\n",
    "        \"difficult_words\": [difficult_words],\n",
    "        \"linsear_write_formula\": [linsear_write_formula],\n",
    "        \"gunning_fog\": [gunning_fog],\n",
    "        \"szigriszt_pazos\": [szigriszt_pazos],\n",
    "        \"gutierrez_polini\": [gutierrez_polini],\n",
    "        \"crawford\": [crawford],\n",
    "        \"osman\": [osman],\n",
    "        \"avg_words\": [avg]\n",
    "    })\n",
    "\n",
    "    predicted_level = xgb_model.predict(data)\n",
    "    predicted_probabilities = xgb_model.predict_proba(data)\n",
    "    # Chuyển kết quả từ số sang trình độ tiếng Anh tương ứng\n",
    "    level_to_index = {0: \"A1\", 1: \"A2\", 2: \"B1\", 3: \"B2\", 4: \"C1\", 5: \"C2\"}\n",
    "    predicted_level_text = level_to_index[predicted_level[0]]\n",
    "\n",
    "    return predicted_level_text, predicted_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted English Level - Neural: B1\n",
      "Predicted English Level - XG: B2\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "In contemporary era thattourism had been show great  impact on society . I agree this statement and my view here\n",
    "\n",
    "Firsty , tourism has great impact on everycountry economy beacuse where foriegners are come to see their tourist place they spend money to buy many things . Moreover , some tourism place like a jaipur there is many historical builiding and every year lot of people visit these place to see there \n",
    "\n",
    "Building and when they saw these building buy some tickets . These money spend on building by government .\n",
    "\n",
    "Moreover if they another countries individuals come their country than they expand their culture one place to another place and it is big way exploure our culture .\n",
    "\n",
    "\"\"\"\n",
    "predicted_level = predict_english_level(text)\n",
    "print(\"Predicted English Level - Neural:\", predicted_level)\n",
    "predicted_level,prob = predict_english_level_XG(text, xgb_model)\n",
    "print(\"Predicted English Level - XG:\", predicted_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"2\"></a>\n",
    "\n",
    "### 2. Grammar Error Corection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "# # model_name = 'deep-learning-analytics/GrammarCorrector'\n",
    "model_name = 'model/gec_model/gec_03'\n",
    "# model_name = 't5_gec_model_02' # model path\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "g_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "g_model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(input_text,num_return_sequences):\n",
    "  batch = g_tokenizer([input_text],truncation=True,padding='max_length',max_length=128, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = g_model.generate(**batch,max_length=128,num_beams=3, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = g_tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Dear Sir/Madam,\n",
    "\n",
    "This letter aims to share my opinions about the booked that influenced me the most. The name of the book is \"You Can Win\" and I found this book very inspiring and knowledgeable. Basically, the book includes real-life examples and motivational language to give a ray of hope to a disappointed person.\n",
    "\n",
    "I started reading the book when I faced a failure in life and it seemed like the end of life. I read the book recommended to me by one of my friends and when I started reading the book, I got my confidence, motivation and self-esteem back and understood the real meaning of life.\n",
    "\n",
    "I are highly recommend this book to everyone to achieve better in life and gain an in-depth understanding of life. Moreover, the book is written in such a good way and language which makes it interesting and inspiring when one feels low and demotivated in life. Though I am not a bibliophile, I found this book really very interesting and motivational.\n",
    "\n",
    "Yours Faithfully,\n",
    "\n",
    "ABC\n",
    "\"\"\"\n",
    "num_return_sequences = 3\n",
    "corrected_texts = correct_grammar(input_text, num_return_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "I agree this statement and my view here.\n",
    "\"\"\"\n",
    "num_return_sequences = 3\n",
    "corrected_texts = correct_grammar(input_text, num_return_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I agree with this statement and my view here.',\n",
       " 'I agree with this statement and my views here.',\n",
       " 'I agree with this statement and my opinion here.']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Text 1:\n",
      "\n",
      "In the modern era tourism has had a great impact on society.\n",
      "I agree with this statement and my views here.\n",
      "First of all, tourism has great impact on every country economy beacuse where tourists come to see their tourist place they spend money to buy many things.\n",
      "Moreover, some tourism places like Jaipur there are many historical buildings and every year lot of people visit these places to see there.\n",
      "Building and when they saw these building they bought some tickets.\n",
      "This money is spent on building by government.\n",
      "Moreover if individuals come from another country then they expand their culture one place to another place and it is a big way to expand our culture.\n",
      "\n",
      "Corrected Text 2:\n",
      "In the modern era, tourism has had a great impact on society.\n",
      "I agree with this statement and my view here.\n",
      "First of all, tourism has great impact on every country economy beacuse where foreigners come to see their tourist place they spend money to buy many things.\n",
      "Moreover, some tourism places like jaipur there are many historical buildings and every year lot of people visit these places to see there.\n",
      "Building and when they saw these buildings they bought some tickets.\n",
      "This money is spent on building by the government.\n",
      "\n",
      "Moreover, if individuals come from another country then they expand their culture one place to another place and it is a big way to expand our culture.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def correct_and_merge(input_text, num_return_sequences):\n",
    "    sentences = re.split(r'[.!?\\n]', input_text)\n",
    "    corrected_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > 3:\n",
    "            corrected_versions = correct_grammar(sentence, num_return_sequences=num_return_sequences)\n",
    "            corrected_sentences.extend(corrected_versions)\n",
    "        else:\n",
    "            corrected_sentences.append(sentence)\n",
    "    corrected_texts = []\n",
    "    for i in range(num_return_sequences):\n",
    "\n",
    "        corrected_texts.append('\\n'.join(corrected_sentences[i::num_return_sequences]))\n",
    "\n",
    "    return corrected_texts\n",
    "\n",
    "input_text = \"\"\"\n",
    "In contemporary era thattourism had been show great  impact on society . I agree this statement and my view here\n",
    "Firsty , tourism has great impact on everycountry economy beacuse where foriegners are come to see their tourist place they spend money to buy many things . Moreover , some tourism place like a jaipur there is many historical builiding and every year lot of people visit these place to see there \n",
    "Building and when they saw these building buy some tickets . These money spend on building by government .\n",
    "Moreover if they another countries individuals come their country than they expand their culture one place to another place and it is big way exploure our culture .\n",
    "\"\"\"\n",
    "num_return_sequences = 2\n",
    "corrected_texts = correct_and_merge(input_text, num_return_sequences)\n",
    "for i, corrected_text in enumerate(corrected_texts):\n",
    "    print(f\"Corrected Text {i + 1}:\\n{corrected_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In contemporary era thattourism had been show great  impact on society . I agree this statement and my view here\n",
      "Firsty , tourism has great impact on everycountry economy beacuse where foriegners are come to see their tourist place they spend money to buy many things . Moreover , some tourism place like a jaipur there is many historical builiding and every year lot of people visit these place to see there \n",
      "Building and when they saw these building buy some tickets . These money spend on building by government .\n",
      "Moreover if they another countries individuals come their country than they expand their culture one place to another place and it is big way exploure our culture .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nIn the modern era tourism has had a great impact on society.\\nI agree with this statement and my views here.\\nFirst of all, tourism has great impact on every country economy beacuse where tourists come to see their tourist place they spend money to buy many things.\\nMoreover, some tourism places like Jaipur there are many historical buildings and every year lot of people visit these places to see there.\\nBuilding and when they saw these building they bought some tickets.\\nThis money is spent on building by government.\\nMoreover if individuals come from another country then they expand their culture one place to another place and it is a big way to expand our culture.\\n',\n",
       " 'In the modern era, tourism has had a great impact on society.\\nI agree with this statement and my view here.\\nFirst of all, tourism has great impact on every country economy beacuse where foreigners come to see their tourist place they spend money to buy many things.\\nMoreover, some tourism places like jaipur there are many historical buildings and every year lot of people visit these places to see there.\\nBuilding and when they saw these buildings they bought some tickets.\\nThis money is spent on building by the government.\\n\\nMoreover, if individuals come from another country then they expand their culture one place to another place and it is a big way to expand our culture.\\n']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "In contemporary era thattourism had been show great  impact on society . I agree this statement and my view here\n",
    "Firsty , tourism has great impact on everycountry economy beacuse where foriegners are come to see their tourist place they spend money to buy many things . Moreover , some tourism place like a jaipur there is many historical builiding and every year lot of people visit these place to see there \n",
    "Building and when they saw these building buy some tickets . These money spend on building by government .\n",
    "Moreover if they another countries individuals come their country than they expand their culture one place to another place and it is big way exploure our culture .\n",
    "\"\"\"\n",
    "num_return_sequences = 2\n",
    "corrected_text = correct_and_merge(input_text, num_return_sequences)\n",
    "print(input_text)\n",
    "corrected_text\n",
    "# https://writing9.com/text/652464bcd74fc1001274c3d0-nodaway-tourism-has-show-great-impact-on-society-do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the modern era, tourism has had a great impact on society.\n",
      "I agree with this statement and my view here.\n",
      "First of all, tourism has great impact on every country economy beacuse where foreigners come to see their tourist place they spend money to buy many things.\n",
      "Moreover, some tourism places like jaipur there are many historical buildings and every year lot of people visit these places to see there.\n",
      "Building and when they saw these buildings they bought some tickets.\n",
      "This money is spent on building by the government.\n",
      "\n",
      "Moreover, if individuals come from another country then they expand their culture one place to another place and it is a big way to expand our culture.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corrected_texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"3\"></a>\n",
    "\n",
    "### 3. Tense Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tense_model = BertModel.from_pretrained(model_name)\n",
    "tense_tokenizer = AutoTokenizer.from_pretrained(\"model/tense_model/tense_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TenseClassifier(nn.Module):\n",
    "  \n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(TenseClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.relu1(pooled_output)\n",
    "        x = self.relu2(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "model_path = \"model/tense_model/tense.pt\"\n",
    "tense_model = TenseClassifier(tense_model, num_classes=12)\n",
    "\n",
    "tense_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tense_labels = {\n",
    "    'present simple': 0,\n",
    "    'future simple': 1,\n",
    "    'past simple': 2,\n",
    "    'present perfect continuous': 3,\n",
    "    'future perfect': 4,\n",
    "    'past perfect': 5,\n",
    "    'future continuous': 6,\n",
    "    'past perfect continuous': 7,\n",
    "    'present continuous': 8,\n",
    "    'past continuous': 9,\n",
    "    'future perfect continuous': 10,\n",
    "    'present perfect': 11,\n",
    "}\n",
    "def predict_tense(sentence, model, tokenizer, tense_labels):\n",
    "    # tokenizer\n",
    "    encoded_sentence = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(encoded_sentence['input_ids'], encoded_sentence['attention_mask'])\n",
    "        predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    predicted_tense = [k for k, v in tense_labels.items() if v == predicted_label][0]\n",
    "    \n",
    "    return predicted_tense\n",
    "\n",
    "sentence_to_predict = \"\"\"\n",
    "In 2009, the inhabitants in Vietnam reached 95 million individuals.\n",
    "\"\"\"\n",
    "\n",
    "predicted_tense = predict_tense(sentence_to_predict, tense_model, tense_tokenizer, tense_labels)\n",
    "print(f\"The predicted tense for the sentence is: {predicted_tense}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"4\"></a>\n",
    "\n",
    "### 4. Spelling check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Text:\n",
      "In contemporary era that tourism had been show great impact on society . I agree this statement and my view here First , tourism has great impact on every country economy because where foreigners are come to see their tourist place they spend money to buy many things . Moreover , some tourism place like a Jaipur there is many historical building and every year lot of people visit these place to see there Building and when they saw these building buy some tickets . These money spend on building by government . Moreover if they another countries individuals come their country than they expand their culture one place to another place and it is big way explore our culture .\n",
      "Corrections:\n",
      "{'thattourism': 'that tourism', 'Firsty': 'First', 'everycountry': 'every country', 'beacuse': 'because', 'foriegners': 'foreigners', 'jaipur': 'Jaipur', 'builiding': 'building', 'exploure': 'explore'}\n"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "import string\n",
    "\n",
    "dictionary = enchant.Dict(\"en_US\")\n",
    "\n",
    "sentence = \"\"\"\n",
    "In contemporary era thattourism had been show great  impact on society . I agree this statement and my view here\n",
    "Firsty , tourism has great impact on everycountry economy beacuse where foriegners are come to see their tourist place they spend money to buy many things . Moreover , some tourism place like a jaipur there is many historical builiding and every year lot of people visit these place to see there \n",
    "Building and when they saw these building buy some tickets . These money spend on building by government .\n",
    "Moreover if they another countries individuals come their country than they expand their culture one place to another place and it is big way exploure our culture .\n",
    "\"\"\"\n",
    "\n",
    "def spell_checker(sentence):\n",
    "    words = sentence.split()\n",
    "    corrected_sentence = []\n",
    "\n",
    "    corrections = {}\n",
    "\n",
    "    for word in words:\n",
    "        original_word = word.strip(string.punctuation)\n",
    "\n",
    "        if not original_word:\n",
    "            corrected_sentence.append(word)  # Skip empty words\n",
    "        elif not dictionary.check(original_word):\n",
    "            suggestions = dictionary.suggest(original_word)\n",
    "            if suggestions:\n",
    "                corrected_word = suggestions[0]  # Use the first suggestion\n",
    "                corrections[original_word] = corrected_word\n",
    "                corrected_sentence.append(word.replace(original_word, corrected_word))\n",
    "            else:\n",
    "                corrected_sentence.append(word)\n",
    "        else:\n",
    "            corrected_sentence.append(word)\n",
    "\n",
    "    corrected_text = ' '.join(corrected_sentence)\n",
    "    return corrected_text, corrections\n",
    "\n",
    "spell_check, corrections = spell_checker(sentence)\n",
    "print(\"Corrected Text:\")\n",
    "print(spell_check)\n",
    "print(\"Corrections:\")\n",
    "print(corrections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the modern era, tourism has had a great impact on society.\\nI agree with this statement and my view here First, tourism has great impact on every country economy because where foreigners come to see their tourist place they spend money to buy many things.\\nMoreover, some tourism places like Jaipur there are many historical buildings and every year lot of people visit these places to see there Building and when they see these buildings buy some tickets.\\nThis money is spent on building by the government.\\nMoreover if individuals come from another country then they expand their culture one place to another place and it is a big way to explore our culture.\\n',\n",
       " 'In a contemporary era, tourism has had a great impact on society.\\nI agree with this statement and my view here. First, tourism has great impact on every country economy because where foreigners come to see their tourist place they spend money to buy many things.\\nMoreover, some tourism places like Jaipur there are many historical buildings and every year lot of people visit these places to see there Building and when they see these building buy some tickets.\\nThis money is spent on building by government.\\nMoreover, if individuals come from another country, they expand their culture one place to another place and it is a big way to explore our culture.']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_return_sequences = 2\n",
    "corrected_texts = correct_and_merge(spell_check, num_return_sequences)\n",
    "corrected_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In <span style=\"background-color:#7ED957;\">the</span> <span style=\"background-color:#7ED957;\">modern</span> <span style=\"background-color:#7ED957;\">era,</span> <span style=\"background-color:#7ED957;\">tourism</span> <span style=\"background-color:#7ED957;\">has</span> had <span style=\"background-color:#7ED957;\">a</span> great impact on <span style=\"background-color:#7ED957;\">society.</span> I agree <span style=\"background-color:#7ED957;\">with</span> this statement and my view here <span style=\"background-color:#7ED957;\">First,</span> tourism has great impact on <span style=\"background-color:#7ED957;\">every</span> <span style=\"background-color:#7ED957;\">country</span> economy <span style=\"background-color:#7ED957;\">because</span> where <span style=\"background-color:#7ED957;\">foreigners</span> come to see their tourist place they spend money to buy many <span style=\"background-color:#7ED957;\">things.</span> <span style=\"background-color:#7ED957;\">Moreover,</span> some tourism <span style=\"background-color:#7ED957;\">places</span> like <span style=\"background-color:#7ED957;\">Jaipur</span> there <span style=\"background-color:#7ED957;\">are</span> many historical <span style=\"background-color:#7ED957;\">buildings</span> and every year lot of people visit these <span style=\"background-color:#7ED957;\">places</span> to see there Building and when they <span style=\"background-color:#7ED957;\">see</span> these <span style=\"background-color:#7ED957;\">buildings</span> buy some <span style=\"background-color:#7ED957;\">tickets.</span> <span style=\"background-color:#7ED957;\">This</span> money <span style=\"background-color:#7ED957;\">is</span> <span style=\"background-color:#7ED957;\">spent</span> on building by <span style=\"background-color:#7ED957;\">the</span> <span style=\"background-color:#7ED957;\">government.</span> Moreover if individuals come <span style=\"background-color:#7ED957;\">from</span> <span style=\"background-color:#7ED957;\">another</span> country <span style=\"background-color:#7ED957;\">then</span> they expand their culture one place to another place and it is <span style=\"background-color:#7ED957;\">a</span> big way <span style=\"background-color:#7ED957;\">to</span> <span style=\"background-color:#7ED957;\">explore</span> our <span style=\"background-color:#7ED957;\">culture.</span>\n",
      "In <span style=\"background-color:#FF6666;\">contemporary</span> <span style=\"background-color:#FF6666;\">era</span> <span style=\"background-color:#FF6666;\">thattourism</span> had <span style=\"background-color:#FF6666;\">been</span> <span style=\"background-color:#FF6666;\">show</span> great impact on <span style=\"background-color:#FF6666;\">society</span> <span style=\"background-color:#FF6666;\">.</span> I agree this statement and my view here <span style=\"background-color:#FF6666;\">Firsty</span> <span style=\"background-color:#FF6666;\">,</span> tourism has great impact on <span style=\"background-color:#FF6666;\">everycountry</span> economy <span style=\"background-color:#FF6666;\">beacuse</span> where <span style=\"background-color:#FF6666;\">foriegners</span> <span style=\"background-color:#FF6666;\">are</span> come to see their tourist place they spend money to buy many <span style=\"background-color:#FF6666;\">things</span> <span style=\"background-color:#FF6666;\">.</span> <span style=\"background-color:#FF6666;\">Moreover</span> <span style=\"background-color:#FF6666;\">,</span> some tourism <span style=\"background-color:#FF6666;\">place</span> like <span style=\"background-color:#FF6666;\">a</span> <span style=\"background-color:#FF6666;\">jaipur</span> there <span style=\"background-color:#FF6666;\">is</span> many historical <span style=\"background-color:#FF6666;\">builiding</span> and every year lot of people visit these <span style=\"background-color:#FF6666;\">place</span> to see there Building and when they <span style=\"background-color:#FF6666;\">saw</span> these <span style=\"background-color:#FF6666;\">building</span> buy some <span style=\"background-color:#FF6666;\">tickets</span> <span style=\"background-color:#FF6666;\">.</span> <span style=\"background-color:#FF6666;\">These</span> money <span style=\"background-color:#FF6666;\">spend</span> on building by <span style=\"background-color:#FF6666;\">government</span> <span style=\"background-color:#FF6666;\">.</span> Moreover if <span style=\"background-color:#FF6666;\">they</span> <span style=\"background-color:#FF6666;\">another</span> <span style=\"background-color:#FF6666;\">countries</span> individuals come <span style=\"background-color:#FF6666;\">their</span> country <span style=\"background-color:#FF6666;\">than</span> they expand their culture one place to another place and it is big way <span style=\"background-color:#FF6666;\">exploure</span> our <span style=\"background-color:#FF6666;\">culture</span> <span style=\"background-color:#FF6666;\">.</span>\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "def highlight(correct_sentence, error_Sentence, color):\n",
    "    differ = difflib.Differ()\n",
    "    diff = list(differ.compare(correct_sentence.split(), error_Sentence.split()))\n",
    "\n",
    "    highlighted_diff = []\n",
    "    for word in diff:\n",
    "        if word.startswith(' '):\n",
    "            highlighted_diff.append(word[2:])\n",
    "        elif word.startswith('- '):\n",
    "            highlighted_diff.append('<span style=\"background-color:'+color+';\">{}</span>'.format(word[2:]))\n",
    "    \n",
    "    highlighted_sentence = ' '.join(highlighted_diff)\n",
    "\n",
    "    return highlighted_sentence\n",
    "\n",
    "mark_f = highlight(sentence,corrected_texts[0],\"#FF6666\")\n",
    "mark_t = highlight(corrected_texts[0],sentence,\"#7ED957\")\n",
    "print(\"<p>\"+mark_t+\"</p>\")\n",
    "print(\"<p>\"+mark_f+\"</p>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"5\"></a>\n",
    "\n",
    "### 5. Identify error types\n",
    "\n",
    "***You need to run all the code in the English learning support section to execute the following part.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences with Grammar Errors:\n",
      "In the modern era, tourism has had a great impact on society.\n",
      "I agree with this statement and my view here First, tourism has great impact on every country economy because where foreigners come to see their tourist place they spend money to buy many things.\n",
      "Moreover, some tourism places like Jaipur there are many historical buildings and every year lot of people visit these places to see there Building and when they see these buildings buy some tickets.\n",
      "This money is spent on building by the government.\n",
      "Moreover if individuals come from another country then they expand their culture one place to another place and it is a big way to explore our culture.\n",
      "Sentences with Spelling Errors:\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "def identify_error_types(paragraph, corrected_paragraph, corrected_words):\n",
    "    differ = difflib.Differ()\n",
    "    \n",
    "    list_wrong_grammar_sentences = []\n",
    "    list_wrong_spell_sentences = []\n",
    "    \n",
    "    for orig_sentence, corr_sentence in zip(paragraph, corrected_paragraph):\n",
    "        diff = list(differ.compare(orig_sentence.split(), corr_sentence.split()))\n",
    "        \n",
    "        grammar_errors = []\n",
    "        spelling_errors = []\n",
    "\n",
    "        for word_diff in diff:\n",
    "            word = word_diff[2:]\n",
    "            \n",
    "            if word_diff.startswith('- '): \n",
    "                if word in corrected_words:\n",
    "                    spelling_errors.append(word)\n",
    "                else:\n",
    "                    grammar_errors.append(word)\n",
    "\n",
    "        if grammar_errors:\n",
    "            list_wrong_grammar_sentences.append(orig_sentence)\n",
    "        elif spelling_errors and not grammar_errors:\n",
    "            list_wrong_spell_sentences.append(orig_sentence)\n",
    "    \n",
    "    return list_wrong_grammar_sentences, list_wrong_spell_sentences\n",
    "\n",
    "wrong_grammar, wrong_spelling = identify_error_types(corrected_texts[0].split('\\n'), sentence.split('\\n'), corrections)\n",
    "\n",
    "print(\"Sentences with Grammar Errors:\")\n",
    "for sentence in wrong_grammar:\n",
    "    print(sentence)\n",
    "\n",
    "print(\"Sentences with Spelling Errors:\")\n",
    "for sentence in wrong_spelling:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the modern era, tourism has had a great impact on society. : past\n",
      "I agree with this statement and my view here First, tourism has great impact on every country economy because where foreigners come to see their tourist place they spend money to buy many things. : past\n",
      "Moreover, some tourism places like Jaipur there are many historical buildings and every year lot of people visit these places to see there Building and when they see these buildings buy some tickets. : past\n",
      "This money is spent on building by the government. : past\n",
      "Moreover if individuals come from another country then they expand their culture one place to another place and it is a big way to explore our culture. : past\n",
      "[None, None, None, None, None]\n",
      "{'thattourism': 'that tourism', 'Firsty': 'First', 'everycountry': 'every country', 'beacuse': 'because', 'foriegners': 'foreigners', 'jaipur': 'Jaipur', 'builiding': 'building', 'exploure': 'explore'}\n"
     ]
    }
   ],
   "source": [
    "w_tense = []\n",
    "for sentence in wrong_grammar:\n",
    "    w_tense.append(predict_tense(sentence_to_predict, tense_model, tense_tokenizer, tense_labels))\n",
    "\n",
    "wrong = []\n",
    "# result = [{\"paragraph\": list_para[i], \"summary\": summaries[i]} for i in range(len(summaries))]\n",
    "wrong = [{\"wrong_sentence\": wrong_grammar[i], \"tense_labels\": w_tense[i]}for i in range(len(w_tense))]\n",
    "print([print(i[\"wrong_sentence\"],\":\",i[\"tense_labels\"]) for i in wrong])\n",
    "print(corrections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"6\"></a>\n",
    "\n",
    "### 6. Structure prediction constituency parser\n",
    "\n",
    "***This part is no longer necessary.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -q allennlp\n",
    "# !pip uninstall -q allennlp-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is unnecessary \n",
    "# !pip install h5py\n",
    "# !pip install typing-extensions\n",
    "# !pip install wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --ignore-installed six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp_models import pretrained\n",
    "# print(pretrained.get_pretrained_models())\n",
    "predictor = pretrained.load_predictor(\"structured-prediction-constituency-parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"he is doing home work. She phaying game.\"\n",
    "# test_sentence = test_sentence.rstrip('?:!.,;')\n",
    "print (test_sentence)\n",
    "parser_output = predictor.predict(test_sentence)\n",
    "# print (parser_output)\n",
    "tag = parser_output[\"pos_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = parser_output[\"trees\"]\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "tree = Tree.fromstring(tree)\n",
    "print(tree)\n",
    "print(tree.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "import re\n",
    "# Tải mô hình ngôn ngữ tiếng Anh của spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_sentence_structure(sentence):\n",
    "    # Sử dụng spaCy để phân tích câu và lấy danh sách các từ (tokens) và POS tags của câu\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.text for token in doc]\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    return tokens, pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Không có câu nào lặp cấu trúc.\n"
     ]
    }
   ],
   "source": [
    "def find_duplicate_sentence_structures(text):\n",
    "    # Sử dụng module re để tách câu theo nhiều dấu câu khác nhau\n",
    "    sentences = re.split(r'[.,!?]', text)\n",
    "    \n",
    "    sentence_structures = defaultdict(list)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        _, pos_tags = get_sentence_structure(sentence)\n",
    "\n",
    "        structure_key = tuple(pos_tags)\n",
    "\n",
    "        sentence_structures[structure_key].append(sentence)\n",
    "\n",
    "    found_duplicates = False \n",
    "    for structure_key, sentences in sentence_structures.items():\n",
    "        if len(sentences) > 1:\n",
    "            found_duplicates = True\n",
    "            print(f\"Similar Struture: {structure_key}\")\n",
    "            print(\"Similar structure sentences:\")\n",
    "            for sentence in sentences:\n",
    "                print(f\"- {sentence.strip()}\\n\")  # Loại bỏ khoảng trắng dư thừa\n",
    "            print()\n",
    "    \n",
    "    if not found_duplicates:\n",
    "        print(\"Không có câu nào lặp cấu trúc.\")\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "text = \"\"\"\n",
    "In 2009, the inhabitants in Vietnam reached 95 million individuals.\n",
    "Simultaneously, the population in Japan hit 50 million citizens.\n",
    "\"\"\"\n",
    "find_duplicate_sentence_structures(corrected_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation and Fact - Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"7\"></a>\n",
    "\n",
    "### 1. T5 for summarize task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model_name = \"t5-small\" # model\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13876\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import textract\n",
    "\n",
    "# Đọc nội dung của tài liệu PDF\n",
    "text = textract.process('pdf/Report.pdf', encoding='utf-8')\n",
    "\n",
    "# Sử dụng biểu thức chính quy để cắt thành các đoạn văn\n",
    "all_paragraphs = re.split(r'\\s{2,}', text.decode('utf-8'))\n",
    "num_paragraph= len(text)\n",
    "print(num_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_para = []\n",
    "list_para = [para for para in all_paragraphs if len(para.split()) >= 20] # list of paragraphs which have more than 20 words\n",
    "len(list_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize paragraph using t5 model\n",
    "\n",
    "summaries = []\n",
    "for i,paragraph in enumerate(list_para):\n",
    "    input_text = \"summarize: \" + paragraph\n",
    "    input_ids = t5_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = t5_model.generate(input_ids, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    summaries.append(summary)\n",
    "    \n",
    "    # if i==3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abstract. This paper aims to develop a system that will help in recommendation of courses for an upcoming semester based on the performance of previous semesters.',\n",
       " \"It has always been a tough choice for the students to choose the courses in different semesters in which there is possibility to score good grades apart from the interest in the course. IIIT-Delhi offers variety of courses with mandatory courses in first 4 semesters (with exception of 2 to 3 electives) and all elective courses from fifth semester onwards. Hence, choosing the courses based on the verbal recommendation from the seniors, instructors and fellowmates becomes a hectic task. For easing this process of course recommendation for an upcoming semester, we have developed a system which deploys simple yet powerful recommendation techniques such as auto-encoders, hybrid matrix factorization and similarity based approaches. It is a GUI based system which takes an input of student's ID (which is stored in the backend database) and semester for which the student wants to get the recommendation. Then, it outputs the top 5 courses that the student can choose based on his/her performance in previous semesters. Also, a confidence score is provided for each recommended course.\",\n",
       " 'The dataset has been acquired from the official IIIT-Delhi academics department for the students of 7 Computer Science passout batches. The dataset consists 739 students and 306 subjects with mapping of each student to the grades for each course the student has taken throughout the duration of their degree. The courses are spread over 8 semesters (also including the courses offered to student with extended semester). The fields in the dataset include Serial Number(SN), Roll Number(anonymized) of the student, Batch/Term Code in which the course was offered, Course Code, Course name, Credit offered for the course, Grade obtained by particular student in the course, SPI (Semester Percentile Index) - Average GPA(Grade Point Average) for the current semester and CPI (Cummulative Percentile Index) - Overall GPA.',\n",
       " 'The data is processed from CSV format to JSON format. Each student is mapped to all the courses with the grade and semester in which the course was offered. If the student has taken the course, then the course key(under the particular student key) will have the corresponding grade obtained in the course and the semester in which the course was offered. Each user and course is mapped to a unique integer ID. Incomplete courses, courses for which leave application was given by the student and courses with grade W(weak) are given grade score 0. Online courses with grade S are given grade score as 10 while with X are given grade score 0. For creation of train and test matrices we included only courses that were offered in first 8 semesters as any semester beyond that signifies the presence of backlog or extended semester. For such cases we took the maximum grade score provided to the student in a particular course including all extended semesters and backlogs. Every course has a list of semesters in which it was offered as the same course can be floated in more than 1 semester. Various split ratios were considered and 5 fold cross validation was done for creating the matrices. Test matrix only considered the student-course pair for courses in 5th, 6th 7th and 8th semesters.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_para[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge into result \n",
    "\n",
    "result = [{\"paragraph\": list_para[i], \"summary\": summaries[i]} for i in range(len(summaries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paragraph': 'Abstract. This paper aims to develop a system that will help in recommendation of courses for an upcoming semester based on the performance of previous semesters.',\n",
       "  'summary': 'this paper aims to develop a system that will help in recommendation of courses for an upcoming semester based on the performance of previous semesters. this paper aims to develop a system that will help in recommendation of courses based on the performance of previous semesters.'},\n",
       " {'paragraph': \"It has always been a tough choice for the students to choose the courses in different semesters in which there is possibility to score good grades apart from the interest in the course. IIIT-Delhi offers variety of courses with mandatory courses in first 4 semesters (with exception of 2 to 3 electives) and all elective courses from fifth semester onwards. Hence, choosing the courses based on the verbal recommendation from the seniors, instructors and fellowmates becomes a hectic task. For easing this process of course recommendation for an upcoming semester, we have developed a system which deploys simple yet powerful recommendation techniques such as auto-encoders, hybrid matrix factorization and similarity based approaches. It is a GUI based system which takes an input of student's ID (which is stored in the backend database) and semester for which the student wants to get the recommendation. Then, it outputs the top 5 courses that the student can choose based on his/her performance in previous semesters. Also, a confidence score is provided for each recommended course.\",\n",
       "  'summary': 'IIIT-Delhi offers variety of courses with mandatory courses in first 4 semesters. all elective courses from fifth semester onwards become a hectic task. it deploys simple yet powerful recommendation techniques such as auto-encoders.'},\n",
       " {'paragraph': 'The dataset has been acquired from the official IIIT-Delhi academics department for the students of 7 Computer Science passout batches. The dataset consists 739 students and 306 subjects with mapping of each student to the grades for each course the student has taken throughout the duration of their degree. The courses are spread over 8 semesters (also including the courses offered to student with extended semester). The fields in the dataset include Serial Number(SN), Roll Number(anonymized) of the student, Batch/Term Code in which the course was offered, Course Code, Course name, Credit offered for the course, Grade obtained by particular student in the course, SPI (Semester Percentile Index) - Average GPA(Grade Point Average) for the current semester and CPI (Cummulative Percentile Index) - Overall GPA.',\n",
       "  'summary': 'the dataset has been acquired from the official IIIT-Delhi academics department for the students of 7 Computer Science passout batches. the dataset consists 739 students and 306 subjects with mapping of each student to the grades for each course the student has taken throughout the duration of their degree.'},\n",
       " {'paragraph': 'The data is processed from CSV format to JSON format. Each student is mapped to all the courses with the grade and semester in which the course was offered. If the student has taken the course, then the course key(under the particular student key) will have the corresponding grade obtained in the course and the semester in which the course was offered. Each user and course is mapped to a unique integer ID. Incomplete courses, courses for which leave application was given by the student and courses with grade W(weak) are given grade score 0. Online courses with grade S are given grade score as 10 while with X are given grade score 0. For creation of train and test matrices we included only courses that were offered in first 8 semesters as any semester beyond that signifies the presence of backlog or extended semester. For such cases we took the maximum grade score provided to the student in a particular course including all extended semesters and backlogs. Every course has a list of semesters in which it was offered as the same course can be floated in more than 1 semester. Various split ratios were considered and 5 fold cross validation was done for creating the matrices. Test matrix only considered the student-course pair for courses in 5th, 6th 7th and 8th semesters.',\n",
       "  'summary': 'the data is processed from CSV format to JSON format. each student is mapped to all the courses with the grade and semester in which the course was offered. if the student has taken the course, then the course key will have the corresponding grade obtained in the course and the semester in which the course was offered.'},\n",
       " {'paragraph': 'Fig. 1: Top 5 elective courses. (CSE535 - Mobile Computing, CSE506 - Data Mining, CSE345 - Foundation of Security, CSE300 - Software Engineering, FIN401 - Foundations of Finance )',\n",
       "  'summary': 'CSE535 - Mobile Computing, CSE506 - Data Mining, CSE345 - Foundation of security, CSE300 - Software Engineering, FIN401 - Foundations of finance. FIN401 - foundations of finance.'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"8\"></a>\n",
    "\n",
    "### 2. Question generation\n",
    "\n",
    "***You have to summary the file before this task***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'c:\\\\Users\\\\Sysme Hue\\\\Desktop\\\\LMS\\\\Libs\\\\run_qg.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Clone this responsitory for questions generator\n",
    "\n",
    "# !git clone https://github.com/amontgomerie/question_generator\n",
    "# !pip install -r question_generator/requirements.txt -qq\n",
    "# !python run_qg.py --text_file question_generator/articles/twitter_hack.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sysme Hue\\Desktop\\LMS\\Libs\\question_generator\n"
     ]
    }
   ],
   "source": [
    "# %cd question_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import file from another folder\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, 'question_generator')\n",
    "\n",
    "from questiongenerator import QuestionGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qg = QuestionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = result[2][\"summary\"]\n",
    "q  = qg.generate(text, num_questions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'how many students have taken the course?',\n",
       "  'answer': 'the dataset consists 739 students and 306 subjects with mapping of each student to the grades for each course the student has taken throughout the duration of their degree.'},\n",
       " {'question': 'how many students have taken the iitt?',\n",
       "  'answer': 'the dataset has been acquired from the official IIIT-Delhi academics department for the students of 7 Computer Science passout batches.'},\n",
       " {'question': 'how many subjects do students have taken?',\n",
       "  'answer': [{'answer': 'dataset', 'correct': False},\n",
       "   {'answer': '7 Computer Science', 'correct': False},\n",
       "   {'answer': '306', 'correct': True},\n",
       "   {'answer': '739', 'correct': False}]}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset has been acquired from the official IIIT-Delhi academics department for the students of 7 Computer Science passout batches. the dataset consists 739 students and 306 subjects with mapping of each student to the grades for each course the student has taken throughout the duration of their degree.\n",
      "how many students have taken the course?\n",
      "how many students have taken the iitt?\n",
      "how many subjects do students have taken?\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "for i in range(len(q)): print(q[i][\"question\"]) #  7 Computer Science passout batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"9\"></a>\n",
    "\n",
    "### 3. Fact -check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT MODEL\n",
    "\n",
    "class BERTClassificationModel(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels):\n",
    "        super(BERTClassificationModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size * 2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = torch.cat((outputs.last_hidden_state[:, 0, :], outputs.last_hidden_state[:, -1, :]), dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_path = \"model/bert_classification_model.pth\"\n",
    "# Tạo mô hình mới\n",
    "loaded_model = BERTClassificationModel('bert-base-uncased', num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    loaded_model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    loaded_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải lại tokenizer từ đường dẫn đã lưu\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(\"model/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise_text : Yesterday, I bought a new computer \n",
      " hypothesis_text : I did not buy a new computer\n",
      "Predicted Label: contradiction\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 128\n",
    "def predict_premise_hypothesis(premise_text, hypothesis_text, model, tokenizer):\n",
    "    # Chuẩn bị dữ liệu đầu vào cho mô hình\n",
    "    inputs = tokenizer(premise_text, hypothesis_text, padding=True, truncation=True, max_length=max_seq_length, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Dự đoán\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predicted_label = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "pre = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "# Sử dụng hàm predict_premise_hypothesis để dự đoán\n",
    "\n",
    "premise_text = \"Yesterday, I bought a new computer\"\n",
    "hypothesis_text = \"I did not buy a new computer\"\n",
    "predicted_label = predict_premise_hypothesis(premise_text, hypothesis_text, loaded_model, loaded_tokenizer)\n",
    "\n",
    "print(\"premise_text :\", premise_text, \"\\n\", \"hypothesis_text :\", hypothesis_text)\n",
    "print(\"Predicted Label:\", pre[predicted_label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise_text : The likelihood is 100% \n",
      " hypothesis_text : Maybe the probability is 100%\n",
      "Predicted Label: neutral\n"
     ]
    }
   ],
   "source": [
    "pre = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "# Sử dụng hàm predict_premise_hypothesis để dự đoán\n",
    "premise_text = \"The likelihood is 100%\"\n",
    "hypothesis_text = \"Maybe the probability is 100%\"\n",
    "predicted_label = predict_premise_hypothesis(premise_text, hypothesis_text, loaded_model, loaded_tokenizer)\n",
    "\n",
    "print(\"premise_text :\", premise_text, \"\\n\", \"hypothesis_text :\", hypothesis_text)\n",
    "print(\"Predicted Label:\", pre[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise_text = \"The dataset had acquired for the student of 7 computer science\"\n",
    "hypothesis_text = result[2][\"summary\"]\n",
    "\n",
    "predicted_label = predict_premise_hypothesis(premise_text, hypothesis_text, loaded_model, loaded_tokenizer)\n",
    "print(\"premise_text :\", premise_text, \"\\n\", \"hypothesis_text :\", hypothesis_text)\n",
    "print(\"Predicted Label:\", pre[predicted_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommend system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"10\"></a>\n",
    "\n",
    "### 1. Association Rule - Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:205: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n",
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import en_core_web_sm\n",
    "spc_en = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset --> https://www.kaggle.com/datasets/leewanhung/coursera-dataset?select=df_c.csv\n",
    "course = pd.read_csv(\"../data/course.csv\")\n",
    "# id = pd.read_csv(\"../data/id.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Course_id</th>\n",
       "      <th>Course Name</th>\n",
       "      <th>University</th>\n",
       "      <th>Difficulty Level</th>\n",
       "      <th>Course Rating</th>\n",
       "      <th>Course URL</th>\n",
       "      <th>Course Description</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Specialized</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Vector</th>\n",
       "      <th>IDF</th>\n",
       "      <th>BERT_Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>write feature length screenplay film television</td>\n",
       "      <td>Michigan State University</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>4.8</td>\n",
       "      <td>https://www.coursera.org/learn/write-a-feature...</td>\n",
       "      <td>write full length feature film script course w...</td>\n",
       "      <td>drama comedy peer screenwrite film document re...</td>\n",
       "      <td>Arts and Humanities</td>\n",
       "      <td>write feature length screenplay film televisio...</td>\n",
       "      <td>[3.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[ 3.09796989e-01  8.47294629e-02  4.00437862e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>business strategy business model canvas analys...</td>\n",
       "      <td>Coursera Project Network</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>4.8</td>\n",
       "      <td>https://www.coursera.org/learn/canvas-analysis...</td>\n",
       "      <td>end guide project fluent identifying create bu...</td>\n",
       "      <td>finance business plan persona user experience ...</td>\n",
       "      <td>Business</td>\n",
       "      <td>business strategy business model canvas analys...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-1.62371062e-02 -5.21499477e-02  3.00578535e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>silicon thin film solar cell</td>\n",
       "      <td>�cole Polytechnique</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://www.coursera.org/learn/silicon-thin-fi...</td>\n",
       "      <td>course consist general presentation solar cell...</td>\n",
       "      <td>chemistry physics solar energy film lambda cal...</td>\n",
       "      <td>Physical Science and Engineering</td>\n",
       "      <td>silicon thin film solar cell course consist ge...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[ 1.11733921e-01  2.74278939e-01  4.72732037e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>finance manager</td>\n",
       "      <td>IESE Business School</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>4.8</td>\n",
       "      <td>https://www.coursera.org/learn/operational-fin...</td>\n",
       "      <td>come number always meet eye operational financ...</td>\n",
       "      <td>account receivable dupont analysis analysis ac...</td>\n",
       "      <td>Business</td>\n",
       "      <td>finance manager come number always meet eye op...</td>\n",
       "      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-4.35875319e-02  1.21336073e-01  3.25895369e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>retrieve datum use single table sql query</td>\n",
       "      <td>Coursera Project Network</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>4.6</td>\n",
       "      <td>https://www.coursera.org/learn/single-table-sq...</td>\n",
       "      <td>course learn effectively retrieve datum relati...</td>\n",
       "      <td>datum analysis select sql database management ...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>retrieve datum use single table sql query cour...</td>\n",
       "      <td>[117.0 34.0 27.0 33.0 258.0 201.0 42.0 0.0 125...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[ 6.04763143e-02  3.37209553e-02  4.53004211e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Course_id                                        Course Name   \n",
       "0           0          1    write feature length screenplay film television  \\\n",
       "1           1          2  business strategy business model canvas analys...   \n",
       "2           2          3                       silicon thin film solar cell   \n",
       "3           3          4                                    finance manager   \n",
       "4           4          5          retrieve datum use single table sql query   \n",
       "\n",
       "                  University Difficulty Level Course Rating   \n",
       "0  Michigan State University         Beginner           4.8  \\\n",
       "1   Coursera Project Network         Beginner           4.8   \n",
       "2        �cole Polytechnique         Advanced           4.1   \n",
       "3       IESE Business School     Intermediate           4.8   \n",
       "4   Coursera Project Network         Beginner           4.6   \n",
       "\n",
       "                                          Course URL   \n",
       "0  https://www.coursera.org/learn/write-a-feature...  \\\n",
       "1  https://www.coursera.org/learn/canvas-analysis...   \n",
       "2  https://www.coursera.org/learn/silicon-thin-fi...   \n",
       "3  https://www.coursera.org/learn/operational-fin...   \n",
       "4  https://www.coursera.org/learn/single-table-sq...   \n",
       "\n",
       "                                  Course Description   \n",
       "0  write full length feature film script course w...  \\\n",
       "1  end guide project fluent identifying create bu...   \n",
       "2  course consist general presentation solar cell...   \n",
       "3  come number always meet eye operational financ...   \n",
       "4  course learn effectively retrieve datum relati...   \n",
       "\n",
       "                                              Skills   \n",
       "0  drama comedy peer screenwrite film document re...  \\\n",
       "1  finance business plan persona user experience ...   \n",
       "2  chemistry physics solar energy film lambda cal...   \n",
       "3  account receivable dupont analysis analysis ac...   \n",
       "4  datum analysis select sql database management ...   \n",
       "\n",
       "                        Specialized   \n",
       "0               Arts and Humanities  \\\n",
       "1                          Business   \n",
       "2  Physical Science and Engineering   \n",
       "3                          Business   \n",
       "4            Information Technology   \n",
       "\n",
       "                                            Sequence   \n",
       "0  write feature length screenplay film televisio...  \\\n",
       "1  business strategy business model canvas analys...   \n",
       "2  silicon thin film solar cell course consist ge...   \n",
       "3  finance manager come number always meet eye op...   \n",
       "4  retrieve datum use single table sql query cour...   \n",
       "\n",
       "                                              Vector   \n",
       "0  [3.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0]  \\\n",
       "1              [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]   \n",
       "2              [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]   \n",
       "3              [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]   \n",
       "4  [117.0 34.0 27.0 33.0 258.0 201.0 42.0 0.0 125...   \n",
       "\n",
       "                                                 IDF   \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \\\n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                        BERT_Encoded  \n",
       "0  [ 3.09796989e-01  8.47294629e-02  4.00437862e-...  \n",
       "1  [-1.62371062e-02 -5.21499477e-02  3.00578535e-...  \n",
       "2  [ 1.11733921e-01  2.74278939e-01  4.72732037e-...  \n",
       "3  [-4.35875319e-02  1.21336073e-01  3.25895369e-...  \n",
       "4  [ 6.04763143e-02  3.37209553e-02  4.53004211e-...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv(\"../data/enrolled_course.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>History_course_id</th>\n",
       "      <th>History_course_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user0001</td>\n",
       "      <td>0687, 0355, 1677</td>\n",
       "      <td>Sequence Models, Natural Language Processing w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user0002</td>\n",
       "      <td>0636, 0637, 1787</td>\n",
       "      <td>Exploratory Data Analysis, Exploratory Data An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user0003</td>\n",
       "      <td>0120, 0286, 2712</td>\n",
       "      <td>Managing Big Data with MySQL, Introduction to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user0004</td>\n",
       "      <td>0406, 0603, 0840</td>\n",
       "      <td>Data Visualization and Communication with Tabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user0005</td>\n",
       "      <td>0241, 2558, 3025, 3503</td>\n",
       "      <td>Deploy Models with TensorFlow Serving and Flas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>user2134</td>\n",
       "      <td>0685, 0819</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>user2135</td>\n",
       "      <td>1677, 2924, 1755, 2754</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>user2136</td>\n",
       "      <td>3361, 3369, 3399</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>user2137</td>\n",
       "      <td>0685, 0819</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>user2138</td>\n",
       "      <td>3356, 0685</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2138 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       User_id       History_course_id   \n",
       "0     user0001        0687, 0355, 1677  \\\n",
       "1     user0002        0636, 0637, 1787   \n",
       "2     user0003        0120, 0286, 2712   \n",
       "3     user0004        0406, 0603, 0840   \n",
       "4     user0005  0241, 2558, 3025, 3503   \n",
       "...        ...                     ...   \n",
       "2133  user2134              0685, 0819   \n",
       "2134  user2135  1677, 2924, 1755, 2754   \n",
       "2135  user2136        3361, 3369, 3399   \n",
       "2136  user2137              0685, 0819   \n",
       "2137  user2138              3356, 0685   \n",
       "\n",
       "                                    History_course_name  \n",
       "0     Sequence Models, Natural Language Processing w...  \n",
       "1     Exploratory Data Analysis, Exploratory Data An...  \n",
       "2     Managing Big Data with MySQL, Introduction to ...  \n",
       "3     Data Visualization and Communication with Tabl...  \n",
       "4     Deploy Models with TensorFlow Serving and Flas...  \n",
       "...                                                 ...  \n",
       "2133                                                NaN  \n",
       "2134                                                NaN  \n",
       "2135                                                NaN  \n",
       "2136                                                NaN  \n",
       "2137                                                NaN  \n",
       "\n",
       "[2138 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history # 395\tuser0396\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(history['History_course_id'].apply(lambda x:x.split(\",\") ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0687', ' 0355', ' 1677'],\n",
       " ['0636', ' 0637', ' 1787'],\n",
       " ['0120', ' 0286', ' 2712'],\n",
       " ['0406', ' 0603', ' 0840'],\n",
       " ['0241', ' 2558', ' 3025', ' 3503'],\n",
       " ['0450', ' 0662', ' 2696'],\n",
       " ['0391', ' 1808', ' 3454'],\n",
       " ['2533', ' 2543', ' 2564'],\n",
       " ['2919', ' 3097', ' 3260'],\n",
       " ['1581', ' 2154', ' 2725'],\n",
       " ['0088', ' 0112', ' 0233'],\n",
       " ['0211', ' 0220', ' 0241'],\n",
       " ['0149', ' 0211', ' 0288'],\n",
       " ['0220', ' 0241', ' 0288'],\n",
       " ['0199', ' 0211', ' 0241'],\n",
       " ['0120', ' 0286', ' 0413'],\n",
       " ['0066', ' 0149', ' 0396'],\n",
       " ['0108', ' 0279', ' 0687'],\n",
       " ['0330', ' 0376', ' 0530'],\n",
       " ['0058', ' 0221', ' 0849'],\n",
       " ['0485', ' 1210', ' 1792'],\n",
       " ['1366', ' 2486', ' 3352'],\n",
       " ['3399', ' 3423', ' 3454'],\n",
       " ['0543', ' 2854', ' 3352'],\n",
       " ['2845', ' 3349', ' 3361'],\n",
       " ['3352', ' 3369', ' 3399'],\n",
       " ['3334', ' 3352', ' 3361'],\n",
       " ['3361', ' 3369', ' 3370'],\n",
       " ['3361', ' 2708'],\n",
       " ['3352', ' 3369', ' 3376'],\n",
       " ['3352', ' 3369', ' 3423'],\n",
       " ['3454', ' 3465', ' 3472'],\n",
       " ['3465', ' 3472', ' 3475'],\n",
       " ['3475', ' 3483', ' 3491'],\n",
       " ['0636', ' 0637', ' 1787', ' 2353'],\n",
       " ['0241', ' 2558', ' 3025', ' 3503'],\n",
       " ['3361', ' 3369', ' 3399'],\n",
       " ['3361', ' 3369', ' 3399'],\n",
       " ['2087', ' 2157', ' 2202', ' 2306', ' 2814', ' 2896'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0075', ' 0343', ' 0913'],\n",
       " ['1045', ' 1586'],\n",
       " ['2862', ' 2896', ' 2946'],\n",
       " ['2144', ' 2202', ' 2215', ' 2306', ' 2353', ' 2401', ' 2402'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588'],\n",
       " ['3384', ' 3398', ' 3465', ' 3472', ' 3475', ' 3483'],\n",
       " ['3475', ' 3483', ' 3491', ' 3503', ' 3361', ' 3369', ' 3370'],\n",
       " ['3465', ' 3472', ' 3475', ' 3483', ' 3491', ' 3503'],\n",
       " ['0241', ' 2558', ' 3025', ' 3503'],\n",
       " ['3361', ' 3369', ' 3399', ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588', ' 2144'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['3361', ' 3369', ' 3399', ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588', ' 2144'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588', ' 2144'],\n",
       " ['3361', ' 3369', ' 3399', ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588', ' 2144'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['3361', ' 3369', ' 3399', ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588', ' 2144'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588', ' 2144'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0385', ' 0992', ' 0670', ' 3469'],\n",
       " ['0687', ' 0355', ' 1677'],\n",
       " ['1677', ' 2924', ' 1755', ' 2754', ' 2570', ' 2497', ' 1838'],\n",
       " ['0385', ' 0992', ' 0670', ' 3469'],\n",
       " ['0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588', ' 2144'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['3361', ' 3369', ' 3399', ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588', ' 2144'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0385', ' 0992', ' 0670', ' 3469'],\n",
       " ['0687', ' 0355', ' 1677'],\n",
       " ['1677', ' 2924', ' 1755', ' 2754', ' 2570', ' 2497', ' 1838'],\n",
       " ['2450', ' 2530', ' 2588', ' 2602    '],\n",
       " ['2190', ' 2836', ' 3184   '],\n",
       " ['1677', ' 2154', ' 2530   '],\n",
       " ['0670', ' 0992', ' 3469    '],\n",
       " ['0992', ' 3469', ' 3503  '],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['3361', ' 3369', ' 3399', ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " [' 0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588', ' 2144'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588', ' 2144'],\n",
       " ['0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588', ' 2144'],\n",
       " ['0687', ' 0355', ' 1677'],\n",
       " ['0687', ' 0355', ' 1677'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['1677', ' 2924', ' 1755', ' 2754', ' 2570', ' 2497', ' 1838'],\n",
       " ['2144', ' 2202', ' 2215', ' 2306', ' 2353', ' 2401'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['1677', ' 2924', ' 1755', ' 2754', ' 2570', ' 2497', ' 1838'],\n",
       " ['3361', ' 2708'],\n",
       " ['1366', ' 2486', ' 3352'],\n",
       " ['1677', ' 2924', ' 1755', ' 2754', ' 2570', ' 2497', ' 1838'],\n",
       " ['1366', ' 2486', ' 3352'],\n",
       " ['1677', ' 2924', ' 1755', ' 2754', ' 2570', ' 2497', ' 1838'],\n",
       " ['3361', ' 2708'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['0636', ' 0637', ' 1787'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0385', ' 0992', ' 0670', ' 3469'],\n",
       " ['3361', ' 2708'],\n",
       " ['0687', ' 0355', ' 1677'],\n",
       " ['0636', ' 0637', ' 1787'],\n",
       " ['0636', ' 0637', ' 1787'],\n",
       " ['0636', ' 0637', ' 1787'],\n",
       " ['3361', ' 2708', ' 0636', ' 0637', ' 1787'],\n",
       " ['3361', ' 2708', ' 0636', ' 0637', ' 1787'],\n",
       " ['0687', ' 0355', ' 1677'],\n",
       " ['1677', ' 2924', ' 1755', ' 2754', ' 2570', ' 2497', ' 1838'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['3361', ' 3369', ' 3399', ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588', ' 2144'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0385', ' 0992', ' 0670', ' 3469'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['3361', ' 3369', ' 3399', ' 3503'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0075', ' 0343', ' 0913', ' 1045', ' 1586', ' 1588', ' 2144'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['0385', ' 0992', ' 0670', ' 3469'],\n",
       " ['3361', ' 2708'],\n",
       " ['3361', ' 2708'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['3361', ' 2708'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['3361', ' 2708'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['3361', ' 2708'],\n",
       " ['3361', ' 2708'],\n",
       " ['3361', ' 2708'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['3361', ' 2708'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['3361', ' 2708'],\n",
       " ['2842', ' 2993', ' 3108', ' 3469', ' 3503'],\n",
       " ['3361', ' 2708'],\n",
       " ['3361', ' 2708'],\n",
       " ['3361', ' 2708'],\n",
       " ['0096', ' 0627'],\n",
       " ['0851', ' 1759'],\n",
       " ['0655', ' 3179'],\n",
       " ['1754', ' 3448'],\n",
       " ['1213', ' 1230'],\n",
       " ['0863', ' 2100'],\n",
       " ['1755', ' 1756'],\n",
       " ['0152', ' 0622'],\n",
       " ['0764', ' 1584'],\n",
       " ['0125', ' 0135'],\n",
       " ['0807', ' 1759'],\n",
       " ['0135', ' 2012'],\n",
       " ['0622', ' 0714'],\n",
       " ['0605', ' 2722'],\n",
       " ['0863', ' 3463'],\n",
       " ['0801', ' 2721'],\n",
       " ['3179', ' 3180'],\n",
       " ['3057', ' 3125'],\n",
       " ['3077', ' 3361'],\n",
       " ['3101', ' 3206'],\n",
       " ['3049', ' 0138'],\n",
       " ['1980', ' 1451'],\n",
       " ['0622', ' 1740'],\n",
       " ['0725', ' 1213'],\n",
       " ['0256', ' 0257'],\n",
       " ['0256', ' 0257'],\n",
       " ['1980', ' 1451', ' 0622                      '],\n",
       " ['0622', ' 1740', ' 3125                      '],\n",
       " ['0725', ' 1213', ' 0863                     '],\n",
       " ['0256', ' 0257', ' 3179                     '],\n",
       " ['0256', ' 0257', ' 3057                      '],\n",
       " ['0256', ' 0257', ' 3179', ' 3057                        '],\n",
       " ['0863', ' 3463'],\n",
       " ['0801', ' 2721', ' 1214                                '],\n",
       " ['0725', ' 1213', ' 0863', ' 0622                        '],\n",
       " ['0256', ' 0257', ' 3179', ' 3057                        '],\n",
       " ['0622', ' 1740', ' 3125', ' 2722', ' 1451                  '],\n",
       " ['0622', ' 1740', ' 3125', ' 2722', ' 1451                  '],\n",
       " ['0725', ' 1213', ' 0863', ' 0622', ' 1740', ' 3125            '],\n",
       " ['3179', ' 3180', ' 3057', ' 3125', ' 0725                    '],\n",
       " ['0801', ' 2721', ' 1214', ' 0863', ' 3463                   '],\n",
       " ['0622', ' 1740', ' 3125', ' 2722', ' 1451', ' 0863            '],\n",
       " ['0256', ' 0257', ' 3179', ' 3057', ' 0725', ' 1213            '],\n",
       " ['0622', ' 1740', ' 3125', ' 2722', ' 1451', ' 0863', ' 3463       '],\n",
       " ['0801', ' 2721', ' 1214', ' 0863', ' 3463', ' 0622', ' 1740        '],\n",
       " ['0725', ' 1213', ' 0863', ' 0622', ' 1740', ' 3125', ' 2722        '],\n",
       " ['0256', ' 0257', ' 3179', ' 3057', ' 0725', ' 1213', ' 0863        '],\n",
       " ['0622', ' 1740', ' 3125', ' 2722', ' 1451', ' 0863', ' 3463', ' 0725  '],\n",
       " ['0256', ' 0257', ' 3179', ' 3057', ' 0725', ' 1213', ' 0863', ' 3463  '],\n",
       " ['0256', ' 0257', ' 3057                              '],\n",
       " ['0622', ' 1740', ' 3125', ' 2722                       '],\n",
       " ['0725', ' 1213', ' 0863                             '],\n",
       " ['0256', ' 0257', ' 3179', ' 3057', ' 0725                  '],\n",
       " ['0622', ' 1740', ' 3125', ' 2722', ' 1451', ' 0863            '],\n",
       " ['0256', ' 0257', ' 3179', ' 3057', ' 0725', ' 1213', ' 0863      '],\n",
       " ['0801', ' 2721', ' 1214                              '],\n",
       " ['0725', ' 1213', ' 0863', ' 0622', ' 1740', ' 3125', ' 2722       '],\n",
       " ['0256', ' 0257', ' 3179', ' 3057', ' 0725', ' 1213', ' 0863      '],\n",
       " ['0622', ' 1740', ' 3125', ' 2722', ' 1451', ' 0863', ' 3463     '],\n",
       " ['0256', ' 0257', ' 3179', ' 3057', ' 0725', ' 1213', ' 0863', ' 3463'],\n",
       " ['0801', ' 2721', ' 1214', ' 0863', ' 3463                  '],\n",
       " ['0725', ' 1213', ' 0863', ' 0622', ' 1740', ' 3125', ' 2722', ' 1451  '],\n",
       " ['0622',\n",
       "  ' 1740',\n",
       "  ' 3125',\n",
       "  ' 2722',\n",
       "  ' 1451',\n",
       "  ' 0863',\n",
       "  ' 3463',\n",
       "  ' 0725',\n",
       "  ' 1213      '],\n",
       " ['0256',\n",
       "  ' 0257',\n",
       "  ' 3179',\n",
       "  ' 3057',\n",
       "  ' 0725',\n",
       "  ' 1213',\n",
       "  ' 0863',\n",
       "  ' 3463          '],\n",
       " ['0801', ' 2721', ' 1214', ' 0863', ' 3463                              '],\n",
       " ['0725',\n",
       "  ' 1213',\n",
       "  ' 0863',\n",
       "  ' 0622',\n",
       "  ' 1740',\n",
       "  ' 3125',\n",
       "  ' 2722',\n",
       "  ' 1451',\n",
       "  ' 3463      '],\n",
       " ['0256',\n",
       "  ' 0257',\n",
       "  ' 3179',\n",
       "  ' 3057',\n",
       "  ' 0725',\n",
       "  ' 1213',\n",
       "  ' 0863',\n",
       "  ' 3463',\n",
       "  ' 0622     '],\n",
       " ['0622',\n",
       "  ' 1740',\n",
       "  ' 3125',\n",
       "  ' 2722',\n",
       "  ' 1451',\n",
       "  ' 0863',\n",
       "  ' 3463',\n",
       "  ' 0725           '],\n",
       " ['0256',\n",
       "  ' 0257',\n",
       "  ' 3179',\n",
       "  ' 3057',\n",
       "  ' 0725',\n",
       "  ' 1213',\n",
       "  ' 0863',\n",
       "  ' 3463',\n",
       "  ' 0622',\n",
       "  ' 1740'],\n",
       " ['0801', ' 2721', ' 1214', ' 0863', ' 3463', ' 0622', ' 1740               '],\n",
       " ['0725',\n",
       "  ' 1213',\n",
       "  ' 0863',\n",
       "  ' 0622',\n",
       "  ' 1740',\n",
       "  ' 3125',\n",
       "  ' 2722',\n",
       "  ' 1451',\n",
       "  ' 3463   '],\n",
       " ['0256',\n",
       "  ' 0257',\n",
       "  ' 3179',\n",
       "  ' 3057',\n",
       "  ' 0725',\n",
       "  ' 1213',\n",
       "  ' 0863',\n",
       "  ' 3463',\n",
       "  ' 0622  '],\n",
       " ['0622',\n",
       "  ' 1740',\n",
       "  ' 3125',\n",
       "  ' 2722',\n",
       "  ' 1451',\n",
       "  ' 0863',\n",
       "  ' 3463',\n",
       "  ' 0725',\n",
       "  ' 1213 '],\n",
       " ['0256',\n",
       "  ' 0257',\n",
       "  ' 3179',\n",
       "  ' 3057',\n",
       "  ' 0725',\n",
       "  ' 1213',\n",
       "  ' 0863',\n",
       "  ' 3463',\n",
       "  ' 0622',\n",
       "  ' 1740',\n",
       "  ' 3125'],\n",
       " ['0622', ' 1740', ' 3125', ' 2722                       '],\n",
       " ['0725', ' 1213', ' 0863                             '],\n",
       " ['0256', ' 0257', ' 3179', ' 3057', ' 0725                  '],\n",
       " ['0622', ' 1740', ' 3125', ' 2722', ' 1451', ' 0863            '],\n",
       " ['0256', ' 0257', ' 3179', ' 3057', ' 0725', ' 1213', ' 0863      '],\n",
       " ['0801', ' 2721', ' 1214                              '],\n",
       " ['0725', ' 1213', ' 0863', ' 0622', ' 1740', ' 3125', ' 2722       '],\n",
       " ['0256', ' 0257', ' 3179', ' 3057', ' 0725', ' 1213', ' 0863      '],\n",
       " ['0622', ' 1740', ' 3125', ' 2722', ' 1451', ' 0863', ' 3463     '],\n",
       " ['0256', ' 0257', ' 3179', ' 3057', ' 0725', ' 1213', ' 0863', ' 3463'],\n",
       " ['0801', ' 2721', ' 1214', ' 0863', ' 3463                  '],\n",
       " ['0725', ' 1213', ' 0863', ' 0622', ' 1740', ' 3125', ' 2722', ' 1451  '],\n",
       " ['0622',\n",
       "  ' 1740',\n",
       "  ' 3125',\n",
       "  ' 2722',\n",
       "  ' 1451',\n",
       "  ' 0863',\n",
       "  ' 3463',\n",
       "  ' 0725',\n",
       "  ' 1213      '],\n",
       " ['0256',\n",
       "  ' 0257',\n",
       "  ' 3179',\n",
       "  ' 3057',\n",
       "  ' 0725',\n",
       "  ' 1213',\n",
       "  ' 0863',\n",
       "  ' 3463          '],\n",
       " ['0801', ' 2721', ' 1214', ' 0863', ' 3463                              '],\n",
       " ['0725',\n",
       "  ' 1213',\n",
       "  ' 0863',\n",
       "  ' 0622',\n",
       "  ' 1740',\n",
       "  ' 3125',\n",
       "  ' 2722',\n",
       "  ' 1451',\n",
       "  ' 3463      '],\n",
       " ['0256',\n",
       "  ' 0257',\n",
       "  ' 3179',\n",
       "  ' 3057',\n",
       "  ' 0725',\n",
       "  ' 1213',\n",
       "  ' 0863',\n",
       "  ' 3463',\n",
       "  ' 0622     '],\n",
       " ['0622',\n",
       "  ' 1740',\n",
       "  ' 3125',\n",
       "  ' 2722',\n",
       "  ' 1451',\n",
       "  ' 0863',\n",
       "  ' 3463',\n",
       "  ' 0725           '],\n",
       " ['0256',\n",
       "  ' 0257',\n",
       "  ' 3179',\n",
       "  ' 3057',\n",
       "  ' 0725',\n",
       "  ' 1213',\n",
       "  ' 0863',\n",
       "  ' 3463',\n",
       "  ' 0622',\n",
       "  ' 1740'],\n",
       " ['0801', ' 2721', ' 1214', ' 0863', ' 3463', ' 0622', ' 1740               '],\n",
       " ['0256', ' 0257', ' 0725', ' 1213', ' 3463', ' 0622'],\n",
       " ['0622', ' 1740', ' 3463', ' 0725'],\n",
       " ['0256', ' 0257', ' 0725', ' 1213', ' 3463', ' 0622', ' 1740'],\n",
       " ['0801', ' 2721', ' 1214', ' 3463', ' 0622', ' 1740'],\n",
       " ['1740', ' 0622', ' 3463', ' 0725', ' 0256', ' 0257     '],\n",
       " ['0256', ' 0257', ' 0725', ' 1213', ' 3463', ' 0622', ' 1740'],\n",
       " ['0801', ' 2721', ' 1214', ' 3463', ' 0622', ' 1740'],\n",
       " ['1740', ' 0622', ' 3463', ' 0725                        '],\n",
       " ['1740', ' 0622', ' 3463', ' 0725', ' 0256', ' 0257'],\n",
       " ['0004', ' 0356', ' 0633', ' 1989'],\n",
       " ['0033', ' 0510', ' 1568', ' 3151'],\n",
       " ['0426', ' 0831', ' 1940', ' 3091'],\n",
       " ['0513', ' 0853', ' 2047', ' 3507'],\n",
       " ['0767', ' 0884', ' 1972', ' 3344'],\n",
       " ['0725', ' 1078', ' 2387', ' 3542'],\n",
       " ['0142', ' 0625', ' 1557', ' 2902'],\n",
       " ['0320', ' 0727', ' 1346', ' 2763'],\n",
       " ['0021', ' 0435', ' 0936', ' 1865'],\n",
       " ['0115', ' 0542', ' 1278', ' 2590'],\n",
       " ['0230', ' 0654', ' 1423', ' 3078'],\n",
       " ['0725', ' 1078', ' 2387', ' 3542'],\n",
       " ['0078', ' 0352', ' 0749', ' 1235', ' 1956', ' 2910'],\n",
       " ['0566', ' 0932', ' 1544', ' 2298', ' 3011'],\n",
       " ['0288', ' 0667', ' 1329', ' 1998', ' 2672'],\n",
       " ['0428', ' 0897', ' 1567', ' 2237', ' 2987'],\n",
       " ['0346', ' 0735', ' 1310', ' 2169', ' 3099'],\n",
       " ['0087', ' 0366', ' 0712', ' 1598'],\n",
       " ['0087', ' 0366', ' 0712', ' 1598'],\n",
       " ['0432', ' 0855', ' 1763', ' 3124', ' 3956'],\n",
       " ['0187', ' 0543', ' 1298'],\n",
       " ['0046', ' 0431', ' 0879', ' 1680', ' 2875'],\n",
       " ['0025', ' 0654', ' 1432', ' 2769'],\n",
       " ['0143', ' 0388', ' 0726', ' 1497', ' 2587'],\n",
       " ['0222', ' 0597', ' 1276'],\n",
       " ['0058', ' 0487', ' 0962', ' 1890', ' 3017'],\n",
       " ['0310', ' 0788', ' 1587', ' 2990'],\n",
       " ['0111', ' 0319', ' 0667', ' 1348', ' 2456'],\n",
       " ['0232', ' 0599', ' 1244', ' 2675'],\n",
       " ['0041', ' 0499', ' 1056'],\n",
       " ['0342', ' 0715', ' 1423', ' 2816'],\n",
       " ['0134', ' 0378', ' 0782', ' 1532', ' 2667'],\n",
       " ['0012', ' 0548', ' 1267', ' 2435'],\n",
       " ['0097', ' 0466', ' 0935', ' 1754', ' 2923'],\n",
       " ['0027', ' 0564', ' 1329'],\n",
       " ['0075', ' 0628', ' 1295', ' 2477', ' 3592'],\n",
       " ['0123', ' 0456', ' 0944', ' 1825'],\n",
       " ['0290', ' 0666', ' 1335', ' 2787'],\n",
       " ['0234', ' 0548', ' 1123', ' 2475'],\n",
       " ['0234', ' 0548', ' 1123', ' 2475'],\n",
       " ['0111', ' 0498', ' 0976', ' 1893', ' 3087'],\n",
       " ['0352', ' 0729', ' 1367', ' 2810'],\n",
       " ['0038', ' 0650', ' 1247', ' 2522'],\n",
       " ['0456', ' 0932', ' 1739', ' 3327'],\n",
       " ['0612', ' 1057', ' 2145', ' 3546'],\n",
       " ['0566', ' 0932', ' 1544', ' 2298', ' 3011'],\n",
       " ['0288', ' 0667', ' 1329', ' 1998', ' 2672'],\n",
       " ['0428', ' 0897', ' 1567', ' 2237', ' 2987'],\n",
       " ['0346', ' 0735', ' 1310', ' 2169', ' 3099'],\n",
       " ['0087', ' 0366', ' 0712', ' 1598'],\n",
       " ['0087', ' 0366', ' 0712', ' 1598'],\n",
       " ['0432', ' 0855', ' 1763', ' 3124', ' 3956'],\n",
       " ['0187', ' 0543', ' 1298'],\n",
       " ['0046', ' 0431', ' 0879', ' 1680', ' 2875'],\n",
       " ['0025', ' 0654', ' 1432', ' 2769'],\n",
       " ['0143', ' 0388', ' 0726', ' 1497', ' 2587'],\n",
       " ['0222', ' 0597', ' 1276'],\n",
       " ['0058', ' 0487', ' 0962', ' 1890', ' 3017'],\n",
       " ['0310', ' 0788', ' 1587', ' 2990'],\n",
       " ['0111', ' 0319', ' 0667', ' 1348', ' 2456'],\n",
       " ['0232', ' 0599', ' 1244', ' 2675'],\n",
       " ['0041', ' 0499', ' 1056'],\n",
       " ['0342', ' 0715', ' 1423', ' 2816'],\n",
       " ['0134', ' 0378', ' 0782', ' 1532', ' 2667'],\n",
       " ['0012', ' 0548', ' 1267', ' 2435'],\n",
       " ['0097', ' 0466', ' 0935', ' 1754', ' 2923'],\n",
       " ['0027', ' 0564', ' 1329'],\n",
       " ['0075', ' 0628', ' 1295', ' 2477', ' 3592'],\n",
       " ['0123', ' 0456', ' 0944', ' 1825'],\n",
       " ['0290', ' 0666', ' 1335', ' 2787'],\n",
       " ['0234', ' 0548', ' 1123', ' 2475'],\n",
       " ['0234', ' 0548', ' 1123', ' 2475'],\n",
       " ['0111', ' 0498', ' 0976', ' 1893', ' 3087'],\n",
       " ['0352', ' 0729', ' 1367', ' 2810'],\n",
       " ['0038', ' 0650', ' 1247', ' 2522'],\n",
       " ['0456', ' 0932', ' 1739', ' 3327'],\n",
       " ['0612', ' 1057', ' 2145', ' 3546'],\n",
       " ['0687', ' 0355', ' 1677'],\n",
       " ['3352', ' 3369', ' 3399'],\n",
       " ['3361', ' 3369', ' 3399'],\n",
       " ['2144', ' 2202', ' 2215', ' 2306', ' 2353', ' 2401', ' 2402'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['0385', ' 0992', ' 0670', ' 3469'],\n",
       " ['0687', ' 0355', ' 1677'],\n",
       " ['1677', ' 2924', ' 1755', ' 2754', ' 2570', ' 2497', ' 1838'],\n",
       " ['3361',\n",
       "  ' 3369',\n",
       "  ' 3370',\n",
       "  ' 3465',\n",
       "  ' 3472',\n",
       "  ' 3475',\n",
       "  ' 3483',\n",
       "  ' 3491',\n",
       "  ' 3503'],\n",
       " ['1677', ' 2924', ' 1755', ' 2754', ' 2570', ' 2497', ' 1838'],\n",
       " ['0112', ' 0556', ' 1043', ' 2045'],\n",
       " ['0027', ' 0478', ' 0923', ' 1776', ' 2989'],\n",
       " ['0336', ' 0669', ' 1224'],\n",
       " ['0098', ' 0345', ' 0689', ' 1512', ' 2655'],\n",
       " ['0263', ' 0587', ' 1134'],\n",
       " ['0015', ' 0445', ' 0849', ' 1627'],\n",
       " ['0202', ' 0755', ' 1347', ' 2764'],\n",
       " ['0046', ' 0498', ' 0975', ' 1987', ' 3167'],\n",
       " ['0301', ' 0674', ' 1211', ' 2298'],\n",
       " ['0029', ' 0638', ' 1123'],\n",
       " ['0056', ' 0537', ' 0921', ' 1698', ' 2987'],\n",
       " ['0567', ' 1289', ' 2057', ' 0568', ' 2862'],\n",
       " ['0567', ' 1289'],\n",
       " ['0567', ' 1289'],\n",
       " ['0567', ' 1289', ' 2057'],\n",
       " ['0566', ' 0932', ' 1544', ' 2298', ' 3011'],\n",
       " ['0288', ' 0667', ' 1329', ' 1998', ' 2672'],\n",
       " ['0428', ' 0897', ' 1567', ' 2237', ' 2987'],\n",
       " ['0346', ' 0735', ' 1310', ' 2169', ' 3099'],\n",
       " ['0087', ' 0366', ' 0712', ' 1598'],\n",
       " ['0087', ' 0366', ' 0712', ' 1598'],\n",
       " ['0432', ' 0855', ' 1763', ' 3124', ' 3956'],\n",
       " ['0187', ' 0543', ' 1298'],\n",
       " ['0046', ' 0431', ' 0879', ' 1680', ' 2875'],\n",
       " ['0025', ' 0654', ' 1432', ' 2769'],\n",
       " ['0143', ' 0388', ' 0726', ' 1497', ' 2587'],\n",
       " ['0222', ' 0597', ' 1276'],\n",
       " ['0058', ' 0487', ' 0962', ' 1890', ' 3017'],\n",
       " ['0310', ' 0788', ' 1587', ' 2990'],\n",
       " ['0111', ' 0319', ' 0667', ' 1348', ' 2456'],\n",
       " ['0232', ' 0599', ' 1244', ' 2675'],\n",
       " ['0041', ' 0499', ' 1056'],\n",
       " ['0342', ' 0715', ' 1423', ' 2816'],\n",
       " ['0134', ' 0378', ' 0782', ' 1532', ' 2667'],\n",
       " ['0012', ' 0548', ' 1267', ' 2435'],\n",
       " ['0097', ' 0466', ' 0935', ' 1754', ' 2923'],\n",
       " ['0027', ' 0564', ' 1329'],\n",
       " ['0075', ' 0628', ' 1295', ' 2477', ' 3592'],\n",
       " ['0123', ' 0456', ' 0944', ' 1825'],\n",
       " ['0290', ' 0666', ' 1335', ' 2787'],\n",
       " ['0234', ' 0548', ' 1123', ' 2475'],\n",
       " ['0234', ' 0548', ' 1123', ' 2475'],\n",
       " ['0111', ' 0498', ' 0976', ' 1893', ' 3087'],\n",
       " ['0352', ' 0729', ' 1367', ' 2810'],\n",
       " ['0038', ' 0650', ' 1247', ' 2522'],\n",
       " ['0456', ' 0932', ' 1739', ' 3327'],\n",
       " ['0612', ' 1057', ' 2145', ' 3546'],\n",
       " ['0780', ' 0966', ' 2754'],\n",
       " ['2754', ' 1677', ' 2611'],\n",
       " ['0966', ' 1677', ' 1109'],\n",
       " ['0780', ' 0966', ' 2754', ' 1677', ' 2116', ' 0687', ' 0816'],\n",
       " ['0687', ' 0816'],\n",
       " ['1677', ' 1109', ' 2497'],\n",
       " ['1677', ' 1109', ' 2497'],\n",
       " ['2558', ' 2924', ' 3334', ' 1801'],\n",
       " ['1350', ' 3472', ' 2558', ' 2924', ' 3334', ' 1801'],\n",
       " ['0780', ' 0966', ' 2754'],\n",
       " ['2754', ' 1677', ' 2611'],\n",
       " ['0966', ' 1677', ' 1109'],\n",
       " ['0780', ' 0966', ' 2754', ' 1677', ' 2116', ' 0687', ' 0816'],\n",
       " ['0687', ' 0816'],\n",
       " ['1677', ' 1109', ' 2497'],\n",
       " ['1677', ' 1109', ' 2497'],\n",
       " ['2558', ' 2924', ' 3334', ' 1801'],\n",
       " ['1350', ' 3472', ' 2558', ' 2924', ' 3334', ' 1801'],\n",
       " ['1677', ' 2611', ' 1109', ' 2497'],\n",
       " ['1677', ' 2611', ' 1109'],\n",
       " ['Data analysis'],\n",
       " ['0567', ' 1289', ' 2057', ' 2708', ' 2359', ' 1888', ' 2087'],\n",
       " ['0567', ' 1289', ' 2057'],\n",
       " ['2359', ' 2919', ' 1888', ' 0555'],\n",
       " ['2057', ' 0567', ' 1289', ' 0014', ' 2708', ' 0985', ' 0603'],\n",
       " ['2057', ' 0567', ' 1289', ' 0985', ' 0603'],\n",
       " ['2057', ' 0567', ' 1289', ' 3503', ' 1370'],\n",
       " ['0567', ' 1289', ' 2708', ' 1956', ' 1158', ' 1544', ' 0555'],\n",
       " ['2075', ' 1289', ' 1218'],\n",
       " ['2075', ' 1289', ' 1218'],\n",
       " ['2708', ' 1956', ' 1158', ' 1544', ' 0555'],\n",
       " ['0567', ' 1289', ' 0014'],\n",
       " ['2116', ' 0687', ' 0816'],\n",
       " ['2558', ' 2924', ' 1801'],\n",
       " ['0780', ' 0966', ' 2754'],\n",
       " ['2754', ' 1677', ' 2611'],\n",
       " ['0966', ' 1677', ' 1109'],\n",
       " ['0780', ' 0966', ' 2754', ' 1677', ' 2116', ' 0687', ' 0816'],\n",
       " ['0687', ' 0816'],\n",
       " ['1677', ' 1109', ' 2497'],\n",
       " ['1677', ' 1109', ' 2497'],\n",
       " ['2558', ' 2924', ' 3334', ' 1801'],\n",
       " ['1350', ' 3472', ' 2558', ' 2924', ' 3334', ' 1801', ' 0992'],\n",
       " ['1677', ' 2611', ' 1109', ' 2497'],\n",
       " ['1677', ' 2611', ' 1109'],\n",
       " ['Data analysis'],\n",
       " ['0567', ' 1289', ' 2057', ' 2708', ' 2359', ' 1888', ' 2087'],\n",
       " ['0567', ' 1289', ' 2057'],\n",
       " ['2359', ' 2919', ' 1888', ' 0555'],\n",
       " ['2057', ' 0567', ' 1289', ' 0014', ' 2708', ' 0985', ' 0603'],\n",
       " ['2057', ' 0567', ' 1289', ' 0985', ' 0603'],\n",
       " ['2057', ' 0567', ' 1289', ' 3503', ' 1370'],\n",
       " ['0567', ' 1289', ' 2708', ' 1956', ' 1158', ' 1544', ' 0555'],\n",
       " ['2075', ' 1289', ' 1218'],\n",
       " ['2075', ' 1289', ' 1218'],\n",
       " ['2708', ' 1956', ' 1158', ' 1544', ' 0555'],\n",
       " ['0567', ' 1289', ' 0014'],\n",
       " ['2116', ' 0687', ' 0816'],\n",
       " ['2558', ' 2924', ' 1801'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599', ' 1496'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599'],\n",
       " ['0567', ' 1289', ' 0014'],\n",
       " ['2116', ' 0687', ' 0816'],\n",
       " ['2558', ' 2924', ' 1801'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599', ' 1496'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599'],\n",
       " ['0567', ' 1289', ' 0014'],\n",
       " ['2116', ' 0687', ' 0816'],\n",
       " ['2558', ' 2924', ' 0992'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599', ' 1496'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599'],\n",
       " ['0567', ' 1289', ' 0014'],\n",
       " ['2116', ' 0687', ' 0816'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['0027', ' 0223', ' 0263', ' 0363', ' 0414', ' 0431'],\n",
       " ['0016', ' 0022', ' 0053', ' 0067', ' 0082'],\n",
       " ['2024', ' 2085', ' 2717'],\n",
       " ['0391', ' 1808', ' 3454'],\n",
       " ['1027', ' 1086', ' 1124', ' 1166', ' 1246'],\n",
       " ['0055', ' 0072', ' 0081', ' 0101', ' 0141'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['0193', ' 0270', ' 0440', ' 0474', ' 0494'],\n",
       " ['0736', ' 0860', ' 0954', ' 0983', ' 0996'],\n",
       " ['2679', ' 2680', ' 2687', ' 2688'],\n",
       " ['3007', ' 3013', ' 3034'],\n",
       " ['0120', ' 0286', ' 2712'],\n",
       " ['2803', ' 2905', ' 2912', ' 2916'],\n",
       " ['1227', ' 1723', ' 3075'],\n",
       " ['0840', ' 1183', ' 1188', ' 1233'],\n",
       " ['0954', ' 0965', ' 0990', ' 0999', ' 1004', ' 1049'],\n",
       " ['0882', ' 0922', ' 0939', ' 0983', ' 1088', ' 1105'],\n",
       " ['1183', ' 1345', ' 1346', ' 1394', ' 1404', ' 1408', ' 1425'],\n",
       " ['1983', ' 2009', ' 2020', ' 2051', ' 2094', ' 2121', ' 2142', ' 2150'],\n",
       " ['2245', ' 2262', ' 2271', ' 2326', ' 2364', ' 2369', ' 2385'],\n",
       " ['2419', ' 2422', ' 2430', ' 2455', ' 2463'],\n",
       " ['2636', ' 2679', ' 2680', ' 2687', ' 2688'],\n",
       " ['2761', ' 2778', ' 2785', ' 2798', ' 2799', ' 2819'],\n",
       " ['3066', ' 3081', ' 3104', ' 3106', ' 3124', ' 3132', ' 3155', ' 3156'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['0200', ' 0261', ' 0264', ' 0378', ' 0410', ' 0443'],\n",
       " ['1480', ' 1541', ' 1607', ' 1651', ' 1658'],\n",
       " ['0427', ' 0465', ' 0650', ' 0668', ' 0674'],\n",
       " ['0248', ' 0270', ' 0277', ' 0280', ' 0336'],\n",
       " ['2907', ' 2965', ' 2989', ' 3000', ' 3016'],\n",
       " ['0970', ' 1201', ' 1286', ' 1507', ' 1845', ' 2341'],\n",
       " ['3034', ' 3077', ' 3104', ' 3106', ' 3124'],\n",
       " ['2641', ' 2732', ' 2761', ' 2778', ' 2785'],\n",
       " ['3075', ' 3286', ' 3296', ' 3355', ' 3443'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['0419', ' 0429', ' 0440', ' 0447', ' 0474'],\n",
       " ['0869', ' 0872', ' 0882', ' 0922', ' 0939'],\n",
       " ['0954', ' 0983', ' 0996', ' 0998', ' 1049'],\n",
       " ['1088', ' 1105', ' 1113', ' 1164', ' 1168'],\n",
       " ['1182', ' 1195', ' 1207', ' 1311', ' 1354'],\n",
       " ['1360', ' 1383', ' 1394', ' 1404', ' 1408'],\n",
       " ['1425', ' 1429', ' 1455', ' 1488', ' 1500'],\n",
       " ['1556', ' 1557', ' 1559', ' 1566', ' 1590'],\n",
       " ['1607', ' 1613', ' 1618', ' 1623', ' 1651'],\n",
       " ['1658', ' 1695', ' 1696', ' 1700', ' 1869'],\n",
       " ['1900', ' 1906', ' 1943', ' 1983', ' 2009'],\n",
       " ['2027', ' 2034', ' 2044', ' 2053', ' 2054'],\n",
       " ['2077', ' 2124', ' 2135', ' 2136', ' 2147'],\n",
       " ['2216', ' 2225', ' 2227', ' 2268', ' 2336'],\n",
       " ['2339', ' 2348', ' 2380', ' 2413', ' 2508'],\n",
       " ['2540', ' 2555', ' 2586', ' 2713', ' 2732'],\n",
       " ['2761', ' 2778', ' 2785', ' 2794', ' 2799'],\n",
       " ['2819', ' 2837', ' 2847', ' 2856', ' 2857'],\n",
       " ['2880', ' 2882', ' 2901', ' 2906', ' 2907'],\n",
       " ['2910', ' 2913', ' 2927', ' 2961', ' 2972'],\n",
       " ['2975', ' 2989', ' 3000', ' 3016', ' 3018'],\n",
       " ['3034', ' 3041', ' 3061', ' 3075', ' 3077'],\n",
       " ['3081', ' 3103', ' 3104', ' 3106', ' 3124'],\n",
       " ['0016', ' 0022', ' 0053', ' 0067', ' 0082'],\n",
       " ['3208', ' 3296', ' 3355', ' 3351', ' 3443'],\n",
       " ['0016', ' 0022', ' 0053', ' 0067', ' 0082'],\n",
       " ['0860', ' 0995', ' 1071', ' 1072', ' 1126'],\n",
       " ['1854', ' 1860', ' 1879', ' 1930', ' 2020'],\n",
       " ['0609', ' 0620', ' 0638', ' 0679', ' 0682'],\n",
       " ['0740', ' 0823', ' 0872', ' 0882', ' 0922'],\n",
       " ['0954', ' 0983', ' 0996', ' 0998', ' 1049'],\n",
       " ['1164', ' 1168', ' 1182', ' 1195', ' 1207'],\n",
       " ['1311', ' 1354', ' 1360', ' 1510', ' 1562'],\n",
       " ['1634', ' 1646', ' 1672', ' 1674', ' 1719'],\n",
       " ['1773', ' 1802', ' 1810', ' 1817', ' 1847'],\n",
       " ['1854', ' 1860', ' 1879', ' 1930', ' 2020'],\n",
       " ['1906', ' 1943', ' 1983', ' 2009', ' 2027'],\n",
       " ['2034', ' 2044', ' 2053', ' 2054', ' 2077'],\n",
       " ['2124', ' 2135', ' 2136', ' 2147', ' 2216'],\n",
       " ['2225', ' 2227', ' 2268', ' 2336', ' 2339'],\n",
       " ['2348', ' 2380', ' 2413', ' 2508', ' 2540'],\n",
       " ['2555', ' 2586', ' 2713', ' 2732', ' 2761'],\n",
       " ['2555', ' 2586', ' 2713', ' 2732', ' 2761'],\n",
       " ['3208', ' 3296', ' 3355', ' 3351', ' 3443'],\n",
       " ['0016', ' 0022', ' 0053', ' 0067', ' 0082'],\n",
       " ['0860', ' 0995', ' 1071', ' 1072', ' 1126'],\n",
       " ['1854', ' 1860', ' 1879', ' 1930', ' 2020'],\n",
       " ['0609', ' 0620', ' 0638', ' 0679', ' 0682'],\n",
       " ['0740', ' 0823', ' 0872', ' 0882', ' 0922'],\n",
       " ['0954', ' 0983', ' 0996', ' 0998', ' 1049'],\n",
       " ['1164', ' 1168', ' 1182', ' 1195', ' 1207'],\n",
       " ['1311', ' 1354', ' 1360', ' 1510', ' 1562'],\n",
       " ['1634', ' 1646', ' 1672', ' 1674', ' 1719'],\n",
       " ['1773', ' 1802', ' 1810', ' 1817', ' 1847'],\n",
       " ['1854', ' 1860', ' 1879', ' 1930', ' 2020'],\n",
       " ['1906', ' 1943', ' 1983', ' 2009', ' 2027'],\n",
       " ['2034', ' 2044', ' 2053', ' 2054', ' 2077'],\n",
       " ['2124', ' 2135', ' 2136', ' 2147', ' 2216'],\n",
       " ['2225', ' 2227', ' 2268', ' 2336', ' 2339'],\n",
       " ['2348', ' 2380', ' 2413', ' 2508', ' 2540'],\n",
       " ['2555', ' 2586', ' 2713', ' 2732', ' 2761'],\n",
       " ['2778', ' 2785', ' 2794', ' 2799', ' 2801'],\n",
       " ['2827', ' 2841', ' 2855', ' 2863', ' 2907'],\n",
       " ['2965', ' 2989', ' 3000', ' 3016', ' 3034'],\n",
       " ['3075', ' 3081', ' 3083', ' 3103', ' 3104'],\n",
       " ['3106', ' 3124', ' 3155', ' 3156', ' 3208'],\n",
       " ['3106', ' 3124', ' 3155', ' 3156', ' 3208'],\n",
       " ['1267', ' 1356', ' 1421', ' 1497', ' 1549'],\n",
       " ['0620', ' 0679', ' 0682', ' 0696', ' 0703'],\n",
       " ['0681', ' 0700', ' 0760', ' 0783', ' 0862'],\n",
       " ['0870', ' 0942', ' 0947', ' 0965', ' 0990'],\n",
       " ['1004', ' 1028', ' 1060', ' 1071', ' 1072'],\n",
       " ['1111', ' 1126', ' 1159', ' 1183', ' 1188'],\n",
       " ['1233', ' 1254', ' 1279', ' 1301', ' 1309'],\n",
       " ['1332', ' 1334', ' 1337', ' 1345', ' 1346'],\n",
       " ['1375', ' 1383', ' 1394', ' 1404', ' 1408'],\n",
       " ['1425', ' 1429', ' 1455', ' 1488', ' 1500'],\n",
       " ['1541', ' 1556', ' 1557', ' 1559', ' 1566'],\n",
       " ['1590', ' 1607', ' 1613', ' 1618', ' 1623'],\n",
       " ['1651', ' 1658', ' 1695', ' 1696', ' 1700'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['0027', ' 0223', ' 0263', ' 0363', ' 0414', ' 0431'],\n",
       " ['0016', ' 0022', ' 0053', ' 0067', ' 0082'],\n",
       " ['2024', ' 2085', ' 2717'],\n",
       " ['0391', ' 1808', ' 3454'],\n",
       " ['1027', ' 1086', ' 1124', ' 1166', ' 1246'],\n",
       " ['0055', ' 0072', ' 0081', ' 0101', ' 0141'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['0193', ' 0270', ' 0440', ' 0474', ' 0494'],\n",
       " ['0736', ' 0860', ' 0954', ' 0983', ' 0996'],\n",
       " ['2679', ' 2680', ' 2687', ' 2688'],\n",
       " ['3007', ' 3013', ' 3034'],\n",
       " ['0120', ' 0286', ' 2712'],\n",
       " ['2803', ' 2905', ' 2912', ' 2916'],\n",
       " ['1227', ' 1723', ' 3075'],\n",
       " ['0840', ' 1183', ' 1188', ' 1233'],\n",
       " ['0954', ' 0965', ' 0990', ' 0999', ' 1004', ' 1049'],\n",
       " ['0882', ' 0922', ' 0939', ' 0983', ' 1088', ' 1105'],\n",
       " ['1183', ' 1345', ' 1346', ' 1394', ' 1404', ' 1408', ' 1425'],\n",
       " ['1983', ' 2009', ' 2020', ' 2051', ' 2094', ' 2121', ' 2142', ' 2150'],\n",
       " ['2245', ' 2262', ' 2271', ' 2326', ' 2364', ' 2369', ' 2385'],\n",
       " ['2419', ' 2422', ' 2430', ' 2455', ' 2463'],\n",
       " ['2636', ' 2679', ' 2680', ' 2687', ' 2688'],\n",
       " ['2761', ' 2778', ' 2785', ' 2798', ' 2799', ' 2819'],\n",
       " ['3066', ' 3081', ' 3104', ' 3106', ' 3124', ' 3132', ' 3155', ' 3156'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['0200', ' 0261', ' 0264', ' 0378', ' 0410', ' 0443'],\n",
       " ['1480', ' 1541', ' 1607', ' 1651', ' 1658'],\n",
       " ['0427', ' 0465', ' 0650', ' 0668', ' 0674'],\n",
       " ['0248', ' 0270', ' 0277', ' 0280', ' 0336'],\n",
       " ['2907', ' 2965', ' 2989', ' 3000', ' 3016'],\n",
       " ['0970', ' 1201', ' 1286', ' 1507', ' 1845', ' 2341'],\n",
       " ['3034', ' 3077', ' 3104', ' 3106', ' 3124'],\n",
       " ['2641', ' 2732', ' 2761', ' 2778', ' 2785'],\n",
       " ['3075', ' 3286', ' 3296', ' 3355', ' 3443'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['0419', ' 0429', ' 0440', ' 0447', ' 0474'],\n",
       " ['0869', ' 0872', ' 0882', ' 0922', ' 0939'],\n",
       " ['0954', ' 0983', ' 0996', ' 0998', ' 1049'],\n",
       " ['1088', ' 1105', ' 1113', ' 1164', ' 1168'],\n",
       " ['1182', ' 1195', ' 1207', ' 1311', ' 1354'],\n",
       " ['1360', ' 1383', ' 1394', ' 1404', ' 1408'],\n",
       " ['1425', ' 1429', ' 1455', ' 1488', ' 1500'],\n",
       " ['1556', ' 1557', ' 1559', ' 1566', ' 1590'],\n",
       " ['1607', ' 1613', ' 1618', ' 1623', ' 1651'],\n",
       " ['1658', ' 1695', ' 1696', ' 1700', ' 1869'],\n",
       " ['1900', ' 1906', ' 1943', ' 1983', ' 2009'],\n",
       " ['2027', ' 2034', ' 2044', ' 2053', ' 2054'],\n",
       " ['2077', ' 2124', ' 2135', ' 2136', ' 2147'],\n",
       " ['2216', ' 2225', ' 2227', ' 2268', ' 2336'],\n",
       " ['2339', ' 2348', ' 2380', ' 2413', ' 2508'],\n",
       " ['2540', ' 2555', ' 2586', ' 2713', ' 2732'],\n",
       " ['2761', ' 2778', ' 2785', ' 2794', ' 2799'],\n",
       " ['2819', ' 2837', ' 2847', ' 2856', ' 2857'],\n",
       " ['2880', ' 2882', ' 2901', ' 2906', ' 2907'],\n",
       " ['2910', ' 2913', ' 2927', ' 2961', ' 2972'],\n",
       " ['2975', ' 2989', ' 3000', ' 3016', ' 3018'],\n",
       " ['3034', ' 3041', ' 3061', ' 3075', ' 3077'],\n",
       " ['3081', ' 3103', ' 3104', ' 3106', ' 3124'],\n",
       " ['0016', ' 0022', ' 0053', ' 0067', ' 0082'],\n",
       " ['3208', ' 3296', ' 3355', ' 3351', ' 3443'],\n",
       " ['0016', ' 0022', ' 0053', ' 0067', ' 0082'],\n",
       " ['0860', ' 0995', ' 1071', ' 1072', ' 1126'],\n",
       " ['1854', ' 1860', ' 1879', ' 1930', ' 2020'],\n",
       " ['0609', ' 0620', ' 0638', ' 0679', ' 0682'],\n",
       " ['0740', ' 0823', ' 0872', ' 0882', ' 0922'],\n",
       " ['0954', ' 0983', ' 0996', ' 0998', ' 1049'],\n",
       " ['1164', ' 1168', ' 1182', ' 1195', ' 1207'],\n",
       " ['1311', ' 1354', ' 1360', ' 1510', ' 1562'],\n",
       " ['1634', ' 1646', ' 1672', ' 1674', ' 1719'],\n",
       " ['1773', ' 1802', ' 1810', ' 1817', ' 1847'],\n",
       " ['1854', ' 1860', ' 1879', ' 1930', ' 2020'],\n",
       " ['1906', ' 1943', ' 1983', ' 2009', ' 2027'],\n",
       " ['2034', ' 2044', ' 2053', ' 2054', ' 2077'],\n",
       " ['2124', ' 2135', ' 2136', ' 2147', ' 2216'],\n",
       " ['2225', ' 2227', ' 2268', ' 2336', ' 2339'],\n",
       " ['2348', ' 2380', ' 2413', ' 2508', ' 2540'],\n",
       " ['2555', ' 2586', ' 2713', ' 2732', ' 2761'],\n",
       " ['2555', ' 2586', ' 2713', ' 2732', ' 2761'],\n",
       " ['3208', ' 3296', ' 3355', ' 3351', ' 3443'],\n",
       " ['0016', ' 0022', ' 0053', ' 0067', ' 0082'],\n",
       " ['0860', ' 0995', ' 1071', ' 1072', ' 1126'],\n",
       " ['1854', ' 1860', ' 1879', ' 1930', ' 2020'],\n",
       " ['0609', ' 0620', ' 0638', ' 0679', ' 0682'],\n",
       " ['0740', ' 0823', ' 0872', ' 0882', ' 0922'],\n",
       " ['0954', ' 0983', ' 0996', ' 0998', ' 1049'],\n",
       " ['1164', ' 1168', ' 1182', ' 1195', ' 1207'],\n",
       " ['1311', ' 1354', ' 1360', ' 1510', ' 1562'],\n",
       " ['1634', ' 1646', ' 1672', ' 1674', ' 1719'],\n",
       " ['1773', ' 1802', ' 1810', ' 1817', ' 1847'],\n",
       " ['1854', ' 1860', ' 1879', ' 1930', ' 2020'],\n",
       " ['1906', ' 1943', ' 1983', ' 2009', ' 2027'],\n",
       " ['2034', ' 2044', ' 2053', ' 2054', ' 2077'],\n",
       " ['2124', ' 2135', ' 2136', ' 2147', ' 2216'],\n",
       " ['2225', ' 2227', ' 2268', ' 2336', ' 2339'],\n",
       " ['2348', ' 2380', ' 2413', ' 2508', ' 2540'],\n",
       " ['2555', ' 2586', ' 2713', ' 2732', ' 2761'],\n",
       " ['2778', ' 2785', ' 2794', ' 2799', ' 2801'],\n",
       " ['2827', ' 2841', ' 2855', ' 2863', ' 2907'],\n",
       " ['2965', ' 2989', ' 3000', ' 3016', ' 3034'],\n",
       " ['3075', ' 3081', ' 3083', ' 3103', ' 3104'],\n",
       " ['3106', ' 3124', ' 3155', ' 3156', ' 3208'],\n",
       " ['3106', ' 3124', ' 3155', ' 3156', ' 3208'],\n",
       " ['1267', ' 1356', ' 1421', ' 1497', ' 1549'],\n",
       " ['0620', ' 0679', ' 0682', ' 0696', ' 0703'],\n",
       " ['0681', ' 0700', ' 0760', ' 0783', ' 0862'],\n",
       " ['0870', ' 0942', ' 0947', ' 0965', ' 0990'],\n",
       " ['1004', ' 1028', ' 1060', ' 1071', ' 1072'],\n",
       " ['1111', ' 1126', ' 1159', ' 1183', ' 1188'],\n",
       " ['1233', ' 1254', ' 1279', ' 1301', ' 1309'],\n",
       " ['1332', ' 1334', ' 1337', ' 1345', ' 1346'],\n",
       " ['1375', ' 1383', ' 1394', ' 1404', ' 1408'],\n",
       " ['1425', ' 1429', ' 1455', ' 1488', ' 1500'],\n",
       " ['1541', ' 1556', ' 1557', ' 1559', ' 1566'],\n",
       " ['1590', ' 1607', ' 1613', ' 1618', ' 1623'],\n",
       " ['1651', ' 1658', ' 1695', ' 1696', ' 1700'],\n",
       " ['1854', ' 1860', ' 1879', ' 1930', ' 2020'],\n",
       " ['1906', ' 1943', ' 1983', ' 2009', ' 2027'],\n",
       " ['2034', ' 2044', ' 2053', ' 2054', ' 2077'],\n",
       " ['2124', ' 2135', ' 2136', ' 2147', ' 2216'],\n",
       " ['2225', ' 2227', ' 2268', ' 2336', ' 2339'],\n",
       " ['2348', ' 2380', ' 2413', ' 2508', ' 2540'],\n",
       " ['2555', ' 2586', ' 2713', ' 2732', ' 2761'],\n",
       " ['2778', ' 2785', ' 2794', ' 2799', ' 2801'],\n",
       " ['2827', ' 2841', ' 2855', ' 2863', ' 2907'],\n",
       " ['2965', ' 2989', ' 3000', ' 3016', ' 3034'],\n",
       " ['3075', ' 3081', ' 3083', ' 3103', ' 3104'],\n",
       " ['3106', ' 3124', ' 3155', ' 3156', ' 3208'],\n",
       " ['3106', ' 3124', ' 3155', ' 3156', ' 3208'],\n",
       " ['1267', ' 1356', ' 1421', ' 1497', ' 1549'],\n",
       " ['0620', ' 0679', ' 0682', ' 0696', ' 0703'],\n",
       " ['0681', ' 0700', ' 0760', ' 0783', ' 0862'],\n",
       " ['0870', ' 0942', ' 0947', ' 0965', ' 0990'],\n",
       " ['1004', ' 1028', ' 1060', ' 1071', ' 1072'],\n",
       " ['1111', ' 1126', ' 1159', ' 1183', ' 1188'],\n",
       " ['1233', ' 1254', ' 1279', ' 1301', ' 1309'],\n",
       " ['1332', ' 1334', ' 1337', ' 1345', ' 1346'],\n",
       " ['1375', ' 1383', ' 1394', ' 1404', ' 1408'],\n",
       " ['1425', ' 1429', ' 1455', ' 1488', ' 1500'],\n",
       " ['1541', ' 1556', ' 1557', ' 1559', ' 1566'],\n",
       " ['1590', ' 1607', ' 1613', ' 1618', ' 1623'],\n",
       " ['1651', ' 1658', ' 1695', ' 1696', ' 1700'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['0027', ' 0223', ' 0263', ' 0363', ' 0414', ' 0431'],\n",
       " ['0016', ' 0022', ' 0053', ' 0067', ' 0082'],\n",
       " ['2024', ' 2085', ' 2717'],\n",
       " ['0391', ' 1808', ' 3454'],\n",
       " ['1027', ' 1086', ' 1124', ' 1166', ' 1246'],\n",
       " ['0055', ' 0072', ' 0081', ' 0101', ' 0141'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['0193', ' 0270', ' 0440', ' 0474', ' 0494'],\n",
       " ['0736', ' 0860', ' 0954', ' 0983', ' 0996'],\n",
       " ['2679', ' 2680', ' 2687', ' 2688'],\n",
       " ['3007', ' 3013', ' 3034'],\n",
       " ['0120', ' 0286', ' 2712'],\n",
       " ['2803', ' 2905', ' 2912', ' 2916'],\n",
       " ['1227', ' 1723', ' 3075'],\n",
       " ['0840', ' 1183', ' 1188', ' 1233'],\n",
       " ['0954', ' 0965', ' 0990', ' 0999', ' 1004', ' 1049'],\n",
       " ['0882', ' 0922', ' 0939', ' 0983', ' 1088', ' 1105'],\n",
       " ['1183', ' 1345', ' 1346', ' 1394', ' 1404', ' 1408', ' 1425'],\n",
       " ['1983', ' 2009', ' 2020', ' 2051', ' 2094', ' 2121', ' 2142', ' 2150'],\n",
       " ['2245', ' 2262', ' 2271', ' 2326', ' 2364', ' 2369', ' 2385'],\n",
       " ['2419', ' 2422', ' 2430', ' 2455', ' 2463'],\n",
       " ['2636', ' 2679', ' 2680', ' 2687', ' 2688'],\n",
       " ['2761', ' 2778', ' 2785', ' 2798', ' 2799', ' 2819'],\n",
       " ['3066', ' 3081', ' 3104', ' 3106', ' 3124', ' 3132', ' 3155', ' 3156'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['3081', ' 3103', ' 3124', ' 3155', ' 3156'],\n",
       " ['0200', ' 0261', ' 0264', ' 0378', ' 0410', ' 0443'],\n",
       " ['1480', ' 1541', ' 1607', ' 1651', ' 1658'],\n",
       " ['0609', ' 0620', ' 0638', ' 0679', ' 0682'],\n",
       " ['0740', ' 0823', ' 0872', ' 0882', ' 0922'],\n",
       " ['0954', ' 0983', ' 0996', ' 0998', ' 1049'],\n",
       " ['1164', ' 1168', ' 1182', ' 1195', ' 1207'],\n",
       " ['1311', ' 1354', ' 1360', ' 1510', ' 1562'],\n",
       " ['1634', ' 1646', ' 1672', ' 1674', ' 1719'],\n",
       " ['1773', ' 1802', ' 1810', ' 1817', ' 1847'],\n",
       " ['1854', ' 1860', ' 1879', ' 1930', ' 2020'],\n",
       " ['1906', ' 1943', ' 1983', ' 2009', ' 2027'],\n",
       " ['2034', ' 2044', ' 2053', ' 2054', ' 2077'],\n",
       " ['2124', ' 2135', ' 2136', ' 2147', ' 2216'],\n",
       " ['2225', ' 2227', ' 2268', ' 2336', ' 2339'],\n",
       " ['2348', ' 2380', ' 2413', ' 2508', ' 2540'],\n",
       " ['2555', ' 2586', ' 2713', ' 2732', ' 2761'],\n",
       " ['2778', ' 2785', ' 2794', ' 2799', ' 2801'],\n",
       " ['2827', ' 2841', ' 2855', ' 2863', ' 2907'],\n",
       " ['2965', ' 2989', ' 3000', ' 3016', ' 3034'],\n",
       " ['3075', ' 3081', ' 3083', ' 3103', ' 3104'],\n",
       " ['3106', ' 3124', ' 3155', ' 3156', ' 3208'],\n",
       " ['3106', ' 3124', ' 3155', ' 3156', ' 3208'],\n",
       " ['1267', ' 1356', ' 1421', ' 1497', ' 1549'],\n",
       " ['0620', ' 0679', ' 0682', ' 0696', ' 0703'],\n",
       " ['0681', ' 0700', ' 0760', ' 0783', ' 0862'],\n",
       " ['0870', ' 0942', ' 0947', ' 0965', ' 0990'],\n",
       " ['1004', ' 1028', ' 1060', ' 1071', ' 1072'],\n",
       " ['1111', ' 1126', ' 1159', ' 1183', ' 1188'],\n",
       " ['1233', ' 1254', ' 1279', ' 1301', ' 1309'],\n",
       " ['1332', ' 1334', ' 1337', ' 1345', ' 1346'],\n",
       " ['1375', ' 1383', ' 1394', ' 1404', ' 1408'],\n",
       " ['1425', ' 1429', ' 1455', ' 1488', ' 1500'],\n",
       " ['1541', ' 1556', ' 1557', ' 1559', ' 1566'],\n",
       " ['1590', ' 1607', ' 1613', ' 1618', ' 1623'],\n",
       " ['1651', ' 1658', ' 1695', ' 1696', ' 1700'],\n",
       " ['1854', ' 1860', ' 1879', ' 1930', ' 2020'],\n",
       " ['1906', ' 1943', ' 1983', ' 2009', ' 2027'],\n",
       " ['2034', ' 2044', ' 2053', ' 2054', ' 2077'],\n",
       " ['2124', ' 2135', ' 2136', ' 2147', ' 2216'],\n",
       " ['2225', ' 2227', ' 2268', ' 2336', ' 2339'],\n",
       " ['2348', ' 2380', ' 2413', ' 2508', ' 2540'],\n",
       " ['0609', ' 0620', ' 0638', ' 0679', ' 0682'],\n",
       " ['0740', ' 0823', ' 0872', ' 0882', ' 0922'],\n",
       " ['0954', ' 0983', ' 0996', ' 0998', ' 1049'],\n",
       " ['1164', ' 1168', ' 1182', ' 1195', ' 1207'],\n",
       " ['1311', ' 1354', ' 1360', ' 1510', ' 1562'],\n",
       " ['1634', ' 1646', ' 1672', ' 1674', ' 1719'],\n",
       " ['1773', ' 1802', ' 1810', ' 1817', ' 1847'],\n",
       " ['1854', ' 1860', ' 1879', ' 1930', ' 2020'],\n",
       " ['1906', ' 1943', ' 1983', ' 2009', ' 2027'],\n",
       " ['2034', ' 2044', ' 2053', ' 2054', ' 2077'],\n",
       " ['2124', ' 2135', ' 2136', ' 2147', ' 2216'],\n",
       " ['2225', ' 2227', ' 2268', ' 2336', ' 2339'],\n",
       " ['2348', ' 2380', ' 2413', ' 2508', ' 2540'],\n",
       " ['2555', ' 2586', ' 2713', ' 2732', ' 2761'],\n",
       " ['2778', ' 2785', ' 2794', ' 2799', ' 2801'],\n",
       " ['2827', ' 2841', ' 2855', ' 2863', ' 2907'],\n",
       " ['2965', ' 2989', ' 3000', ' 3016', ' 3034'],\n",
       " ['3075', ' 3081', ' 3083', ' 3103', ' 3104'],\n",
       " ['3106', ' 3124', ' 3155', ' 3156', ' 3208'],\n",
       " ['3106', ' 3124', ' 3155', ' 3156', ' 3208'],\n",
       " ['1267', ' 1356', ' 1421', ' 1497', ' 1549'],\n",
       " ['0620', ' 0679', ' 0682', ' 0696', ' 0703'],\n",
       " ['0681', ' 0700', ' 0760', ' 0783', ' 0862'],\n",
       " ['0870', ' 0942', ' 0947', ' 0965', ' 0990'],\n",
       " ['1004', ' 1028', ' 1060', ' 1071', ' 1072'],\n",
       " ['1111', ' 1126', ' 1159', ' 1183', ' 1188'],\n",
       " ['1233', ' 1254', ' 1279', ' 1301', ' 1309'],\n",
       " ['1332', ' 1334', ' 1337', ' 1345', ' 1346'],\n",
       " ['1375', ' 1383', ' 1394', ' 1404', ' 1408'],\n",
       " ['1425', ' 1429', ' 1455', ' 1488', ' 1500'],\n",
       " ['1541', ' 1556', ' 1557', ' 1559', ' 1566'],\n",
       " ['1590', ' 1607', ' 1613', ' 1618', ' 1623'],\n",
       " ['1651', ' 1658', ' 1695', ' 1696', ' 1700'],\n",
       " ['1854', ' 1860', ' 1879', ' 1930', ' 2020'],\n",
       " ['1906', ' 1943', ' 1983', ' 2009', ' 2027'],\n",
       " ['2034', ' 2044', ' 2053', ' 2054', ' 2077'],\n",
       " ['2124', ' 2135', ' 2136', ' 2147', ' 2216'],\n",
       " ['2225', ' 2227', ' 2268', ' 2336', ' 2339'],\n",
       " ['2348', ' 2380', ' 2413', ' 2508', ' 2540'],\n",
       " ['2348', ' 2380', ' 2413', ' 2508', ' 2540'],\n",
       " ['0027', ' 0223', ' 0263', ' 0363', ' 0414', ' 0431'],\n",
       " ['0028', ' 0097', ' 0164', ' 0176', ' 0177', ' 0179'],\n",
       " ['0209', ' 0224', ' 0324', ' 0411', ' 0457', ' 0501'],\n",
       " ['0534', ' 0538', ' 0571', ' 0576', ' 0598', ' 0710'],\n",
       " ['0804', ' 0815', ' 0873', ' 0903', ' 0910', ' 1002'],\n",
       " ['1084', ' 1135', ' 1267', ' 1317', ' 1333', ' 1369'],\n",
       " ['1407', ' 1469', ' 1526', ' 1527', ' 1778', ' 2052'],\n",
       " ['2169', ' 2194', ' 2289', ' 2329', ' 2454', ' 2550'],\n",
       " ['2577', ' 2730', ' 2790', ' 2917', ' 3291', ' 3315'],\n",
       " ['3388', ' 3516', ' 0005', ' 0024', ' 0044', ' 0045'],\n",
       " ['0093', ' 0130', ' 0136', ' 0140', ' 0163', ' 0173'],\n",
       " ['0187', ' 0197', ' 0212', ' 0237', ' 0321', ' 0337'],\n",
       " ['0380', ' 0522', ' 0541', ' 0612', ' 0618', ' 0721'],\n",
       " ['0790', ' 0805', ' 0880', ' 0898', ' 0932', ' 0933'],\n",
       " ['0980', ' 1050', ' 1051', ' 1082', ' 1083', ' 1100'],\n",
       " ['1263', ' 1266', ' 1271', ' 1274', ' 1378', ' 1402'],\n",
       " ['1417', ' 1466', ' 1485', ' 1492', ' 1522', ' 1547'],\n",
       " ['1615', ' 1633', ' 1647', ' 1790', ' 1850', ' 1948'],\n",
       " ['2084', ' 2137', ' 2218', ' 2295', ' 2464', ' 2531'],\n",
       " ['2575', ' 2595', ' 2596', ' 2645', ' 2720', ' 2745'],\n",
       " ['2760', ' 2860', ' 2870', ' 2893', ' 2897', ' 2898'],\n",
       " ['2915', ' 3028', ' 3033', ' 3209', ' 3217', ' 3223'],\n",
       " ['3261', ' 3273', ' 3282', ' 3323', ' 3362', ' 3368'],\n",
       " ['3377', ' 3479', ' 0403', ' 0861', ' 0959', ' 1001'],\n",
       " ['1186', ' 1242', ' 1243', ' 1453', ' 1716', ' 1717'],\n",
       " ['2333', ' 3022', ' 3067', ' 3068', ' 3193', ' 3283'],\n",
       " ['3293', ' 3326', ' 3327', ' 3441', ' 3442', ' 3470'],\n",
       " ['3471', ' 3500', ' 3501', ' 0086', ' 0106', ' 0415'],\n",
       " ['0425', ' 0460', ' 0477', ' 0492', ' 0943', ' 1009'],\n",
       " ['1268', ' 1272', ' 1416', ' 1477', ' 1561', ' 1697'],\n",
       " ['2770', ' 3186', ' 3347', ' 3451'],\n",
       " ['3293', ' 3326', ' 3327', ' 3441', ' 3442', ' 3470'],\n",
       " ['3471', ' 3500', ' 3501', ' 0086', ' 0106', ' 0415'],\n",
       " ['0425', ' 0460', ' 0477', ' 0492', ' 0943', ' 1009'],\n",
       " ['1268', ' 1272', ' 1416', ' 1477', ' 1561', ' 1697'],\n",
       " ['2770', ' 3186', ' 3347', ' 3451'],\n",
       " ['0028', ' 0097', ' 0164', ' 0176', ' 0177        '],\n",
       " [' 0209', ' 0224', ' 0324', ' 0411', ' 0457        '],\n",
       " ['0501', ' 0534', ' 0538', ' 0571', ' 0576'],\n",
       " ['0598', ' 0710', ' 0804', ' 0815', ' 0873        '],\n",
       " ['0903', ' 0910', ' 1002', ' 1084', ' 1135        '],\n",
       " ['1267', ' 1317', ' 1333', ' 1369', ' 1407        '],\n",
       " ['1469', ' 1526', ' 1527', ' 1778', ' 2052        '],\n",
       " ['2169', ' 2194', ' 2289', ' 2329', ' 2454        '],\n",
       " ['2550', ' 2577', ' 2730', ' 2790', ' 2917        '],\n",
       " ['2550', ' 2577', ' 2730', ' 2790', ' 2917        '],\n",
       " ['3291', ' 3315', ' 3388', ' 3516', ' 0005        '],\n",
       " ['0024', ' 0044', ' 0045', ' 0093', ' 0130        '],\n",
       " ['0136', ' 0140', ' 0163', ' 0173', ' 0187        '],\n",
       " ['0197', ' 0212', ' 0237', ' 0321', ' 0337        '],\n",
       " ['0380', ' 0522', ' 0541', ' 0612', ' 0618        '],\n",
       " ['0721', ' 0790', ' 0805', ' 0880', ' 0898        '],\n",
       " ['0932', ' 0933', ' 0980', ' 1050', ' 1051        '],\n",
       " ['1082', ' 1083', ' 1100', ' 1263', ' 1266        '],\n",
       " ['1271', ' 1274', ' 1378', ' 1402', ' 1417        '],\n",
       " ['1466', ' 1485', ' 1492', ' 1522', ' 1547        '],\n",
       " ['1615', ' 1633', ' 1647', ' 1790', ' 1850        '],\n",
       " ['1948', ' 2084', ' 2137', ' 2218', ' 2295        '],\n",
       " ['2464', ' 2531', ' 2575', ' 2595', ' 2596        '],\n",
       " ['2645', ' 2720', ' 2745', ' 2760', ' 2860        '],\n",
       " ['2870', ' 2893', ' 2897', ' 2898', ' 2915        '],\n",
       " ['3028', ' 3033', ' 3209', ' 3217', ' 3223        '],\n",
       " ['3261', ' 3273', ' 3282', ' 3323', ' 3362        '],\n",
       " ['3368', ' 3377', ' 3479', ' 0403', ' 0861        '],\n",
       " ['0959', ' 1001', ' 1186', ' 1242', ' 1243        '],\n",
       " ['1453', ' 1716', ' 1717', ' 2333', ' 3022        '],\n",
       " ['3067', ' 3068', ' 3193', ' 3283', ' 3293        '],\n",
       " ['3326', ' 3327', ' 3441', ' 3442', ' 3470        '],\n",
       " ['3500', ' 3501', ' 0086', ' 0106', ' 0415        '],\n",
       " ['0425', ' 0460', ' 0477', ' 0492', ' 0943        '],\n",
       " ['1009', ' 1268', ' 1272', ' 1416', ' 1477        '],\n",
       " ['1561', ' 1697', ' 2770', ' 3186', ' 3347        '],\n",
       " ['0780', ' 0966', ' 2754'],\n",
       " ['2754', ' 1677', ' 2611'],\n",
       " ['0966', ' 1677', ' 1109'],\n",
       " ['0780', ' 0966', ' 2754', ' 1677', ' 2116', ' 0687', ' 0816'],\n",
       " ['0687', ' 0816'],\n",
       " ['1677', ' 1109', ' 2497'],\n",
       " ['1677', ' 1109', ' 2497'],\n",
       " ['2558', ' 2924', ' 3334', ' 1801'],\n",
       " ['1350', ' 3472', ' 2558', ' 2924', ' 3334', ' 1801', ' 0992'],\n",
       " ['1677', ' 2611', ' 1109', ' 2497'],\n",
       " ['1677', ' 2611', ' 1109'],\n",
       " ['Data analysis'],\n",
       " ['0567', ' 1289', ' 2057', ' 2708', ' 2359', ' 1888', ' 2087'],\n",
       " ['0567', ' 1289', ' 2057'],\n",
       " ['2359', ' 2919', ' 1888', ' 0555'],\n",
       " ['2057', ' 0567', ' 1289', ' 0014', ' 2708', ' 0985', ' 0603'],\n",
       " ['2057', ' 0567', ' 1289', ' 0985', ' 0603'],\n",
       " ['2057', ' 0567', ' 1289', ' 3503', ' 1370'],\n",
       " ['0567', ' 1289', ' 2708', ' 1956', ' 1158', ' 1544', ' 0555'],\n",
       " ['2075', ' 1289', ' 1218'],\n",
       " ['2075', ' 1289', ' 1218'],\n",
       " ['2708', ' 1956', ' 1158', ' 1544', ' 0555'],\n",
       " ['0567', ' 1289', ' 0014'],\n",
       " ['2116', ' 0687', ' 0816'],\n",
       " ['2558', ' 2924'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599', ' 1496'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599'],\n",
       " ['0567', ' 1289', ' 0014'],\n",
       " ['2116', ' 0687', ' 0816'],\n",
       " ['2558', ' 2924', ' 1801'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599', ' 1496'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599'],\n",
       " ['0567', ' 1289', ' 0014'],\n",
       " ['2116', ' 0687', ' 0816'],\n",
       " ['2558', ' 2924', ' 1801'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599', ' 1496'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 3278', ' 1075'],\n",
       " ['0014', ' 1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599'],\n",
       " ['1075', ' 1496', ' 1502', ' 1599'],\n",
       " ['1075', ' 1502', ' 1599'],\n",
       " ['0567', ' 1289', ' 0014'],\n",
       " ['2116', ' 0687', ' 0816'],\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0005</th>\n",
       "      <th>0005</th>\n",
       "      <th>0014</th>\n",
       "      <th>0022</th>\n",
       "      <th>0024</th>\n",
       "      <th>0031</th>\n",
       "      <th>0044</th>\n",
       "      <th>0045</th>\n",
       "      <th>0053</th>\n",
       "      <th>0062</th>\n",
       "      <th>...</th>\n",
       "      <th>3465</th>\n",
       "      <th>3471</th>\n",
       "      <th>3475</th>\n",
       "      <th>3494</th>\n",
       "      <th>3498</th>\n",
       "      <th>3500</th>\n",
       "      <th>3502</th>\n",
       "      <th>3504</th>\n",
       "      <th>3514</th>\n",
       "      <th>Data analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2138 rows × 1896 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0005  0005          0014  0022  0024   \n",
       "0        0             0     0     0     0  \\\n",
       "1        0             0     0     0     0   \n",
       "2        0             0     0     0     0   \n",
       "3        0             0     0     0     0   \n",
       "4        0             0     0     0     0   \n",
       "...    ...           ...   ...   ...   ...   \n",
       "2133     0             0     0     0     0   \n",
       "2134     0             0     0     0     0   \n",
       "2135     0             0     0     0     0   \n",
       "2136     0             0     0     0     0   \n",
       "2137     0             0     0     0     0   \n",
       "\n",
       "      0031                                 0044  0045  0053  0062  ... 3465   \n",
       "0                                       0     0     0     0     0  ...    0  \\\n",
       "1                                       0     0     0     0     0  ...    0   \n",
       "2                                       0     0     0     0     0  ...    0   \n",
       "3                                       0     0     0     0     0  ...    0   \n",
       "4                                       0     0     0     0     0  ...    0   \n",
       "...                                   ...   ...   ...   ...   ...  ...  ...   \n",
       "2133                                    0     0     0     0     0  ...    0   \n",
       "2134                                    0     0     0     0     0  ...    0   \n",
       "2135                                    0     0     0     0     0  ...    0   \n",
       "2136                                    0     0     0     0     0  ...    0   \n",
       "2137                                    0     0     0     0     0  ...    0   \n",
       "\n",
       "     3471 3475 3494 3498 3500 3502 3504 3514 Data analysis  \n",
       "0       0    0    0    0    0    0    0    0             0  \n",
       "1       0    0    0    0    0    0    0    0             0  \n",
       "2       0    0    0    0    0    0    0    0             0  \n",
       "3       0    0    0    0    0    0    0    0             0  \n",
       "4       0    0    0    0    0    0    0    0             0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...           ...  \n",
       "2133    0    0    0    0    0    0    0    0             0  \n",
       "2134    0    0    0    0    0    0    0    0             0  \n",
       "2135    0    0    0    0    0    0    0    0             0  \n",
       "2136    0    0    0    0    0    0    0    0             0  \n",
       "2137    0    0    0    0    0    0    0    0             0  \n",
       "\n",
       "[2138 rows x 1896 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's transform the list, with one-hot encoding\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "a = TransactionEncoder()\n",
    "a_data = a.fit(data).transform(data)\n",
    "df = pd.DataFrame(a_data,columns=a.columns_)\n",
    "df = df.replace(False,0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 155 combinations | Sampling itemset size 54\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011225</td>\n",
       "      <td>( 0014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011225</td>\n",
       "      <td>( 0236)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014032</td>\n",
       "      <td>( 0257)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012629</td>\n",
       "      <td>( 0272)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013096</td>\n",
       "      <td>( 0364)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.013096</td>\n",
       "      <td>( 3469,  2993,  3108, 2842)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.013096</td>\n",
       "      <td>( 2993,  3503,  3108, 2842)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.013096</td>\n",
       "      <td>( 2993,  3469,  3503, 2842)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.013096</td>\n",
       "      <td>( 3469,  3503,  3108, 2842)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.013096</td>\n",
       "      <td>( 2993, 2842,  3503,  3108,  3469)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      support                            itemsets\n",
       "0    0.011225                             ( 0014)\n",
       "1    0.011225                             ( 0236)\n",
       "2    0.014032                             ( 0257)\n",
       "3    0.012629                             ( 0272)\n",
       "4    0.013096                             ( 0364)\n",
       "..        ...                                 ...\n",
       "189  0.013096         ( 3469,  2993,  3108, 2842)\n",
       "190  0.013096         ( 2993,  3503,  3108, 2842)\n",
       "191  0.013096         ( 2993,  3469,  3503, 2842)\n",
       "192  0.013096         ( 3469,  3503,  3108, 2842)\n",
       "193  0.013096  ( 2993, 2842,  3503,  3108,  3469)\n",
       "\n",
       "[194 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set a threshold value for the support value and calculate the support value.\n",
    "apriori_t = apriori(df2, min_support = 0.01, use_colnames = True, verbose = 1)\n",
    "apriori_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>( 0014)</td>\n",
       "      <td>( 1289)</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.028064</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.633333</td>\n",
       "      <td>0.010910</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.982971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>( 0236)</td>\n",
       "      <td>( 1115)</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>82.230769</td>\n",
       "      <td>0.011089</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.999054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>( 1115)</td>\n",
       "      <td>( 0236)</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>82.230769</td>\n",
       "      <td>0.011089</td>\n",
       "      <td>12.854069</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>( 2542)</td>\n",
       "      <td>( 0236)</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>82.230769</td>\n",
       "      <td>0.011089</td>\n",
       "      <td>12.854069</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>( 0236)</td>\n",
       "      <td>( 2542)</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>82.230769</td>\n",
       "      <td>0.011089</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.999054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>( 3469,  3108)</td>\n",
       "      <td>( 3503,  2993, 2842)</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.357143</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>( 2993)</td>\n",
       "      <td>( 3469,  3503,  3108, 2842)</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.357143</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>(2842)</td>\n",
       "      <td>( 3469,  3503,  2993,  3108)</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.357143</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>( 3108)</td>\n",
       "      <td>( 3469,  3503,  2993, 2842)</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.357143</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>( 3469)</td>\n",
       "      <td>( 3503,  2993,  3108, 2842)</td>\n",
       "      <td>0.017774</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>56.263158</td>\n",
       "      <td>0.012864</td>\n",
       "      <td>3.750234</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>519 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        antecedents                   consequents  antecedent support   \n",
       "0           ( 0014)                       ( 1289)            0.011225  \\\n",
       "1           ( 0236)                       ( 1115)            0.011225   \n",
       "2           ( 1115)                       ( 0236)            0.012161   \n",
       "3           ( 2542)                       ( 0236)            0.012161   \n",
       "4           ( 0236)                       ( 2542)            0.011225   \n",
       "..              ...                           ...                 ...   \n",
       "514  ( 3469,  3108)          ( 3503,  2993, 2842)            0.013096   \n",
       "515         ( 2993)   ( 3469,  3503,  3108, 2842)            0.013096   \n",
       "516          (2842)  ( 3469,  3503,  2993,  3108)            0.013096   \n",
       "517         ( 3108)   ( 3469,  3503,  2993, 2842)            0.013096   \n",
       "518         ( 3469)   ( 3503,  2993,  3108, 2842)            0.017774   \n",
       "\n",
       "     consequent support   support  confidence       lift  leverage   \n",
       "0              0.028064  0.011225    1.000000  35.633333  0.010910  \\\n",
       "1              0.012161  0.011225    1.000000  82.230769  0.011089   \n",
       "2              0.011225  0.011225    0.923077  82.230769  0.011089   \n",
       "3              0.011225  0.011225    0.923077  82.230769  0.011089   \n",
       "4              0.012161  0.011225    1.000000  82.230769  0.011089   \n",
       "..                  ...       ...         ...        ...       ...   \n",
       "514            0.013096  0.013096    1.000000  76.357143  0.012925   \n",
       "515            0.013096  0.013096    1.000000  76.357143  0.012925   \n",
       "516            0.013096  0.013096    1.000000  76.357143  0.012925   \n",
       "517            0.013096  0.013096    1.000000  76.357143  0.012925   \n",
       "518            0.013096  0.013096    0.736842  56.263158  0.012864   \n",
       "\n",
       "     conviction  zhangs_metric  \n",
       "0           inf       0.982971  \n",
       "1           inf       0.999054  \n",
       "2     12.854069       1.000000  \n",
       "3     12.854069       1.000000  \n",
       "4           inf       0.999054  \n",
       "..          ...            ...  \n",
       "514         inf       1.000000  \n",
       "515         inf       1.000000  \n",
       "516         inf       1.000000  \n",
       "517         inf       1.000000  \n",
       "518    3.750234       1.000000  \n",
       "\n",
       "[519 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's view our interpretation values using the Associan rule function.\n",
    "df_ar = association_rules(apriori_t, metric = \"confidence\", min_threshold = 0.6)\n",
    "df_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>( 0014)</td>\n",
       "      <td>( 1289)</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.028064</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.633333</td>\n",
       "      <td>0.010910</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.982971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>( 0236)</td>\n",
       "      <td>( 1115)</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>82.230769</td>\n",
       "      <td>0.011089</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.999054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>( 1115)</td>\n",
       "      <td>( 0236)</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>82.230769</td>\n",
       "      <td>0.011089</td>\n",
       "      <td>12.854069</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>( 2542)</td>\n",
       "      <td>( 0236)</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>82.230769</td>\n",
       "      <td>0.011089</td>\n",
       "      <td>12.854069</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>( 0236)</td>\n",
       "      <td>( 2542)</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>82.230769</td>\n",
       "      <td>0.011089</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.999054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>( 0236)</td>\n",
       "      <td>(0342)</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>73.724138</td>\n",
       "      <td>0.011073</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.997635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0342)</td>\n",
       "      <td>( 0236)</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>73.724138</td>\n",
       "      <td>0.011073</td>\n",
       "      <td>5.734892</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(0256)</td>\n",
       "      <td>( 0257)</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>0.014032</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>71.266667</td>\n",
       "      <td>0.013374</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.999526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>( 0257)</td>\n",
       "      <td>(0256)</td>\n",
       "      <td>0.014032</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>71.266667</td>\n",
       "      <td>0.013374</td>\n",
       "      <td>29.593078</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>( 0488)</td>\n",
       "      <td>( 0272)</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.185185</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>( 0272)</td>\n",
       "      <td>( 0488)</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.185185</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>( 1245)</td>\n",
       "      <td>( 0272)</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.185185</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>( 0272)</td>\n",
       "      <td>( 1245)</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.185185</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(0189)</td>\n",
       "      <td>( 0272)</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.185185</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>( 0272)</td>\n",
       "      <td>(0189)</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.185185</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>( 0364)</td>\n",
       "      <td>( 0454)</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>67.873016</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>6.911600</td>\n",
       "      <td>0.998341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>( 0454)</td>\n",
       "      <td>( 0364)</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>67.873016</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>8.882133</td>\n",
       "      <td>0.997868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>( 0364)</td>\n",
       "      <td>( 2731)</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>76.357143</td>\n",
       "      <td>0.012002</td>\n",
       "      <td>13.829747</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>( 2731)</td>\n",
       "      <td>( 0364)</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.357143</td>\n",
       "      <td>0.012002</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.999053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>( 0364)</td>\n",
       "      <td>(2976)</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>76.357143</td>\n",
       "      <td>0.011078</td>\n",
       "      <td>6.921422</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(2976)</td>\n",
       "      <td>( 0364)</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.013096</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.357143</td>\n",
       "      <td>0.011078</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.998108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>( 2731)</td>\n",
       "      <td>( 0454)</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>73.094017</td>\n",
       "      <td>0.011072</td>\n",
       "      <td>12.835828</td>\n",
       "      <td>0.998461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>( 0454)</td>\n",
       "      <td>( 2731)</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>73.094017</td>\n",
       "      <td>0.011072</td>\n",
       "      <td>8.890552</td>\n",
       "      <td>0.998934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(2976)</td>\n",
       "      <td>( 0454)</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.185185</td>\n",
       "      <td>0.011084</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.998581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>( 0454)</td>\n",
       "      <td>(2976)</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>79.185185</td>\n",
       "      <td>0.011084</td>\n",
       "      <td>8.898971</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>( 1245)</td>\n",
       "      <td>( 0488)</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.185185</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>( 0488)</td>\n",
       "      <td>( 1245)</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.185185</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>( 0488)</td>\n",
       "      <td>(0189)</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.185185</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(0189)</td>\n",
       "      <td>( 0488)</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.185185</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>( 0819)</td>\n",
       "      <td>( 0685)</td>\n",
       "      <td>0.017306</td>\n",
       "      <td>0.014967</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>43.337838</td>\n",
       "      <td>0.010966</td>\n",
       "      <td>2.803555</td>\n",
       "      <td>0.994130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   antecedents consequents  antecedent support  consequent support   support   \n",
       "0      ( 0014)     ( 1289)            0.011225            0.028064  0.011225  \\\n",
       "1      ( 0236)     ( 1115)            0.011225            0.012161  0.011225   \n",
       "2      ( 1115)     ( 0236)            0.012161            0.011225  0.011225   \n",
       "3      ( 2542)     ( 0236)            0.012161            0.011225  0.011225   \n",
       "4      ( 0236)     ( 2542)            0.011225            0.012161  0.011225   \n",
       "5      ( 0236)      (0342)            0.011225            0.013564  0.011225   \n",
       "6       (0342)     ( 0236)            0.013564            0.011225  0.011225   \n",
       "7       (0256)     ( 0257)            0.013564            0.014032  0.013564   \n",
       "8      ( 0257)      (0256)            0.014032            0.013564  0.013564   \n",
       "9      ( 0488)     ( 0272)            0.012629            0.012629  0.012629   \n",
       "10     ( 0272)     ( 0488)            0.012629            0.012629  0.012629   \n",
       "11     ( 1245)     ( 0272)            0.012629            0.012629  0.012629   \n",
       "12     ( 0272)     ( 1245)            0.012629            0.012629  0.012629   \n",
       "13      (0189)     ( 0272)            0.012629            0.012629  0.012629   \n",
       "14     ( 0272)      (0189)            0.012629            0.012629  0.012629   \n",
       "15     ( 0364)     ( 0454)            0.013096            0.012629  0.011225   \n",
       "16     ( 0454)     ( 0364)            0.012629            0.013096  0.011225   \n",
       "17     ( 0364)     ( 2731)            0.013096            0.012161  0.012161   \n",
       "18     ( 2731)     ( 0364)            0.012161            0.013096  0.012161   \n",
       "19     ( 0364)      (2976)            0.013096            0.011225  0.011225   \n",
       "20      (2976)     ( 0364)            0.011225            0.013096  0.011225   \n",
       "21     ( 2731)     ( 0454)            0.012161            0.012629  0.011225   \n",
       "22     ( 0454)     ( 2731)            0.012629            0.012161  0.011225   \n",
       "23      (2976)     ( 0454)            0.011225            0.012629  0.011225   \n",
       "24     ( 0454)      (2976)            0.012629            0.011225  0.011225   \n",
       "25     ( 1245)     ( 0488)            0.012629            0.012629  0.012629   \n",
       "26     ( 0488)     ( 1245)            0.012629            0.012629  0.012629   \n",
       "27     ( 0488)      (0189)            0.012629            0.012629  0.012629   \n",
       "28      (0189)     ( 0488)            0.012629            0.012629  0.012629   \n",
       "29     ( 0819)     ( 0685)            0.017306            0.014967  0.011225   \n",
       "\n",
       "    confidence       lift  leverage  conviction  zhangs_metric  \n",
       "0     1.000000  35.633333  0.010910         inf       0.982971  \n",
       "1     1.000000  82.230769  0.011089         inf       0.999054  \n",
       "2     0.923077  82.230769  0.011089   12.854069       1.000000  \n",
       "3     0.923077  82.230769  0.011089   12.854069       1.000000  \n",
       "4     1.000000  82.230769  0.011089         inf       0.999054  \n",
       "5     1.000000  73.724138  0.011073         inf       0.997635  \n",
       "6     0.827586  73.724138  0.011073    5.734892       1.000000  \n",
       "7     1.000000  71.266667  0.013374         inf       0.999526  \n",
       "8     0.966667  71.266667  0.013374   29.593078       1.000000  \n",
       "9     1.000000  79.185185  0.012469         inf       1.000000  \n",
       "10    1.000000  79.185185  0.012469         inf       1.000000  \n",
       "11    1.000000  79.185185  0.012469         inf       1.000000  \n",
       "12    1.000000  79.185185  0.012469         inf       1.000000  \n",
       "13    1.000000  79.185185  0.012469         inf       1.000000  \n",
       "14    1.000000  79.185185  0.012469         inf       1.000000  \n",
       "15    0.857143  67.873016  0.011060    6.911600       0.998341  \n",
       "16    0.888889  67.873016  0.011060    8.882133       0.997868  \n",
       "17    0.928571  76.357143  0.012002   13.829747       1.000000  \n",
       "18    1.000000  76.357143  0.012002         inf       0.999053  \n",
       "19    0.857143  76.357143  0.011078    6.921422       1.000000  \n",
       "20    1.000000  76.357143  0.011078         inf       0.998108  \n",
       "21    0.923077  73.094017  0.011072   12.835828       0.998461  \n",
       "22    0.888889  73.094017  0.011072    8.890552       0.998934  \n",
       "23    1.000000  79.185185  0.011084         inf       0.998581  \n",
       "24    0.888889  79.185185  0.011084    8.898971       1.000000  \n",
       "25    1.000000  79.185185  0.012469         inf       1.000000  \n",
       "26    1.000000  79.185185  0.012469         inf       1.000000  \n",
       "27    1.000000  79.185185  0.012469         inf       1.000000  \n",
       "28    1.000000  79.185185  0.012469         inf       1.000000  \n",
       "29    0.648649  43.337838  0.010966    2.803555       0.994130  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ar[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antecedents: 1288    datum visualization python\n",
      "Name: Course Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "result = course.loc[course['Course_id'] == 1289]['Course Name']\n",
    "print('antecedents:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "antecedents: business statistic analysis capstone\n",
      "datum visualization python\n",
      "consequents: datum visualization python \n",
      " ------------------------------\n",
      "1\n",
      "antecedents: matrix method\n",
      "precalculus periodic function\n",
      "consequents: precalculus periodic function \n",
      " ------------------------------\n",
      "2\n",
      "antecedents: precalculus periodic function\n",
      "matrix method\n",
      "consequents: matrix method \n",
      " ------------------------------\n",
      "3\n",
      "antecedents: matrix method\n",
      "logic economist\n",
      "consequents: logic economist \n",
      " ------------------------------\n",
      "4\n",
      "antecedents: logic economist\n",
      "matrix method\n",
      "consequents: matrix method \n",
      " ------------------------------\n",
      "5\n",
      "antecedents: game theory python\n",
      "matrix method\n",
      "consequents: matrix method \n",
      " ------------------------------\n",
      "6\n",
      "antecedents: matrix method\n",
      "game theory python\n",
      "consequents: game theory python \n",
      " ------------------------------\n",
      "7\n",
      "antecedents: art music production\n",
      "art music production\n",
      "consequents: art music production \n",
      " ------------------------------\n",
      "8\n",
      "antecedents: art music production\n",
      "art music production\n",
      "consequents: art music production \n",
      " ------------------------------\n",
      "9\n",
      "antecedents: discrete mathematic\n",
      "game theoretic solution concept spread sheet\n",
      "consequents: game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "10\n",
      "antecedents: game theoretic solution concept spread sheet\n",
      "discrete mathematic\n",
      "consequents: discrete mathematic \n",
      " ------------------------------\n",
      "11\n",
      "antecedents: mathematic machine learn multivariate calculus\n",
      "game theoretic solution concept spread sheet\n",
      "consequents: game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "12\n",
      "antecedents: game theoretic solution concept spread sheet\n",
      "mathematic machine learn multivariate calculus\n",
      "consequents: mathematic machine learn multivariate calculus \n",
      " ------------------------------\n",
      "13\n",
      "antecedents: precalculus mathematical modeling\n",
      "game theoretic solution concept spread sheet\n",
      "consequents: game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "14\n",
      "antecedents: game theoretic solution concept spread sheet\n",
      "precalculus mathematical modeling\n",
      "consequents: precalculus mathematical modeling \n",
      " ------------------------------\n",
      "15\n",
      "antecedents: differential equation engineer\n",
      "data science math skill\n",
      "consequents: data science math skill \n",
      " ------------------------------\n",
      "16\n",
      "antecedents: data science math skill\n",
      "differential equation engineer\n",
      "consequents: differential equation engineer \n",
      " ------------------------------\n",
      "17\n",
      "antecedents: image video processing mar hollywood stop hospital\n",
      "data science math skill\n",
      "consequents: data science math skill \n",
      " ------------------------------\n",
      "18\n",
      "antecedents: data science math skill\n",
      "image video processing mar hollywood stop hospital\n",
      "consequents: image video processing mar hollywood stop hospital \n",
      " ------------------------------\n",
      "19\n",
      "antecedents: precalculus relation function\n",
      "data science math skill\n",
      "consequents: data science math skill \n",
      " ------------------------------\n",
      "20\n",
      "antecedents: data science math skill\n",
      "precalculus relation function\n",
      "consequents: precalculus relation function \n",
      " ------------------------------\n",
      "21\n",
      "antecedents: differential equation engineer\n",
      "image video processing mar hollywood stop hospital\n",
      "consequents: image video processing mar hollywood stop hospital \n",
      " ------------------------------\n",
      "22\n",
      "antecedents: image video processing mar hollywood stop hospital\n",
      "differential equation engineer\n",
      "consequents: differential equation engineer \n",
      " ------------------------------\n",
      "23\n",
      "antecedents: differential equation engineer\n",
      "precalculus relation function\n",
      "consequents: precalculus relation function \n",
      " ------------------------------\n",
      "24\n",
      "antecedents: precalculus relation function\n",
      "differential equation engineer\n",
      "consequents: differential equation engineer \n",
      " ------------------------------\n",
      "25\n",
      "antecedents: discrete mathematic\n",
      "mathematic machine learn multivariate calculus\n",
      "consequents: mathematic machine learn multivariate calculus \n",
      " ------------------------------\n",
      "26\n",
      "antecedents: mathematic machine learn multivariate calculus\n",
      "discrete mathematic\n",
      "consequents: discrete mathematic \n",
      " ------------------------------\n",
      "27\n",
      "antecedents: discrete mathematic\n",
      "precalculus mathematical modeling\n",
      "consequents: precalculus mathematical modeling \n",
      " ------------------------------\n",
      "28\n",
      "antecedents: precalculus mathematical modeling\n",
      "discrete mathematic\n",
      "consequents: discrete mathematic \n",
      " ------------------------------\n",
      "29\n",
      "antecedents: chinese hsk\n",
      "chinese hsk part\n",
      "consequents: chinese hsk part \n",
      " ------------------------------\n",
      "30\n",
      "antecedents: chinese hsk part\n",
      "chinese hsk\n",
      "consequents: chinese hsk \n",
      " ------------------------------\n",
      "31\n",
      "antecedents: chinese hsk\n",
      "chinese hsk\n",
      "consequents: chinese hsk \n",
      " ------------------------------\n",
      "32\n",
      "antecedents: chinese hsk\n",
      "chinese hsk\n",
      "consequents: chinese hsk \n",
      " ------------------------------\n",
      "33\n",
      "antecedents: sequence model time series natural language processing\n",
      "sequence model\n",
      "consequents: sequence model \n",
      " ------------------------------\n",
      "34\n",
      "antecedents: sequence model\n",
      "sequence model time series natural language processing\n",
      "consequents: sequence model time series natural language processing \n",
      " ------------------------------\n",
      "35\n",
      "antecedents: chinese hsk\n",
      "chinese hsk part\n",
      "consequents: chinese hsk part \n",
      " ------------------------------\n",
      "36\n",
      "antecedents: chinese hsk part\n",
      "chinese hsk\n",
      "consequents: chinese hsk \n",
      " ------------------------------\n",
      "37\n",
      "antecedents: introduction complex analysis\n",
      "game without chance combinatorial game theory\n",
      "consequents: game without chance combinatorial game theory \n",
      " ------------------------------\n",
      "38\n",
      "antecedents: game without chance combinatorial game theory\n",
      "introduction complex analysis\n",
      "consequents: introduction complex analysis \n",
      " ------------------------------\n",
      "39\n",
      "antecedents: introduction complex analysis\n",
      "mathematical biostatistic boot camp\n",
      "consequents: mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "40\n",
      "antecedents: mathematical biostatistic boot camp\n",
      "introduction complex analysis\n",
      "consequents: introduction complex analysis \n",
      " ------------------------------\n",
      "41\n",
      "antecedents: introduction complex analysis\n",
      "fibonacci number golden ratio\n",
      "consequents: fibonacci number golden ratio \n",
      " ------------------------------\n",
      "42\n",
      "antecedents: fibonacci number golden ratio\n",
      "introduction complex analysis\n",
      "consequents: introduction complex analysis \n",
      " ------------------------------\n",
      "43\n",
      "antecedents: datum analytic accounting capstone\n",
      "advanced business analytic capstone\n",
      "consequents: advanced business analytic capstone \n",
      " ------------------------------\n",
      "44\n",
      "antecedents: advanced business analytic capstone\n",
      "datum analytic accounting capstone\n",
      "consequents: datum analytic accounting capstone \n",
      " ------------------------------\n",
      "45\n",
      "antecedents: business statistic analysis capstone\n",
      "advanced business analytic capstone\n",
      "consequents: advanced business analytic capstone \n",
      " ------------------------------\n",
      "46\n",
      "antecedents: advanced business analytic capstone\n",
      "business statistic analysis capstone\n",
      "consequents: business statistic analysis capstone \n",
      " ------------------------------\n",
      "47\n",
      "antecedents: sentiment analysis deep learning use bert\n",
      "nlp twitter sentiment analysis\n",
      "consequents: nlp twitter sentiment analysis \n",
      " ------------------------------\n",
      "48\n",
      "antecedents: nlp twitter sentiment analysis\n",
      "sentiment analysis deep learning use bert\n",
      "consequents: sentiment analysis deep learning use bert \n",
      " ------------------------------\n",
      "49\n",
      "antecedents: logic economist\n",
      "precalculus periodic function\n",
      "consequents: precalculus periodic function \n",
      " ------------------------------\n",
      "50\n",
      "antecedents: precalculus periodic function\n",
      "logic economist\n",
      "consequents: logic economist \n",
      " ------------------------------\n",
      "51\n",
      "antecedents: game theory python\n",
      "precalculus periodic function\n",
      "consequents: precalculus periodic function \n",
      " ------------------------------\n",
      "52\n",
      "antecedents: precalculus periodic function\n",
      "game theory python\n",
      "consequents: game theory python \n",
      " ------------------------------\n",
      "53\n",
      "antecedents: precalculus mathematical modeling\n",
      "mathematic machine learn multivariate calculus\n",
      "consequents: mathematic machine learn multivariate calculus \n",
      " ------------------------------\n",
      "54\n",
      "antecedents: mathematic machine learn multivariate calculus\n",
      "precalculus mathematical modeling\n",
      "consequents: precalculus mathematical modeling \n",
      " ------------------------------\n",
      "55\n",
      "antecedents: inferential statistical analysis python\n",
      "datum visualization python\n",
      "consequents: datum visualization python \n",
      " ------------------------------\n",
      "56\n",
      "antecedents: datum visualization python\n",
      "inferential statistical analysis python\n",
      "consequents: inferential statistical analysis python \n",
      " ------------------------------\n",
      "57\n",
      "antecedents: vector calculus engineer\n",
      "matrix algebra engineer\n",
      "consequents: matrix algebra engineer \n",
      " ------------------------------\n",
      "58\n",
      "antecedents: matrix algebra engineer\n",
      "vector calculus engineer\n",
      "consequents: vector calculus engineer \n",
      " ------------------------------\n",
      "59\n",
      "antecedents: introduction calculus\n",
      "vector calculus engineer\n",
      "consequents: vector calculus engineer \n",
      " ------------------------------\n",
      "60\n",
      "antecedents: vector calculus engineer\n",
      "introduction calculus\n",
      "consequents: introduction calculus \n",
      " ------------------------------\n",
      "61\n",
      "antecedents: vector calculus engineer\n",
      "analytic combinatoric\n",
      "consequents: analytic combinatoric \n",
      " ------------------------------\n",
      "62\n",
      "antecedents: analytic combinatoric\n",
      "vector calculus engineer\n",
      "consequents: vector calculus engineer \n",
      " ------------------------------\n",
      "63\n",
      "antecedents: datum analysis interpretation capstone\n",
      "big datum capstone project\n",
      "consequents: big datum capstone project \n",
      " ------------------------------\n",
      "64\n",
      "antecedents: datum analysis interpretation capstone\n",
      "data analytics foundations accountancy ii\n",
      "consequents: data analytics foundations accountancy ii \n",
      " ------------------------------\n",
      "65\n",
      "antecedents: datum analysis interpretation capstone\n",
      "advanced business analytic capstone\n",
      "consequents: advanced business analytic capstone \n",
      " ------------------------------\n",
      "66\n",
      "antecedents: advanced business analytic capstone\n",
      "datum analysis interpretation capstone\n",
      "consequents: datum analysis interpretation capstone \n",
      " ------------------------------\n",
      "67\n",
      "antecedents: data analytics foundations accountancy ii\n",
      "big datum capstone project\n",
      "consequents: big datum capstone project \n",
      " ------------------------------\n",
      "68\n",
      "antecedents: big datum capstone project\n",
      "data analytics foundations accountancy ii\n",
      "consequents: data analytics foundations accountancy ii \n",
      " ------------------------------\n",
      "69\n",
      "antecedents: advanced business analytic capstone\n",
      "big datum capstone project\n",
      "consequents: big datum capstone project \n",
      " ------------------------------\n",
      "70\n",
      "antecedents: big datum capstone project\n",
      "advanced business analytic capstone\n",
      "consequents: advanced business analytic capstone \n",
      " ------------------------------\n",
      "71\n",
      "antecedents: advanced business analytic capstone\n",
      "data analytics foundations accountancy ii\n",
      "consequents: data analytics foundations accountancy ii \n",
      " ------------------------------\n",
      "72\n",
      "antecedents: data analytics foundations accountancy ii\n",
      "advanced business analytic capstone\n",
      "consequents: advanced business analytic capstone \n",
      " ------------------------------\n",
      "73\n",
      "antecedents: curanderismo traditional healing body\n",
      "visual element user interface design\n",
      "consequents: visual element user interface design \n",
      " ------------------------------\n",
      "74\n",
      "antecedents: visual element user interface design\n",
      "curanderismo traditional healing body\n",
      "consequents: curanderismo traditional healing body \n",
      " ------------------------------\n",
      "75\n",
      "antecedents: predictive modelling azure machine learn studio\n",
      "machine learn regression\n",
      "consequents: machine learn regression \n",
      " ------------------------------\n",
      "76\n",
      "antecedents: machine learn regression\n",
      "predictive modelling azure machine learn studio\n",
      "consequents: predictive modelling azure machine learn studio \n",
      " ------------------------------\n",
      "77\n",
      "antecedents: introduction calculus\n",
      "matrix algebra engineer\n",
      "consequents: matrix algebra engineer \n",
      " ------------------------------\n",
      "78\n",
      "antecedents: matrix algebra engineer\n",
      "introduction calculus\n",
      "consequents: introduction calculus \n",
      " ------------------------------\n",
      "79\n",
      "antecedents: matrix algebra engineer\n",
      "analytic combinatoric\n",
      "consequents: analytic combinatoric \n",
      " ------------------------------\n",
      "80\n",
      "antecedents: analytic combinatoric\n",
      "matrix algebra engineer\n",
      "consequents: matrix algebra engineer \n",
      " ------------------------------\n",
      "81\n",
      "antecedents: transfer learn nlp tensorflow hub\n",
      "nlp twitter sentiment analysis\n",
      "consequents: nlp twitter sentiment analysis \n",
      " ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "antecedents: nlp twitter sentiment analysis\n",
      "transfer learn nlp tensorflow hub\n",
      "consequents: transfer learn nlp tensorflow hub \n",
      " ------------------------------\n",
      "83\n",
      "antecedents: game theory python\n",
      "logic economist\n",
      "consequents: logic economist \n",
      " ------------------------------\n",
      "84\n",
      "antecedents: logic economist\n",
      "game theory python\n",
      "consequents: game theory python \n",
      " ------------------------------\n",
      "85\n",
      "antecedents: introduction calculus\n",
      "analytic combinatoric\n",
      "consequents: analytic combinatoric \n",
      " ------------------------------\n",
      "86\n",
      "antecedents: analytic combinatoric\n",
      "introduction calculus\n",
      "consequents: introduction calculus \n",
      " ------------------------------\n",
      "87\n",
      "antecedents: image video processing mar hollywood stop hospital\n",
      "precalculus relation function\n",
      "consequents: precalculus relation function \n",
      " ------------------------------\n",
      "88\n",
      "antecedents: precalculus relation function\n",
      "image video processing mar hollywood stop hospital\n",
      "consequents: image video processing mar hollywood stop hospital \n",
      " ------------------------------\n",
      "89\n",
      "antecedents: linear regression python\n",
      "machine learn regression\n",
      "consequents: machine learn regression \n",
      " ------------------------------\n",
      "90\n",
      "antecedents: data analytic lean six sigma\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "91\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "92\n",
      "antecedents: data analytic lean six sigma\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "93\n",
      "antecedents: getting start tensorflow\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "94\n",
      "antecedents: data analytic lean six sigma\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "95\n",
      "antecedents: data analytic lean six sigma\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "96\n",
      "antecedents: communicate datum science result\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "97\n",
      "antecedents: game without chance combinatorial game theory\n",
      "mathematical biostatistic boot camp\n",
      "consequents: mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "98\n",
      "antecedents: mathematical biostatistic boot camp\n",
      "game without chance combinatorial game theory\n",
      "consequents: game without chance combinatorial game theory \n",
      " ------------------------------\n",
      "99\n",
      "antecedents: game without chance combinatorial game theory\n",
      "fibonacci number golden ratio\n",
      "consequents: fibonacci number golden ratio \n",
      " ------------------------------\n",
      "100\n",
      "antecedents: fibonacci number golden ratio\n",
      "game without chance combinatorial game theory\n",
      "consequents: game without chance combinatorial game theory \n",
      " ------------------------------\n",
      "101\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "102\n",
      "antecedents: getting start tensorflow\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "103\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "104\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "105\n",
      "antecedents: communicate datum science result\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "106\n",
      "antecedents: plant bioinformatic\n",
      "diabete global challenge\n",
      "consequents: diabete global challenge \n",
      " ------------------------------\n",
      "107\n",
      "antecedents: diabete global challenge\n",
      "plant bioinformatic\n",
      "consequents: plant bioinformatic \n",
      " ------------------------------\n",
      "108\n",
      "antecedents: health complex humanitarian emergency\n",
      "diabete global challenge\n",
      "consequents: diabete global challenge \n",
      " ------------------------------\n",
      "109\n",
      "antecedents: diabete global challenge\n",
      "health complex humanitarian emergency\n",
      "consequents: health complex humanitarian emergency \n",
      " ------------------------------\n",
      "110\n",
      "antecedents: plant bioinformatic\n",
      "health complex humanitarian emergency\n",
      "consequents: health complex humanitarian emergency \n",
      " ------------------------------\n",
      "111\n",
      "antecedents: health complex humanitarian emergency\n",
      "plant bioinformatic\n",
      "consequents: plant bioinformatic \n",
      " ------------------------------\n",
      "112\n",
      "antecedents: datum analytic accounting capstone\n",
      "business statistic analysis capstone\n",
      "consequents: business statistic analysis capstone \n",
      " ------------------------------\n",
      "113\n",
      "antecedents: business statistic analysis capstone\n",
      "datum analytic accounting capstone\n",
      "consequents: datum analytic accounting capstone \n",
      " ------------------------------\n",
      "114\n",
      "antecedents: fake news detection machine learning\n",
      "image compression k mean cluster\n",
      "consequents: image compression k mean cluster \n",
      " ------------------------------\n",
      "115\n",
      "antecedents: image compression k mean cluster\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "116\n",
      "antecedents: image compression k mean cluster\n",
      "capstone autonomous runway detection iot\n",
      "consequents: capstone autonomous runway detection iot \n",
      " ------------------------------\n",
      "117\n",
      "antecedents: capstone autonomous runway detection iot\n",
      "image compression k mean cluster\n",
      "consequents: image compression k mean cluster \n",
      " ------------------------------\n",
      "118\n",
      "antecedents: fake news detection machine learning\n",
      "capstone autonomous runway detection iot\n",
      "consequents: capstone autonomous runway detection iot \n",
      " ------------------------------\n",
      "119\n",
      "antecedents: fibonacci number golden ratio\n",
      "mathematical biostatistic boot camp\n",
      "consequents: mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "120\n",
      "antecedents: mathematical biostatistic boot camp\n",
      "fibonacci number golden ratio\n",
      "consequents: fibonacci number golden ratio \n",
      " ------------------------------\n",
      "121\n",
      "antecedents: getting start tensorflow\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "122\n",
      "antecedents: communicate datum science result\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "123\n",
      "antecedents: getting start tensorflow\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "124\n",
      "antecedents: communicate datum science result\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "125\n",
      "antecedents: matrix method, logic economist\n",
      "precalculus periodic function\n",
      "consequents: precalculus periodic function \n",
      " ------------------------------\n",
      "126\n",
      "antecedents: matrix method, precalculus periodic function\n",
      "logic economist\n",
      "consequents: logic economist \n",
      " ------------------------------\n",
      "127\n",
      "antecedents: logic economist, precalculus periodic function\n",
      "matrix method\n",
      "consequents: matrix method \n",
      " ------------------------------\n",
      "128\n",
      "antecedents: matrix method\n",
      "logic economist, precalculus periodic function\n",
      "consequents: logic economist, precalculus periodic function \n",
      " ------------------------------\n",
      "129\n",
      "antecedents: logic economist\n",
      "matrix method, precalculus periodic function\n",
      "consequents: matrix method, precalculus periodic function \n",
      " ------------------------------\n",
      "130\n",
      "antecedents: precalculus periodic function\n",
      "matrix method, logic economist\n",
      "consequents: matrix method, logic economist \n",
      " ------------------------------\n",
      "131\n",
      "antecedents: game theory python, matrix method\n",
      "precalculus periodic function\n",
      "consequents: precalculus periodic function \n",
      " ------------------------------\n",
      "132\n",
      "antecedents: game theory python, precalculus periodic function\n",
      "matrix method\n",
      "consequents: matrix method \n",
      " ------------------------------\n",
      "133\n",
      "antecedents: matrix method, precalculus periodic function\n",
      "game theory python\n",
      "consequents: game theory python \n",
      " ------------------------------\n",
      "134\n",
      "antecedents: game theory python\n",
      "matrix method, precalculus periodic function\n",
      "consequents: matrix method, precalculus periodic function \n",
      " ------------------------------\n",
      "135\n",
      "antecedents: matrix method\n",
      "game theory python, precalculus periodic function\n",
      "consequents: game theory python, precalculus periodic function \n",
      " ------------------------------\n",
      "136\n",
      "antecedents: precalculus periodic function\n",
      "game theory python, matrix method\n",
      "consequents: game theory python, matrix method \n",
      " ------------------------------\n",
      "137\n",
      "antecedents: game theory python, matrix method\n",
      "logic economist\n",
      "consequents: logic economist \n",
      " ------------------------------\n",
      "138\n",
      "antecedents: game theory python, logic economist\n",
      "matrix method\n",
      "consequents: matrix method \n",
      " ------------------------------\n",
      "139\n",
      "antecedents: matrix method, logic economist\n",
      "game theory python\n",
      "consequents: game theory python \n",
      " ------------------------------\n",
      "140\n",
      "antecedents: game theory python\n",
      "matrix method, logic economist\n",
      "consequents: matrix method, logic economist \n",
      " ------------------------------\n",
      "141\n",
      "antecedents: matrix method\n",
      "game theory python, logic economist\n",
      "consequents: game theory python, logic economist \n",
      " ------------------------------\n",
      "142\n",
      "antecedents: logic economist\n",
      "game theory python, matrix method\n",
      "consequents: game theory python, matrix method \n",
      " ------------------------------\n",
      "143\n",
      "antecedents: discrete mathematic, mathematic machine learn multivariate calculus\n",
      "game theoretic solution concept spread sheet\n",
      "consequents: game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "144\n",
      "antecedents: discrete mathematic, game theoretic solution concept spread sheet\n",
      "mathematic machine learn multivariate calculus\n",
      "consequents: mathematic machine learn multivariate calculus \n",
      " ------------------------------\n",
      "145\n",
      "antecedents: mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet\n",
      "discrete mathematic\n",
      "consequents: discrete mathematic \n",
      " ------------------------------\n",
      "146\n",
      "antecedents: discrete mathematic\n",
      "mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet\n",
      "consequents: mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "147\n",
      "antecedents: mathematic machine learn multivariate calculus\n",
      "discrete mathematic, game theoretic solution concept spread sheet\n",
      "consequents: discrete mathematic, game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "148\n",
      "antecedents: game theoretic solution concept spread sheet\n",
      "discrete mathematic, mathematic machine learn multivariate calculus\n",
      "consequents: discrete mathematic, mathematic machine learn multivariate calculus \n",
      " ------------------------------\n",
      "149\n",
      "antecedents: discrete mathematic, precalculus mathematical modeling\n",
      "game theoretic solution concept spread sheet\n",
      "consequents: game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "150\n",
      "antecedents: discrete mathematic, game theoretic solution concept spread sheet\n",
      "precalculus mathematical modeling\n",
      "consequents: precalculus mathematical modeling \n",
      " ------------------------------\n",
      "151\n",
      "antecedents: precalculus mathematical modeling, game theoretic solution concept spread sheet\n",
      "discrete mathematic\n",
      "consequents: discrete mathematic \n",
      " ------------------------------\n",
      "152\n",
      "antecedents: discrete mathematic\n",
      "precalculus mathematical modeling, game theoretic solution concept spread sheet\n",
      "consequents: precalculus mathematical modeling, game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "153\n",
      "antecedents: precalculus mathematical modeling\n",
      "discrete mathematic, game theoretic solution concept spread sheet\n",
      "consequents: discrete mathematic, game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "154\n",
      "antecedents: game theoretic solution concept spread sheet\n",
      "discrete mathematic, precalculus mathematical modeling\n",
      "consequents: discrete mathematic, precalculus mathematical modeling \n",
      " ------------------------------\n",
      "155\n",
      "antecedents: precalculus mathematical modeling, mathematic machine learn multivariate calculus\n",
      "game theoretic solution concept spread sheet\n",
      "consequents: game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "156\n",
      "antecedents: precalculus mathematical modeling, game theoretic solution concept spread sheet\n",
      "mathematic machine learn multivariate calculus\n",
      "consequents: mathematic machine learn multivariate calculus \n",
      " ------------------------------\n",
      "157\n",
      "antecedents: mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet\n",
      "precalculus mathematical modeling\n",
      "consequents: precalculus mathematical modeling \n",
      " ------------------------------\n",
      "158\n",
      "antecedents: precalculus mathematical modeling\n",
      "mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet\n",
      "consequents: mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "159\n",
      "antecedents: mathematic machine learn multivariate calculus\n",
      "precalculus mathematical modeling, game theoretic solution concept spread sheet\n",
      "consequents: precalculus mathematical modeling, game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "160\n",
      "antecedents: game theoretic solution concept spread sheet\n",
      "precalculus mathematical modeling, mathematic machine learn multivariate calculus\n",
      "consequents: precalculus mathematical modeling, mathematic machine learn multivariate calculus \n",
      " ------------------------------\n",
      "161\n",
      "antecedents: differential equation engineer, image video processing mar hollywood stop hospital\n",
      "data science math skill\n",
      "consequents: data science math skill \n",
      " ------------------------------\n",
      "162\n",
      "antecedents: differential equation engineer, data science math skill\n",
      "image video processing mar hollywood stop hospital\n",
      "consequents: image video processing mar hollywood stop hospital \n",
      " ------------------------------\n",
      "163\n",
      "antecedents: image video processing mar hollywood stop hospital, data science math skill\n",
      "differential equation engineer\n",
      "consequents: differential equation engineer \n",
      " ------------------------------\n",
      "164\n",
      "antecedents: differential equation engineer\n",
      "image video processing mar hollywood stop hospital, data science math skill\n",
      "consequents: image video processing mar hollywood stop hospital, data science math skill \n",
      " ------------------------------\n",
      "165\n",
      "antecedents: image video processing mar hollywood stop hospital\n",
      "differential equation engineer, data science math skill\n",
      "consequents: differential equation engineer, data science math skill \n",
      " ------------------------------\n",
      "166\n",
      "antecedents: data science math skill\n",
      "differential equation engineer, image video processing mar hollywood stop hospital\n",
      "consequents: differential equation engineer, image video processing mar hollywood stop hospital \n",
      " ------------------------------\n",
      "167\n",
      "antecedents: differential equation engineer, precalculus relation function\n",
      "data science math skill\n",
      "consequents: data science math skill \n",
      " ------------------------------\n",
      "168\n",
      "antecedents: differential equation engineer, data science math skill\n",
      "precalculus relation function\n",
      "consequents: precalculus relation function \n",
      " ------------------------------\n",
      "169\n",
      "antecedents: precalculus relation function, data science math skill\n",
      "differential equation engineer\n",
      "consequents: differential equation engineer \n",
      " ------------------------------\n",
      "170\n",
      "antecedents: differential equation engineer\n",
      "precalculus relation function, data science math skill\n",
      "consequents: precalculus relation function, data science math skill \n",
      " ------------------------------\n",
      "171\n",
      "antecedents: precalculus relation function\n",
      "differential equation engineer, data science math skill\n",
      "consequents: differential equation engineer, data science math skill \n",
      " ------------------------------\n",
      "172\n",
      "antecedents: data science math skill\n",
      "differential equation engineer, precalculus relation function\n",
      "consequents: differential equation engineer, precalculus relation function \n",
      " ------------------------------\n",
      "173\n",
      "antecedents: image video processing mar hollywood stop hospital, precalculus relation function\n",
      "data science math skill\n",
      "consequents: data science math skill \n",
      " ------------------------------\n",
      "174\n",
      "antecedents: image video processing mar hollywood stop hospital, data science math skill\n",
      "precalculus relation function\n",
      "consequents: precalculus relation function \n",
      " ------------------------------\n",
      "175\n",
      "antecedents: precalculus relation function, data science math skill\n",
      "image video processing mar hollywood stop hospital\n",
      "consequents: image video processing mar hollywood stop hospital \n",
      " ------------------------------\n",
      "176\n",
      "antecedents: image video processing mar hollywood stop hospital\n",
      "precalculus relation function, data science math skill\n",
      "consequents: precalculus relation function, data science math skill \n",
      " ------------------------------\n",
      "177\n",
      "antecedents: precalculus relation function\n",
      "image video processing mar hollywood stop hospital, data science math skill\n",
      "consequents: image video processing mar hollywood stop hospital, data science math skill \n",
      " ------------------------------\n",
      "178\n",
      "antecedents: data science math skill\n",
      "image video processing mar hollywood stop hospital, precalculus relation function\n",
      "consequents: image video processing mar hollywood stop hospital, precalculus relation function \n",
      " ------------------------------\n",
      "179\n",
      "antecedents: differential equation engineer, image video processing mar hollywood stop hospital\n",
      "precalculus relation function\n",
      "consequents: precalculus relation function \n",
      " ------------------------------\n",
      "180\n",
      "antecedents: differential equation engineer, precalculus relation function\n",
      "image video processing mar hollywood stop hospital\n",
      "consequents: image video processing mar hollywood stop hospital \n",
      " ------------------------------\n",
      "181\n",
      "antecedents: image video processing mar hollywood stop hospital, precalculus relation function\n",
      "differential equation engineer\n",
      "consequents: differential equation engineer \n",
      " ------------------------------\n",
      "182\n",
      "antecedents: differential equation engineer\n",
      "image video processing mar hollywood stop hospital, precalculus relation function\n",
      "consequents: image video processing mar hollywood stop hospital, precalculus relation function \n",
      " ------------------------------\n",
      "183\n",
      "antecedents: image video processing mar hollywood stop hospital\n",
      "differential equation engineer, precalculus relation function\n",
      "consequents: differential equation engineer, precalculus relation function \n",
      " ------------------------------\n",
      "184\n",
      "antecedents: precalculus relation function\n",
      "differential equation engineer, image video processing mar hollywood stop hospital\n",
      "consequents: differential equation engineer, image video processing mar hollywood stop hospital \n",
      " ------------------------------\n",
      "185\n",
      "antecedents: precalculus mathematical modeling, discrete mathematic\n",
      "mathematic machine learn multivariate calculus\n",
      "consequents: mathematic machine learn multivariate calculus \n",
      " ------------------------------\n",
      "186\n",
      "antecedents: precalculus mathematical modeling, mathematic machine learn multivariate calculus\n",
      "discrete mathematic\n",
      "consequents: discrete mathematic \n",
      " ------------------------------\n",
      "187\n",
      "antecedents: discrete mathematic, mathematic machine learn multivariate calculus\n",
      "precalculus mathematical modeling\n",
      "consequents: precalculus mathematical modeling \n",
      " ------------------------------\n",
      "188\n",
      "antecedents: precalculus mathematical modeling\n",
      "discrete mathematic, mathematic machine learn multivariate calculus\n",
      "consequents: discrete mathematic, mathematic machine learn multivariate calculus \n",
      " ------------------------------\n",
      "189\n",
      "antecedents: discrete mathematic\n",
      "precalculus mathematical modeling, mathematic machine learn multivariate calculus\n",
      "consequents: precalculus mathematical modeling, mathematic machine learn multivariate calculus \n",
      " ------------------------------\n",
      "190\n",
      "antecedents: mathematic machine learn multivariate calculus\n",
      "precalculus mathematical modeling, discrete mathematic\n",
      "consequents: precalculus mathematical modeling, discrete mathematic \n",
      " ------------------------------\n",
      "191\n",
      "antecedents: chinese hsk, chinese hsk\n",
      "chinese hsk part\n",
      "consequents: chinese hsk part \n",
      " ------------------------------\n",
      "192\n",
      "antecedents: chinese hsk, chinese hsk part\n",
      "chinese hsk\n",
      "consequents: chinese hsk \n",
      " ------------------------------\n",
      "193\n",
      "antecedents: chinese hsk, chinese hsk part\n",
      "chinese hsk\n",
      "consequents: chinese hsk \n",
      " ------------------------------\n",
      "194\n",
      "antecedents: chinese hsk\n",
      "chinese hsk, chinese hsk part\n",
      "consequents: chinese hsk, chinese hsk part \n",
      " ------------------------------\n",
      "195\n",
      "antecedents: chinese hsk\n",
      "chinese hsk, chinese hsk part\n",
      "consequents: chinese hsk, chinese hsk part \n",
      " ------------------------------\n",
      "196\n",
      "antecedents: chinese hsk part\n",
      "chinese hsk, chinese hsk\n",
      "consequents: chinese hsk, chinese hsk \n",
      " ------------------------------\n",
      "197\n",
      "antecedents: introduction complex analysis, game without chance combinatorial game theory\n",
      "mathematical biostatistic boot camp\n",
      "consequents: mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "198\n",
      "antecedents: introduction complex analysis, mathematical biostatistic boot camp\n",
      "game without chance combinatorial game theory\n",
      "consequents: game without chance combinatorial game theory \n",
      " ------------------------------\n",
      "199\n",
      "antecedents: game without chance combinatorial game theory, mathematical biostatistic boot camp\n",
      "introduction complex analysis\n",
      "consequents: introduction complex analysis \n",
      " ------------------------------\n",
      "200\n",
      "antecedents: introduction complex analysis\n",
      "game without chance combinatorial game theory, mathematical biostatistic boot camp\n",
      "consequents: game without chance combinatorial game theory, mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "201\n",
      "antecedents: game without chance combinatorial game theory\n",
      "introduction complex analysis, mathematical biostatistic boot camp\n",
      "consequents: introduction complex analysis, mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "202\n",
      "antecedents: mathematical biostatistic boot camp\n",
      "introduction complex analysis, game without chance combinatorial game theory\n",
      "consequents: introduction complex analysis, game without chance combinatorial game theory \n",
      " ------------------------------\n",
      "203\n",
      "antecedents: introduction complex analysis, game without chance combinatorial game theory\n",
      "fibonacci number golden ratio\n",
      "consequents: fibonacci number golden ratio \n",
      " ------------------------------\n",
      "204\n",
      "antecedents: introduction complex analysis, fibonacci number golden ratio\n",
      "game without chance combinatorial game theory\n",
      "consequents: game without chance combinatorial game theory \n",
      " ------------------------------\n",
      "205\n",
      "antecedents: game without chance combinatorial game theory, fibonacci number golden ratio\n",
      "introduction complex analysis\n",
      "consequents: introduction complex analysis \n",
      " ------------------------------\n",
      "206\n",
      "antecedents: introduction complex analysis\n",
      "game without chance combinatorial game theory, fibonacci number golden ratio\n",
      "consequents: game without chance combinatorial game theory, fibonacci number golden ratio \n",
      " ------------------------------\n",
      "207\n",
      "antecedents: game without chance combinatorial game theory\n",
      "introduction complex analysis, fibonacci number golden ratio\n",
      "consequents: introduction complex analysis, fibonacci number golden ratio \n",
      " ------------------------------\n",
      "208\n",
      "antecedents: fibonacci number golden ratio\n",
      "introduction complex analysis, game without chance combinatorial game theory\n",
      "consequents: introduction complex analysis, game without chance combinatorial game theory \n",
      " ------------------------------\n",
      "209\n",
      "antecedents: introduction complex analysis, fibonacci number golden ratio\n",
      "mathematical biostatistic boot camp\n",
      "consequents: mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "210\n",
      "antecedents: introduction complex analysis, mathematical biostatistic boot camp\n",
      "fibonacci number golden ratio\n",
      "consequents: fibonacci number golden ratio \n",
      " ------------------------------\n",
      "211\n",
      "antecedents: fibonacci number golden ratio, mathematical biostatistic boot camp\n",
      "introduction complex analysis\n",
      "consequents: introduction complex analysis \n",
      " ------------------------------\n",
      "212\n",
      "antecedents: introduction complex analysis\n",
      "fibonacci number golden ratio, mathematical biostatistic boot camp\n",
      "consequents: fibonacci number golden ratio, mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "213\n",
      "antecedents: fibonacci number golden ratio\n",
      "introduction complex analysis, mathematical biostatistic boot camp\n",
      "consequents: introduction complex analysis, mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "214\n",
      "antecedents: mathematical biostatistic boot camp\n",
      "introduction complex analysis, fibonacci number golden ratio\n",
      "consequents: introduction complex analysis, fibonacci number golden ratio \n",
      " ------------------------------\n",
      "215\n",
      "antecedents: datum analytic accounting capstone, business statistic analysis capstone\n",
      "advanced business analytic capstone\n",
      "consequents: advanced business analytic capstone \n",
      " ------------------------------\n",
      "216\n",
      "antecedents: datum analytic accounting capstone, advanced business analytic capstone\n",
      "business statistic analysis capstone\n",
      "consequents: business statistic analysis capstone \n",
      " ------------------------------\n",
      "217\n",
      "antecedents: business statistic analysis capstone, advanced business analytic capstone\n",
      "datum analytic accounting capstone\n",
      "consequents: datum analytic accounting capstone \n",
      " ------------------------------\n",
      "218\n",
      "antecedents: datum analytic accounting capstone\n",
      "business statistic analysis capstone, advanced business analytic capstone\n",
      "consequents: business statistic analysis capstone, advanced business analytic capstone \n",
      " ------------------------------\n",
      "219\n",
      "antecedents: business statistic analysis capstone\n",
      "datum analytic accounting capstone, advanced business analytic capstone\n",
      "consequents: datum analytic accounting capstone, advanced business analytic capstone \n",
      " ------------------------------\n",
      "220\n",
      "antecedents: advanced business analytic capstone\n",
      "datum analytic accounting capstone, business statistic analysis capstone\n",
      "consequents: datum analytic accounting capstone, business statistic analysis capstone \n",
      " ------------------------------\n",
      "221\n",
      "antecedents: game theory python, logic economist\n",
      "precalculus periodic function\n",
      "consequents: precalculus periodic function \n",
      " ------------------------------\n",
      "222\n",
      "antecedents: game theory python, precalculus periodic function\n",
      "logic economist\n",
      "consequents: logic economist \n",
      " ------------------------------\n",
      "223\n",
      "antecedents: logic economist, precalculus periodic function\n",
      "game theory python\n",
      "consequents: game theory python \n",
      " ------------------------------\n",
      "224\n",
      "antecedents: game theory python\n",
      "logic economist, precalculus periodic function\n",
      "consequents: logic economist, precalculus periodic function \n",
      " ------------------------------\n",
      "225\n",
      "antecedents: logic economist\n",
      "game theory python, precalculus periodic function\n",
      "consequents: game theory python, precalculus periodic function \n",
      " ------------------------------\n",
      "226\n",
      "antecedents: precalculus periodic function\n",
      "game theory python, logic economist\n",
      "consequents: game theory python, logic economist \n",
      " ------------------------------\n",
      "227\n",
      "antecedents: introduction calculus, vector calculus engineer\n",
      "matrix algebra engineer\n",
      "consequents: matrix algebra engineer \n",
      " ------------------------------\n",
      "228\n",
      "antecedents: introduction calculus, matrix algebra engineer\n",
      "vector calculus engineer\n",
      "consequents: vector calculus engineer \n",
      " ------------------------------\n",
      "229\n",
      "antecedents: vector calculus engineer, matrix algebra engineer\n",
      "introduction calculus\n",
      "consequents: introduction calculus \n",
      " ------------------------------\n",
      "230\n",
      "antecedents: introduction calculus\n",
      "vector calculus engineer, matrix algebra engineer\n",
      "consequents: vector calculus engineer, matrix algebra engineer \n",
      " ------------------------------\n",
      "231\n",
      "antecedents: vector calculus engineer\n",
      "introduction calculus, matrix algebra engineer\n",
      "consequents: introduction calculus, matrix algebra engineer \n",
      " ------------------------------\n",
      "232\n",
      "antecedents: matrix algebra engineer\n",
      "introduction calculus, vector calculus engineer\n",
      "consequents: introduction calculus, vector calculus engineer \n",
      " ------------------------------\n",
      "233\n",
      "antecedents: vector calculus engineer, matrix algebra engineer\n",
      "analytic combinatoric\n",
      "consequents: analytic combinatoric \n",
      " ------------------------------\n",
      "234\n",
      "antecedents: analytic combinatoric, matrix algebra engineer\n",
      "vector calculus engineer\n",
      "consequents: vector calculus engineer \n",
      " ------------------------------\n",
      "235\n",
      "antecedents: vector calculus engineer, analytic combinatoric\n",
      "matrix algebra engineer\n",
      "consequents: matrix algebra engineer \n",
      " ------------------------------\n",
      "236\n",
      "antecedents: matrix algebra engineer\n",
      "vector calculus engineer, analytic combinatoric\n",
      "consequents: vector calculus engineer, analytic combinatoric \n",
      " ------------------------------\n",
      "237\n",
      "antecedents: vector calculus engineer\n",
      "analytic combinatoric, matrix algebra engineer\n",
      "consequents: analytic combinatoric, matrix algebra engineer \n",
      " ------------------------------\n",
      "238\n",
      "antecedents: analytic combinatoric\n",
      "vector calculus engineer, matrix algebra engineer\n",
      "consequents: vector calculus engineer, matrix algebra engineer \n",
      " ------------------------------\n",
      "239\n",
      "antecedents: introduction calculus, vector calculus engineer\n",
      "analytic combinatoric\n",
      "consequents: analytic combinatoric \n",
      " ------------------------------\n",
      "240\n",
      "antecedents: introduction calculus, analytic combinatoric\n",
      "vector calculus engineer\n",
      "consequents: vector calculus engineer \n",
      " ------------------------------\n",
      "241\n",
      "antecedents: vector calculus engineer, analytic combinatoric\n",
      "introduction calculus\n",
      "consequents: introduction calculus \n",
      " ------------------------------\n",
      "242\n",
      "antecedents: introduction calculus\n",
      "vector calculus engineer, analytic combinatoric\n",
      "consequents: vector calculus engineer, analytic combinatoric \n",
      " ------------------------------\n",
      "243\n",
      "antecedents: vector calculus engineer\n",
      "introduction calculus, analytic combinatoric\n",
      "consequents: introduction calculus, analytic combinatoric \n",
      " ------------------------------\n",
      "244\n",
      "antecedents: analytic combinatoric\n",
      "introduction calculus, vector calculus engineer\n",
      "consequents: introduction calculus, vector calculus engineer \n",
      " ------------------------------\n",
      "245\n",
      "antecedents: data analytics foundations accountancy ii, datum analysis interpretation capstone\n",
      "big datum capstone project\n",
      "consequents: big datum capstone project \n",
      " ------------------------------\n",
      "246\n",
      "antecedents: datum analysis interpretation capstone, big datum capstone project\n",
      "data analytics foundations accountancy ii\n",
      "consequents: data analytics foundations accountancy ii \n",
      " ------------------------------\n",
      "247\n",
      "antecedents: datum analysis interpretation capstone\n",
      "data analytics foundations accountancy ii, big datum capstone project\n",
      "consequents: data analytics foundations accountancy ii, big datum capstone project \n",
      " ------------------------------\n",
      "248\n",
      "antecedents: datum analysis interpretation capstone, advanced business analytic capstone\n",
      "big datum capstone project\n",
      "consequents: big datum capstone project \n",
      " ------------------------------\n",
      "249\n",
      "antecedents: datum analysis interpretation capstone, big datum capstone project\n",
      "advanced business analytic capstone\n",
      "consequents: advanced business analytic capstone \n",
      " ------------------------------\n",
      "250\n",
      "antecedents: advanced business analytic capstone, big datum capstone project\n",
      "datum analysis interpretation capstone\n",
      "consequents: datum analysis interpretation capstone \n",
      " ------------------------------\n",
      "251\n",
      "antecedents: datum analysis interpretation capstone\n",
      "advanced business analytic capstone, big datum capstone project\n",
      "consequents: advanced business analytic capstone, big datum capstone project \n",
      " ------------------------------\n",
      "252\n",
      "antecedents: advanced business analytic capstone\n",
      "datum analysis interpretation capstone, big datum capstone project\n",
      "consequents: datum analysis interpretation capstone, big datum capstone project \n",
      " ------------------------------\n",
      "253\n",
      "antecedents: datum analysis interpretation capstone, advanced business analytic capstone\n",
      "data analytics foundations accountancy ii\n",
      "consequents: data analytics foundations accountancy ii \n",
      " ------------------------------\n",
      "254\n",
      "antecedents: datum analysis interpretation capstone, data analytics foundations accountancy ii\n",
      "advanced business analytic capstone\n",
      "consequents: advanced business analytic capstone \n",
      " ------------------------------\n",
      "255\n",
      "antecedents: advanced business analytic capstone, data analytics foundations accountancy ii\n",
      "datum analysis interpretation capstone\n",
      "consequents: datum analysis interpretation capstone \n",
      " ------------------------------\n",
      "256\n",
      "antecedents: datum analysis interpretation capstone\n",
      "advanced business analytic capstone, data analytics foundations accountancy ii\n",
      "consequents: advanced business analytic capstone, data analytics foundations accountancy ii \n",
      " ------------------------------\n",
      "257\n",
      "antecedents: advanced business analytic capstone\n",
      "datum analysis interpretation capstone, data analytics foundations accountancy ii\n",
      "consequents: datum analysis interpretation capstone, data analytics foundations accountancy ii \n",
      " ------------------------------\n",
      "258\n",
      "antecedents: data analytics foundations accountancy ii, advanced business analytic capstone\n",
      "big datum capstone project\n",
      "consequents: big datum capstone project \n",
      " ------------------------------\n",
      "259\n",
      "antecedents: data analytics foundations accountancy ii, big datum capstone project\n",
      "advanced business analytic capstone\n",
      "consequents: advanced business analytic capstone \n",
      " ------------------------------\n",
      "260\n",
      "antecedents: advanced business analytic capstone, big datum capstone project\n",
      "data analytics foundations accountancy ii\n",
      "consequents: data analytics foundations accountancy ii \n",
      " ------------------------------\n",
      "261\n",
      "antecedents: data analytics foundations accountancy ii\n",
      "advanced business analytic capstone, big datum capstone project\n",
      "consequents: advanced business analytic capstone, big datum capstone project \n",
      " ------------------------------\n",
      "262\n",
      "antecedents: advanced business analytic capstone\n",
      "data analytics foundations accountancy ii, big datum capstone project\n",
      "consequents: data analytics foundations accountancy ii, big datum capstone project \n",
      " ------------------------------\n",
      "263\n",
      "antecedents: big datum capstone project\n",
      "data analytics foundations accountancy ii, advanced business analytic capstone\n",
      "consequents: data analytics foundations accountancy ii, advanced business analytic capstone \n",
      " ------------------------------\n",
      "264\n",
      "antecedents: introduction calculus, matrix algebra engineer\n",
      "analytic combinatoric\n",
      "consequents: analytic combinatoric \n",
      " ------------------------------\n",
      "265\n",
      "antecedents: analytic combinatoric, matrix algebra engineer\n",
      "introduction calculus\n",
      "consequents: introduction calculus \n",
      " ------------------------------\n",
      "266\n",
      "antecedents: introduction calculus, analytic combinatoric\n",
      "matrix algebra engineer\n",
      "consequents: matrix algebra engineer \n",
      " ------------------------------\n",
      "267\n",
      "antecedents: matrix algebra engineer\n",
      "introduction calculus, analytic combinatoric\n",
      "consequents: introduction calculus, analytic combinatoric \n",
      " ------------------------------\n",
      "268\n",
      "antecedents: introduction calculus\n",
      "analytic combinatoric, matrix algebra engineer\n",
      "consequents: analytic combinatoric, matrix algebra engineer \n",
      " ------------------------------\n",
      "269\n",
      "antecedents: analytic combinatoric\n",
      "introduction calculus, matrix algebra engineer\n",
      "consequents: introduction calculus, matrix algebra engineer \n",
      " ------------------------------\n",
      "270\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "271\n",
      "antecedents: data analytic lean six sigma, getting start tensorflow\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "272\n",
      "antecedents: increase real estate management profit harness data analytic, getting start tensorflow\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "273\n",
      "antecedents: data analytic lean six sigma\n",
      "increase real estate management profit harness data analytic, getting start tensorflow\n",
      "consequents: increase real estate management profit harness data analytic, getting start tensorflow \n",
      " ------------------------------\n",
      "274\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "data analytic lean six sigma, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, getting start tensorflow \n",
      " ------------------------------\n",
      "275\n",
      "antecedents: getting start tensorflow\n",
      "data analytic lean six sigma, increase real estate management profit harness data analytic\n",
      "consequents: data analytic lean six sigma, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "276\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "277\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "278\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "279\n",
      "antecedents: data analytic lean six sigma\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "280\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "data analytic lean six sigma, datum modeling regression analysis business\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business \n",
      " ------------------------------\n",
      "281\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "282\n",
      "antecedents: data analytic lean six sigma, communicate datum science result\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "283\n",
      "antecedents: increase real estate management profit harness data analytic, communicate datum science result\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "284\n",
      "antecedents: data analytic lean six sigma\n",
      "increase real estate management profit harness data analytic, communicate datum science result\n",
      "consequents: increase real estate management profit harness data analytic, communicate datum science result \n",
      " ------------------------------\n",
      "285\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "data analytic lean six sigma, communicate datum science result\n",
      "consequents: data analytic lean six sigma, communicate datum science result \n",
      " ------------------------------\n",
      "286\n",
      "antecedents: communicate datum science result\n",
      "data analytic lean six sigma, increase real estate management profit harness data analytic\n",
      "consequents: data analytic lean six sigma, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "287\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "288\n",
      "antecedents: data analytic lean six sigma, getting start tensorflow\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "289\n",
      "antecedents: datum modeling regression analysis business, getting start tensorflow\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "290\n",
      "antecedents: data analytic lean six sigma\n",
      "datum modeling regression analysis business, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, getting start tensorflow \n",
      " ------------------------------\n",
      "291\n",
      "antecedents: getting start tensorflow\n",
      "data analytic lean six sigma, datum modeling regression analysis business\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business \n",
      " ------------------------------\n",
      "292\n",
      "antecedents: data analytic lean six sigma, communicate datum science result\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "293\n",
      "antecedents: data analytic lean six sigma, getting start tensorflow\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "294\n",
      "antecedents: communicate datum science result, getting start tensorflow\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "295\n",
      "antecedents: data analytic lean six sigma\n",
      "communicate datum science result, getting start tensorflow\n",
      "consequents: communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "296\n",
      "antecedents: communicate datum science result\n",
      "data analytic lean six sigma, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, getting start tensorflow \n",
      " ------------------------------\n",
      "297\n",
      "antecedents: getting start tensorflow\n",
      "data analytic lean six sigma, communicate datum science result\n",
      "consequents: data analytic lean six sigma, communicate datum science result \n",
      " ------------------------------\n",
      "298\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "299\n",
      "antecedents: data analytic lean six sigma, communicate datum science result\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "300\n",
      "antecedents: datum modeling regression analysis business, communicate datum science result\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "301\n",
      "antecedents: data analytic lean six sigma\n",
      "datum modeling regression analysis business, communicate datum science result\n",
      "consequents: datum modeling regression analysis business, communicate datum science result \n",
      " ------------------------------\n",
      "302\n",
      "antecedents: communicate datum science result\n",
      "data analytic lean six sigma, datum modeling regression analysis business\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business \n",
      " ------------------------------\n",
      "303\n",
      "antecedents: game without chance combinatorial game theory, fibonacci number golden ratio\n",
      "mathematical biostatistic boot camp\n",
      "consequents: mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "304\n",
      "antecedents: game without chance combinatorial game theory, mathematical biostatistic boot camp\n",
      "fibonacci number golden ratio\n",
      "consequents: fibonacci number golden ratio \n",
      " ------------------------------\n",
      "305\n",
      "antecedents: fibonacci number golden ratio, mathematical biostatistic boot camp\n",
      "game without chance combinatorial game theory\n",
      "consequents: game without chance combinatorial game theory \n",
      " ------------------------------\n",
      "306\n",
      "antecedents: game without chance combinatorial game theory\n",
      "fibonacci number golden ratio, mathematical biostatistic boot camp\n",
      "consequents: fibonacci number golden ratio, mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "307\n",
      "antecedents: fibonacci number golden ratio\n",
      "game without chance combinatorial game theory, mathematical biostatistic boot camp\n",
      "consequents: game without chance combinatorial game theory, mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "308\n",
      "antecedents: mathematical biostatistic boot camp\n",
      "game without chance combinatorial game theory, fibonacci number golden ratio\n",
      "consequents: game without chance combinatorial game theory, fibonacci number golden ratio \n",
      " ------------------------------\n",
      "309\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "310\n",
      "antecedents: datum modeling regression analysis business, getting start tensorflow\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "311\n",
      "antecedents: increase real estate management profit harness data analytic, getting start tensorflow\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "312\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "datum modeling regression analysis business, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, getting start tensorflow \n",
      " ------------------------------\n",
      "313\n",
      "antecedents: getting start tensorflow\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "314\n",
      "antecedents: increase real estate management profit harness data analytic, communicate datum science result\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "315\n",
      "antecedents: increase real estate management profit harness data analytic, getting start tensorflow\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "316\n",
      "antecedents: communicate datum science result, getting start tensorflow\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "317\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "communicate datum science result, getting start tensorflow\n",
      "consequents: communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "318\n",
      "antecedents: communicate datum science result\n",
      "increase real estate management profit harness data analytic, getting start tensorflow\n",
      "consequents: increase real estate management profit harness data analytic, getting start tensorflow \n",
      " ------------------------------\n",
      "319\n",
      "antecedents: getting start tensorflow\n",
      "increase real estate management profit harness data analytic, communicate datum science result\n",
      "consequents: increase real estate management profit harness data analytic, communicate datum science result \n",
      " ------------------------------\n",
      "320\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "321\n",
      "antecedents: datum modeling regression analysis business, communicate datum science result\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "322\n",
      "antecedents: increase real estate management profit harness data analytic, communicate datum science result\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "323\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "datum modeling regression analysis business, communicate datum science result\n",
      "consequents: datum modeling regression analysis business, communicate datum science result \n",
      " ------------------------------\n",
      "324\n",
      "antecedents: communicate datum science result\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "325\n",
      "antecedents: plant bioinformatic, health complex humanitarian emergency\n",
      "diabete global challenge\n",
      "consequents: diabete global challenge \n",
      " ------------------------------\n",
      "326\n",
      "antecedents: plant bioinformatic, diabete global challenge\n",
      "health complex humanitarian emergency\n",
      "consequents: health complex humanitarian emergency \n",
      " ------------------------------\n",
      "327\n",
      "antecedents: health complex humanitarian emergency, diabete global challenge\n",
      "plant bioinformatic\n",
      "consequents: plant bioinformatic \n",
      " ------------------------------\n",
      "328\n",
      "antecedents: plant bioinformatic\n",
      "health complex humanitarian emergency, diabete global challenge\n",
      "consequents: health complex humanitarian emergency, diabete global challenge \n",
      " ------------------------------\n",
      "329\n",
      "antecedents: health complex humanitarian emergency\n",
      "plant bioinformatic, diabete global challenge\n",
      "consequents: plant bioinformatic, diabete global challenge \n",
      " ------------------------------\n",
      "330\n",
      "antecedents: diabete global challenge\n",
      "plant bioinformatic, health complex humanitarian emergency\n",
      "consequents: plant bioinformatic, health complex humanitarian emergency \n",
      " ------------------------------\n",
      "331\n",
      "antecedents: image compression k mean cluster, fake news detection machine learning\n",
      "capstone autonomous runway detection iot\n",
      "consequents: capstone autonomous runway detection iot \n",
      " ------------------------------\n",
      "332\n",
      "antecedents: capstone autonomous runway detection iot, fake news detection machine learning\n",
      "image compression k mean cluster\n",
      "consequents: image compression k mean cluster \n",
      " ------------------------------\n",
      "333\n",
      "antecedents: fake news detection machine learning\n",
      "image compression k mean cluster, capstone autonomous runway detection iot\n",
      "consequents: image compression k mean cluster, capstone autonomous runway detection iot \n",
      " ------------------------------\n",
      "334\n",
      "antecedents: image compression k mean cluster, capstone autonomous runway detection iot\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "335\n",
      "antecedents: image compression k mean cluster, datum modeling regression analysis business\n",
      "capstone autonomous runway detection iot\n",
      "consequents: capstone autonomous runway detection iot \n",
      " ------------------------------\n",
      "336\n",
      "antecedents: datum modeling regression analysis business, capstone autonomous runway detection iot\n",
      "image compression k mean cluster\n",
      "consequents: image compression k mean cluster \n",
      " ------------------------------\n",
      "337\n",
      "antecedents: image compression k mean cluster\n",
      "datum modeling regression analysis business, capstone autonomous runway detection iot\n",
      "consequents: datum modeling regression analysis business, capstone autonomous runway detection iot \n",
      " ------------------------------\n",
      "338\n",
      "antecedents: datum modeling regression analysis business, communicate datum science result\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "339\n",
      "antecedents: datum modeling regression analysis business, getting start tensorflow\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "340\n",
      "antecedents: communicate datum science result, getting start tensorflow\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "341\n",
      "antecedents: communicate datum science result\n",
      "datum modeling regression analysis business, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, getting start tensorflow \n",
      " ------------------------------\n",
      "342\n",
      "antecedents: getting start tensorflow\n",
      "datum modeling regression analysis business, communicate datum science result\n",
      "consequents: datum modeling regression analysis business, communicate datum science result \n",
      " ------------------------------\n",
      "343\n",
      "antecedents: game theory python, matrix method, logic economist\n",
      "precalculus periodic function\n",
      "consequents: precalculus periodic function \n",
      " ------------------------------\n",
      "344\n",
      "antecedents: game theory python, matrix method, precalculus periodic function\n",
      "logic economist\n",
      "consequents: logic economist \n",
      " ------------------------------\n",
      "345\n",
      "antecedents: game theory python, logic economist, precalculus periodic function\n",
      "matrix method\n",
      "consequents: matrix method \n",
      " ------------------------------\n",
      "346\n",
      "antecedents: matrix method, logic economist, precalculus periodic function\n",
      "game theory python\n",
      "consequents: game theory python \n",
      " ------------------------------\n",
      "347\n",
      "antecedents: game theory python, matrix method\n",
      "logic economist, precalculus periodic function\n",
      "consequents: logic economist, precalculus periodic function \n",
      " ------------------------------\n",
      "348\n",
      "antecedents: game theory python, logic economist\n",
      "matrix method, precalculus periodic function\n",
      "consequents: matrix method, precalculus periodic function \n",
      " ------------------------------\n",
      "349\n",
      "antecedents: game theory python, precalculus periodic function\n",
      "matrix method, logic economist\n",
      "consequents: matrix method, logic economist \n",
      " ------------------------------\n",
      "350\n",
      "antecedents: matrix method, logic economist\n",
      "game theory python, precalculus periodic function\n",
      "consequents: game theory python, precalculus periodic function \n",
      " ------------------------------\n",
      "351\n",
      "antecedents: matrix method, precalculus periodic function\n",
      "game theory python, logic economist\n",
      "consequents: game theory python, logic economist \n",
      " ------------------------------\n",
      "352\n",
      "antecedents: logic economist, precalculus periodic function\n",
      "game theory python, matrix method\n",
      "consequents: game theory python, matrix method \n",
      " ------------------------------\n",
      "353\n",
      "antecedents: game theory python\n",
      "matrix method, logic economist, precalculus periodic function\n",
      "consequents: matrix method, logic economist, precalculus periodic function \n",
      " ------------------------------\n",
      "354\n",
      "antecedents: matrix method\n",
      "game theory python, logic economist, precalculus periodic function\n",
      "consequents: game theory python, logic economist, precalculus periodic function \n",
      " ------------------------------\n",
      "355\n",
      "antecedents: logic economist\n",
      "game theory python, matrix method, precalculus periodic function\n",
      "consequents: game theory python, matrix method, precalculus periodic function \n",
      " ------------------------------\n",
      "356\n",
      "antecedents: precalculus periodic function\n",
      "game theory python, matrix method, logic economist\n",
      "consequents: game theory python, matrix method, logic economist \n",
      " ------------------------------\n",
      "357\n",
      "antecedents: mathematic machine learn multivariate calculus, precalculus mathematical modeling, discrete mathematic\n",
      "game theoretic solution concept spread sheet\n",
      "consequents: game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "358\n",
      "antecedents: precalculus mathematical modeling, discrete mathematic, game theoretic solution concept spread sheet\n",
      "mathematic machine learn multivariate calculus\n",
      "consequents: mathematic machine learn multivariate calculus \n",
      " ------------------------------\n",
      "359\n",
      "antecedents: precalculus mathematical modeling, mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet\n",
      "discrete mathematic\n",
      "consequents: discrete mathematic \n",
      " ------------------------------\n",
      "360\n",
      "antecedents: discrete mathematic, mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet\n",
      "precalculus mathematical modeling\n",
      "consequents: precalculus mathematical modeling \n",
      " ------------------------------\n",
      "361\n",
      "antecedents: precalculus mathematical modeling, discrete mathematic\n",
      "mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet\n",
      "consequents: mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "362\n",
      "antecedents: precalculus mathematical modeling, mathematic machine learn multivariate calculus\n",
      "discrete mathematic, game theoretic solution concept spread sheet\n",
      "consequents: discrete mathematic, game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "363\n",
      "antecedents: precalculus mathematical modeling, game theoretic solution concept spread sheet\n",
      "discrete mathematic, mathematic machine learn multivariate calculus\n",
      "consequents: discrete mathematic, mathematic machine learn multivariate calculus \n",
      " ------------------------------\n",
      "364\n",
      "antecedents: discrete mathematic, mathematic machine learn multivariate calculus\n",
      "precalculus mathematical modeling, game theoretic solution concept spread sheet\n",
      "consequents: precalculus mathematical modeling, game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "365\n",
      "antecedents: discrete mathematic, game theoretic solution concept spread sheet\n",
      "precalculus mathematical modeling, mathematic machine learn multivariate calculus\n",
      "consequents: precalculus mathematical modeling, mathematic machine learn multivariate calculus \n",
      " ------------------------------\n",
      "366\n",
      "antecedents: mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet\n",
      "precalculus mathematical modeling, discrete mathematic\n",
      "consequents: precalculus mathematical modeling, discrete mathematic \n",
      " ------------------------------\n",
      "367\n",
      "antecedents: precalculus mathematical modeling\n",
      "discrete mathematic, mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet\n",
      "consequents: discrete mathematic, mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "368\n",
      "antecedents: discrete mathematic\n",
      "precalculus mathematical modeling, mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet\n",
      "consequents: precalculus mathematical modeling, mathematic machine learn multivariate calculus, game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "369\n",
      "antecedents: mathematic machine learn multivariate calculus\n",
      "precalculus mathematical modeling, discrete mathematic, game theoretic solution concept spread sheet\n",
      "consequents: precalculus mathematical modeling, discrete mathematic, game theoretic solution concept spread sheet \n",
      " ------------------------------\n",
      "370\n",
      "antecedents: game theoretic solution concept spread sheet\n",
      "mathematic machine learn multivariate calculus, precalculus mathematical modeling, discrete mathematic\n",
      "consequents: mathematic machine learn multivariate calculus, precalculus mathematical modeling, discrete mathematic \n",
      " ------------------------------\n",
      "371\n",
      "antecedents: differential equation engineer, image video processing mar hollywood stop hospital, precalculus relation function\n",
      "data science math skill\n",
      "consequents: data science math skill \n",
      " ------------------------------\n",
      "372\n",
      "antecedents: differential equation engineer, image video processing mar hollywood stop hospital, data science math skill\n",
      "precalculus relation function\n",
      "consequents: precalculus relation function \n",
      " ------------------------------\n",
      "373\n",
      "antecedents: differential equation engineer, precalculus relation function, data science math skill\n",
      "image video processing mar hollywood stop hospital\n",
      "consequents: image video processing mar hollywood stop hospital \n",
      " ------------------------------\n",
      "374\n",
      "antecedents: image video processing mar hollywood stop hospital, precalculus relation function, data science math skill\n",
      "differential equation engineer\n",
      "consequents: differential equation engineer \n",
      " ------------------------------\n",
      "375\n",
      "antecedents: differential equation engineer, image video processing mar hollywood stop hospital\n",
      "precalculus relation function, data science math skill\n",
      "consequents: precalculus relation function, data science math skill \n",
      " ------------------------------\n",
      "376\n",
      "antecedents: differential equation engineer, precalculus relation function\n",
      "image video processing mar hollywood stop hospital, data science math skill\n",
      "consequents: image video processing mar hollywood stop hospital, data science math skill \n",
      " ------------------------------\n",
      "377\n",
      "antecedents: differential equation engineer, data science math skill\n",
      "image video processing mar hollywood stop hospital, precalculus relation function\n",
      "consequents: image video processing mar hollywood stop hospital, precalculus relation function \n",
      " ------------------------------\n",
      "378\n",
      "antecedents: image video processing mar hollywood stop hospital, precalculus relation function\n",
      "differential equation engineer, data science math skill\n",
      "consequents: differential equation engineer, data science math skill \n",
      " ------------------------------\n",
      "379\n",
      "antecedents: image video processing mar hollywood stop hospital, data science math skill\n",
      "differential equation engineer, precalculus relation function\n",
      "consequents: differential equation engineer, precalculus relation function \n",
      " ------------------------------\n",
      "380\n",
      "antecedents: precalculus relation function, data science math skill\n",
      "differential equation engineer, image video processing mar hollywood stop hospital\n",
      "consequents: differential equation engineer, image video processing mar hollywood stop hospital \n",
      " ------------------------------\n",
      "381\n",
      "antecedents: differential equation engineer\n",
      "image video processing mar hollywood stop hospital, precalculus relation function, data science math skill\n",
      "consequents: image video processing mar hollywood stop hospital, precalculus relation function, data science math skill \n",
      " ------------------------------\n",
      "382\n",
      "antecedents: image video processing mar hollywood stop hospital\n",
      "differential equation engineer, precalculus relation function, data science math skill\n",
      "consequents: differential equation engineer, precalculus relation function, data science math skill \n",
      " ------------------------------\n",
      "383\n",
      "antecedents: precalculus relation function\n",
      "differential equation engineer, image video processing mar hollywood stop hospital, data science math skill\n",
      "consequents: differential equation engineer, image video processing mar hollywood stop hospital, data science math skill \n",
      " ------------------------------\n",
      "384\n",
      "antecedents: data science math skill\n",
      "differential equation engineer, image video processing mar hollywood stop hospital, precalculus relation function\n",
      "consequents: differential equation engineer, image video processing mar hollywood stop hospital, precalculus relation function \n",
      " ------------------------------\n",
      "385\n",
      "antecedents: introduction complex analysis, game without chance combinatorial game theory, fibonacci number golden ratio\n",
      "mathematical biostatistic boot camp\n",
      "consequents: mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "386\n",
      "antecedents: introduction complex analysis, game without chance combinatorial game theory, mathematical biostatistic boot camp\n",
      "fibonacci number golden ratio\n",
      "consequents: fibonacci number golden ratio \n",
      " ------------------------------\n",
      "387\n",
      "antecedents: introduction complex analysis, fibonacci number golden ratio, mathematical biostatistic boot camp\n",
      "game without chance combinatorial game theory\n",
      "consequents: game without chance combinatorial game theory \n",
      " ------------------------------\n",
      "388\n",
      "antecedents: game without chance combinatorial game theory, fibonacci number golden ratio, mathematical biostatistic boot camp\n",
      "introduction complex analysis\n",
      "consequents: introduction complex analysis \n",
      " ------------------------------\n",
      "389\n",
      "antecedents: introduction complex analysis, game without chance combinatorial game theory\n",
      "fibonacci number golden ratio, mathematical biostatistic boot camp\n",
      "consequents: fibonacci number golden ratio, mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "390\n",
      "antecedents: introduction complex analysis, fibonacci number golden ratio\n",
      "game without chance combinatorial game theory, mathematical biostatistic boot camp\n",
      "consequents: game without chance combinatorial game theory, mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "391\n",
      "antecedents: introduction complex analysis, mathematical biostatistic boot camp\n",
      "game without chance combinatorial game theory, fibonacci number golden ratio\n",
      "consequents: game without chance combinatorial game theory, fibonacci number golden ratio \n",
      " ------------------------------\n",
      "392\n",
      "antecedents: game without chance combinatorial game theory, fibonacci number golden ratio\n",
      "introduction complex analysis, mathematical biostatistic boot camp\n",
      "consequents: introduction complex analysis, mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "393\n",
      "antecedents: game without chance combinatorial game theory, mathematical biostatistic boot camp\n",
      "introduction complex analysis, fibonacci number golden ratio\n",
      "consequents: introduction complex analysis, fibonacci number golden ratio \n",
      " ------------------------------\n",
      "394\n",
      "antecedents: fibonacci number golden ratio, mathematical biostatistic boot camp\n",
      "introduction complex analysis, game without chance combinatorial game theory\n",
      "consequents: introduction complex analysis, game without chance combinatorial game theory \n",
      " ------------------------------\n",
      "395\n",
      "antecedents: introduction complex analysis\n",
      "game without chance combinatorial game theory, fibonacci number golden ratio, mathematical biostatistic boot camp\n",
      "consequents: game without chance combinatorial game theory, fibonacci number golden ratio, mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "396\n",
      "antecedents: game without chance combinatorial game theory\n",
      "introduction complex analysis, fibonacci number golden ratio, mathematical biostatistic boot camp\n",
      "consequents: introduction complex analysis, fibonacci number golden ratio, mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "397\n",
      "antecedents: fibonacci number golden ratio\n",
      "introduction complex analysis, game without chance combinatorial game theory, mathematical biostatistic boot camp\n",
      "consequents: introduction complex analysis, game without chance combinatorial game theory, mathematical biostatistic boot camp \n",
      " ------------------------------\n",
      "398\n",
      "antecedents: mathematical biostatistic boot camp\n",
      "introduction complex analysis, game without chance combinatorial game theory, fibonacci number golden ratio\n",
      "consequents: introduction complex analysis, game without chance combinatorial game theory, fibonacci number golden ratio \n",
      " ------------------------------\n",
      "399\n",
      "antecedents: introduction calculus, vector calculus engineer, matrix algebra engineer\n",
      "analytic combinatoric\n",
      "consequents: analytic combinatoric \n",
      " ------------------------------\n",
      "400\n",
      "antecedents: analytic combinatoric, introduction calculus, matrix algebra engineer\n",
      "vector calculus engineer\n",
      "consequents: vector calculus engineer \n",
      " ------------------------------\n",
      "401\n",
      "antecedents: analytic combinatoric, vector calculus engineer, matrix algebra engineer\n",
      "introduction calculus\n",
      "consequents: introduction calculus \n",
      " ------------------------------\n",
      "402\n",
      "antecedents: introduction calculus, vector calculus engineer, analytic combinatoric\n",
      "matrix algebra engineer\n",
      "consequents: matrix algebra engineer \n",
      " ------------------------------\n",
      "403\n",
      "antecedents: introduction calculus, matrix algebra engineer\n",
      "vector calculus engineer, analytic combinatoric\n",
      "consequents: vector calculus engineer, analytic combinatoric \n",
      " ------------------------------\n",
      "404\n",
      "antecedents: vector calculus engineer, matrix algebra engineer\n",
      "introduction calculus, analytic combinatoric\n",
      "consequents: introduction calculus, analytic combinatoric \n",
      " ------------------------------\n",
      "405\n",
      "antecedents: analytic combinatoric, matrix algebra engineer\n",
      "introduction calculus, vector calculus engineer\n",
      "consequents: introduction calculus, vector calculus engineer \n",
      " ------------------------------\n",
      "406\n",
      "antecedents: introduction calculus, vector calculus engineer\n",
      "analytic combinatoric, matrix algebra engineer\n",
      "consequents: analytic combinatoric, matrix algebra engineer \n",
      " ------------------------------\n",
      "407\n",
      "antecedents: introduction calculus, analytic combinatoric\n",
      "vector calculus engineer, matrix algebra engineer\n",
      "consequents: vector calculus engineer, matrix algebra engineer \n",
      " ------------------------------\n",
      "408\n",
      "antecedents: vector calculus engineer, analytic combinatoric\n",
      "introduction calculus, matrix algebra engineer\n",
      "consequents: introduction calculus, matrix algebra engineer \n",
      " ------------------------------\n",
      "409\n",
      "antecedents: matrix algebra engineer\n",
      "introduction calculus, vector calculus engineer, analytic combinatoric\n",
      "consequents: introduction calculus, vector calculus engineer, analytic combinatoric \n",
      " ------------------------------\n",
      "410\n",
      "antecedents: introduction calculus\n",
      "analytic combinatoric, vector calculus engineer, matrix algebra engineer\n",
      "consequents: analytic combinatoric, vector calculus engineer, matrix algebra engineer \n",
      " ------------------------------\n",
      "411\n",
      "antecedents: vector calculus engineer\n",
      "analytic combinatoric, introduction calculus, matrix algebra engineer\n",
      "consequents: analytic combinatoric, introduction calculus, matrix algebra engineer \n",
      " ------------------------------\n",
      "412\n",
      "antecedents: analytic combinatoric\n",
      "introduction calculus, vector calculus engineer, matrix algebra engineer\n",
      "consequents: introduction calculus, vector calculus engineer, matrix algebra engineer \n",
      " ------------------------------\n",
      "413\n",
      "antecedents: data analytics foundations accountancy ii, advanced business analytic capstone, datum analysis interpretation capstone\n",
      "big datum capstone project\n",
      "consequents: big datum capstone project \n",
      " ------------------------------\n",
      "414\n",
      "antecedents: data analytics foundations accountancy ii, advanced business analytic capstone, big datum capstone project\n",
      "datum analysis interpretation capstone\n",
      "consequents: datum analysis interpretation capstone \n",
      " ------------------------------\n",
      "415\n",
      "antecedents: data analytics foundations accountancy ii, big datum capstone project, datum analysis interpretation capstone\n",
      "advanced business analytic capstone\n",
      "consequents: advanced business analytic capstone \n",
      " ------------------------------\n",
      "416\n",
      "antecedents: advanced business analytic capstone, big datum capstone project, datum analysis interpretation capstone\n",
      "data analytics foundations accountancy ii\n",
      "consequents: data analytics foundations accountancy ii \n",
      " ------------------------------\n",
      "417\n",
      "antecedents: data analytics foundations accountancy ii, advanced business analytic capstone\n",
      "datum analysis interpretation capstone, big datum capstone project\n",
      "consequents: datum analysis interpretation capstone, big datum capstone project \n",
      " ------------------------------\n",
      "418\n",
      "antecedents: data analytics foundations accountancy ii, datum analysis interpretation capstone\n",
      "advanced business analytic capstone, big datum capstone project\n",
      "consequents: advanced business analytic capstone, big datum capstone project \n",
      " ------------------------------\n",
      "419\n",
      "antecedents: advanced business analytic capstone, datum analysis interpretation capstone\n",
      "data analytics foundations accountancy ii, big datum capstone project\n",
      "consequents: data analytics foundations accountancy ii, big datum capstone project \n",
      " ------------------------------\n",
      "420\n",
      "antecedents: advanced business analytic capstone, big datum capstone project\n",
      "data analytics foundations accountancy ii, datum analysis interpretation capstone\n",
      "consequents: data analytics foundations accountancy ii, datum analysis interpretation capstone \n",
      " ------------------------------\n",
      "421\n",
      "antecedents: datum analysis interpretation capstone, big datum capstone project\n",
      "data analytics foundations accountancy ii, advanced business analytic capstone\n",
      "consequents: data analytics foundations accountancy ii, advanced business analytic capstone \n",
      " ------------------------------\n",
      "422\n",
      "antecedents: advanced business analytic capstone\n",
      "data analytics foundations accountancy ii, big datum capstone project, datum analysis interpretation capstone\n",
      "consequents: data analytics foundations accountancy ii, big datum capstone project, datum analysis interpretation capstone \n",
      " ------------------------------\n",
      "423\n",
      "antecedents: datum analysis interpretation capstone\n",
      "data analytics foundations accountancy ii, advanced business analytic capstone, big datum capstone project\n",
      "consequents: data analytics foundations accountancy ii, advanced business analytic capstone, big datum capstone project \n",
      " ------------------------------\n",
      "424\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "425\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business, getting start tensorflow\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "426\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic, getting start tensorflow\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "427\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic, getting start tensorflow\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "428\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business\n",
      "increase real estate management profit harness data analytic, getting start tensorflow\n",
      "consequents: increase real estate management profit harness data analytic, getting start tensorflow \n",
      " ------------------------------\n",
      "429\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic\n",
      "datum modeling regression analysis business, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, getting start tensorflow \n",
      " ------------------------------\n",
      "430\n",
      "antecedents: data analytic lean six sigma, getting start tensorflow\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "431\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "data analytic lean six sigma, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, getting start tensorflow \n",
      " ------------------------------\n",
      "432\n",
      "antecedents: datum modeling regression analysis business, getting start tensorflow\n",
      "data analytic lean six sigma, increase real estate management profit harness data analytic\n",
      "consequents: data analytic lean six sigma, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "433\n",
      "antecedents: increase real estate management profit harness data analytic, getting start tensorflow\n",
      "data analytic lean six sigma, datum modeling regression analysis business\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business \n",
      " ------------------------------\n",
      "434\n",
      "antecedents: data analytic lean six sigma\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic, getting start tensorflow \n",
      " ------------------------------\n",
      "435\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "data analytic lean six sigma, datum modeling regression analysis business, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business, getting start tensorflow \n",
      " ------------------------------\n",
      "436\n",
      "antecedents: getting start tensorflow\n",
      "data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "437\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic, communicate datum science result\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "438\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic, getting start tensorflow\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "439\n",
      "antecedents: data analytic lean six sigma, communicate datum science result, getting start tensorflow\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "440\n",
      "antecedents: increase real estate management profit harness data analytic, communicate datum science result, getting start tensorflow\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "441\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic\n",
      "communicate datum science result, getting start tensorflow\n",
      "consequents: communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "442\n",
      "antecedents: data analytic lean six sigma, communicate datum science result\n",
      "increase real estate management profit harness data analytic, getting start tensorflow\n",
      "consequents: increase real estate management profit harness data analytic, getting start tensorflow \n",
      " ------------------------------\n",
      "443\n",
      "antecedents: data analytic lean six sigma, getting start tensorflow\n",
      "increase real estate management profit harness data analytic, communicate datum science result\n",
      "consequents: increase real estate management profit harness data analytic, communicate datum science result \n",
      " ------------------------------\n",
      "444\n",
      "antecedents: increase real estate management profit harness data analytic, communicate datum science result\n",
      "data analytic lean six sigma, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, getting start tensorflow \n",
      " ------------------------------\n",
      "445\n",
      "antecedents: increase real estate management profit harness data analytic, getting start tensorflow\n",
      "data analytic lean six sigma, communicate datum science result\n",
      "consequents: data analytic lean six sigma, communicate datum science result \n",
      " ------------------------------\n",
      "446\n",
      "antecedents: communicate datum science result, getting start tensorflow\n",
      "data analytic lean six sigma, increase real estate management profit harness data analytic\n",
      "consequents: data analytic lean six sigma, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "447\n",
      "antecedents: data analytic lean six sigma\n",
      "increase real estate management profit harness data analytic, communicate datum science result, getting start tensorflow\n",
      "consequents: increase real estate management profit harness data analytic, communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "448\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "data analytic lean six sigma, communicate datum science result, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "449\n",
      "antecedents: communicate datum science result\n",
      "data analytic lean six sigma, increase real estate management profit harness data analytic, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, increase real estate management profit harness data analytic, getting start tensorflow \n",
      " ------------------------------\n",
      "450\n",
      "antecedents: getting start tensorflow\n",
      "data analytic lean six sigma, increase real estate management profit harness data analytic, communicate datum science result\n",
      "consequents: data analytic lean six sigma, increase real estate management profit harness data analytic, communicate datum science result \n",
      " ------------------------------\n",
      "451\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "452\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business, communicate datum science result\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "453\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic, communicate datum science result\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "454\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "455\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business\n",
      "increase real estate management profit harness data analytic, communicate datum science result\n",
      "consequents: increase real estate management profit harness data analytic, communicate datum science result \n",
      " ------------------------------\n",
      "456\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic\n",
      "datum modeling regression analysis business, communicate datum science result\n",
      "consequents: datum modeling regression analysis business, communicate datum science result \n",
      " ------------------------------\n",
      "457\n",
      "antecedents: data analytic lean six sigma, communicate datum science result\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "458\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "data analytic lean six sigma, communicate datum science result\n",
      "consequents: data analytic lean six sigma, communicate datum science result \n",
      " ------------------------------\n",
      "459\n",
      "antecedents: datum modeling regression analysis business, communicate datum science result\n",
      "data analytic lean six sigma, increase real estate management profit harness data analytic\n",
      "consequents: data analytic lean six sigma, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "460\n",
      "antecedents: increase real estate management profit harness data analytic, communicate datum science result\n",
      "data analytic lean six sigma, datum modeling regression analysis business\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business \n",
      " ------------------------------\n",
      "461\n",
      "antecedents: data analytic lean six sigma\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result \n",
      " ------------------------------\n",
      "462\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "data analytic lean six sigma, datum modeling regression analysis business, communicate datum science result\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business, communicate datum science result \n",
      " ------------------------------\n",
      "463\n",
      "antecedents: communicate datum science result\n",
      "data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "464\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business, communicate datum science result\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "465\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business, getting start tensorflow\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "466\n",
      "antecedents: data analytic lean six sigma, communicate datum science result, getting start tensorflow\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "467\n",
      "antecedents: datum modeling regression analysis business, communicate datum science result, getting start tensorflow\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "468\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business\n",
      "communicate datum science result, getting start tensorflow\n",
      "consequents: communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "469\n",
      "antecedents: data analytic lean six sigma, communicate datum science result\n",
      "datum modeling regression analysis business, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, getting start tensorflow \n",
      " ------------------------------\n",
      "470\n",
      "antecedents: data analytic lean six sigma, getting start tensorflow\n",
      "datum modeling regression analysis business, communicate datum science result\n",
      "consequents: datum modeling regression analysis business, communicate datum science result \n",
      " ------------------------------\n",
      "471\n",
      "antecedents: datum modeling regression analysis business, communicate datum science result\n",
      "data analytic lean six sigma, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, getting start tensorflow \n",
      " ------------------------------\n",
      "472\n",
      "antecedents: datum modeling regression analysis business, getting start tensorflow\n",
      "data analytic lean six sigma, communicate datum science result\n",
      "consequents: data analytic lean six sigma, communicate datum science result \n",
      " ------------------------------\n",
      "473\n",
      "antecedents: communicate datum science result, getting start tensorflow\n",
      "data analytic lean six sigma, datum modeling regression analysis business\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business \n",
      " ------------------------------\n",
      "474\n",
      "antecedents: data analytic lean six sigma\n",
      "datum modeling regression analysis business, communicate datum science result, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "475\n",
      "antecedents: communicate datum science result\n",
      "data analytic lean six sigma, datum modeling regression analysis business, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business, getting start tensorflow \n",
      " ------------------------------\n",
      "476\n",
      "antecedents: getting start tensorflow\n",
      "data analytic lean six sigma, datum modeling regression analysis business, communicate datum science result\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business, communicate datum science result \n",
      " ------------------------------\n",
      "477\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "478\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic, getting start tensorflow\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "479\n",
      "antecedents: datum modeling regression analysis business, communicate datum science result, getting start tensorflow\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "480\n",
      "antecedents: increase real estate management profit harness data analytic, communicate datum science result, getting start tensorflow\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "481\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "communicate datum science result, getting start tensorflow\n",
      "consequents: communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "482\n",
      "antecedents: datum modeling regression analysis business, communicate datum science result\n",
      "increase real estate management profit harness data analytic, getting start tensorflow\n",
      "consequents: increase real estate management profit harness data analytic, getting start tensorflow \n",
      " ------------------------------\n",
      "483\n",
      "antecedents: datum modeling regression analysis business, getting start tensorflow\n",
      "increase real estate management profit harness data analytic, communicate datum science result\n",
      "consequents: increase real estate management profit harness data analytic, communicate datum science result \n",
      " ------------------------------\n",
      "484\n",
      "antecedents: increase real estate management profit harness data analytic, communicate datum science result\n",
      "datum modeling regression analysis business, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, getting start tensorflow \n",
      " ------------------------------\n",
      "485\n",
      "antecedents: increase real estate management profit harness data analytic, getting start tensorflow\n",
      "datum modeling regression analysis business, communicate datum science result\n",
      "consequents: datum modeling regression analysis business, communicate datum science result \n",
      " ------------------------------\n",
      "486\n",
      "antecedents: communicate datum science result, getting start tensorflow\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "487\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "datum modeling regression analysis business, communicate datum science result, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "488\n",
      "antecedents: communicate datum science result\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic, getting start tensorflow \n",
      " ------------------------------\n",
      "489\n",
      "antecedents: getting start tensorflow\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result \n",
      " ------------------------------\n",
      "490\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result\n",
      "getting start tensorflow\n",
      "consequents: getting start tensorflow \n",
      " ------------------------------\n",
      "491\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business, communicate datum science result, getting start tensorflow\n",
      "increase real estate management profit harness data analytic\n",
      "consequents: increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "492\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic, getting start tensorflow\n",
      "communicate datum science result\n",
      "consequents: communicate datum science result \n",
      " ------------------------------\n",
      "493\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic, communicate datum science result, getting start tensorflow\n",
      "datum modeling regression analysis business\n",
      "consequents: datum modeling regression analysis business \n",
      " ------------------------------\n",
      "494\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result, getting start tensorflow\n",
      "data analytic lean six sigma\n",
      "consequents: data analytic lean six sigma \n",
      " ------------------------------\n",
      "495\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business, communicate datum science result\n",
      "increase real estate management profit harness data analytic, getting start tensorflow\n",
      "consequents: increase real estate management profit harness data analytic, getting start tensorflow \n",
      " ------------------------------\n",
      "496\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "communicate datum science result, getting start tensorflow\n",
      "consequents: communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "497\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business, getting start tensorflow\n",
      "increase real estate management profit harness data analytic, communicate datum science result\n",
      "consequents: increase real estate management profit harness data analytic, communicate datum science result \n",
      " ------------------------------\n",
      "498\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic, communicate datum science result\n",
      "datum modeling regression analysis business, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, getting start tensorflow \n",
      " ------------------------------\n",
      "499\n",
      "antecedents: data analytic lean six sigma, communicate datum science result, getting start tensorflow\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "500\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic, getting start tensorflow\n",
      "datum modeling regression analysis business, communicate datum science result\n",
      "consequents: datum modeling regression analysis business, communicate datum science result \n",
      " ------------------------------\n",
      "501\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result\n",
      "data analytic lean six sigma, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, getting start tensorflow \n",
      " ------------------------------\n",
      "502\n",
      "antecedents: datum modeling regression analysis business, communicate datum science result, getting start tensorflow\n",
      "data analytic lean six sigma, increase real estate management profit harness data analytic\n",
      "consequents: data analytic lean six sigma, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "503\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic, getting start tensorflow\n",
      "data analytic lean six sigma, communicate datum science result\n",
      "consequents: data analytic lean six sigma, communicate datum science result \n",
      " ------------------------------\n",
      "504\n",
      "antecedents: increase real estate management profit harness data analytic, communicate datum science result, getting start tensorflow\n",
      "data analytic lean six sigma, datum modeling regression analysis business\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business \n",
      " ------------------------------\n",
      "505\n",
      "antecedents: data analytic lean six sigma, datum modeling regression analysis business\n",
      "increase real estate management profit harness data analytic, communicate datum science result, getting start tensorflow\n",
      "consequents: increase real estate management profit harness data analytic, communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "506\n",
      "antecedents: data analytic lean six sigma, communicate datum science result\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic, getting start tensorflow \n",
      " ------------------------------\n",
      "507\n",
      "antecedents: data analytic lean six sigma, increase real estate management profit harness data analytic\n",
      "datum modeling regression analysis business, communicate datum science result, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "508\n",
      "antecedents: data analytic lean six sigma, getting start tensorflow\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result \n",
      " ------------------------------\n",
      "509\n",
      "antecedents: datum modeling regression analysis business, communicate datum science result\n",
      "data analytic lean six sigma, increase real estate management profit harness data analytic, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, increase real estate management profit harness data analytic, getting start tensorflow \n",
      " ------------------------------\n",
      "510\n",
      "antecedents: datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "data analytic lean six sigma, communicate datum science result, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "511\n",
      "antecedents: datum modeling regression analysis business, getting start tensorflow\n",
      "data analytic lean six sigma, increase real estate management profit harness data analytic, communicate datum science result\n",
      "consequents: data analytic lean six sigma, increase real estate management profit harness data analytic, communicate datum science result \n",
      " ------------------------------\n",
      "512\n",
      "antecedents: increase real estate management profit harness data analytic, communicate datum science result\n",
      "data analytic lean six sigma, datum modeling regression analysis business, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business, getting start tensorflow \n",
      " ------------------------------\n",
      "513\n",
      "antecedents: communicate datum science result, getting start tensorflow\n",
      "data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic \n",
      " ------------------------------\n",
      "514\n",
      "antecedents: increase real estate management profit harness data analytic, getting start tensorflow\n",
      "data analytic lean six sigma, datum modeling regression analysis business, communicate datum science result\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business, communicate datum science result \n",
      " ------------------------------\n",
      "515\n",
      "antecedents: data analytic lean six sigma\n",
      "datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result, getting start tensorflow\n",
      "consequents: datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "516\n",
      "antecedents: communicate datum science result\n",
      "data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic, getting start tensorflow \n",
      " ------------------------------\n",
      "517\n",
      "antecedents: increase real estate management profit harness data analytic\n",
      "data analytic lean six sigma, datum modeling regression analysis business, communicate datum science result, getting start tensorflow\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business, communicate datum science result, getting start tensorflow \n",
      " ------------------------------\n",
      "518\n",
      "antecedents: getting start tensorflow\n",
      "data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result\n",
      "consequents: data analytic lean six sigma, datum modeling regression analysis business, increase real estate management profit harness data analytic, communicate datum science result \n",
      " ------------------------------\n"
     ]
    }
   ],
   "source": [
    "for n, antecedents in enumerate(df_ar['antecedents']):\n",
    "    if isinstance(antecedents, frozenset):\n",
    "        antecedents = list(antecedents)\n",
    "    antecedent_ids = ','.join(map(str, antecedents)).strip()  # Chuyển thành danh sách và tách các ID\n",
    "\n",
    "    antecedent_names = []\n",
    "    for antecedent_id in antecedent_ids.split(','):\n",
    "        antecedent_id = antecedent_id.strip()\n",
    "        result = course.loc[course['Course_id'] == int(antecedent_id)]['Course Name'].iloc[0]\n",
    "        antecedent_names.append(result)\n",
    "\n",
    "    antecedent_names_str = ', '.join(antecedent_names)\n",
    "\n",
    "    print(n)\n",
    "    print('antecedents:', antecedent_names_str)\n",
    "\n",
    "    consequents = df_ar['consequents'][n]\n",
    "    if isinstance(consequents, frozenset):\n",
    "        consequents = list(consequents)\n",
    "    consequent_ids = ','.join(map(str, consequents)).strip()  # Chuyển thành danh sách và tách các ID\n",
    "\n",
    "    consequent_names = []\n",
    "    for consequent_id in consequent_ids.split(','):\n",
    "        consequent_id = consequent_id.strip()\n",
    "        result = course.loc[course['Course_id'] == int(consequent_id)]['Course Name'].iloc[0]\n",
    "        consequent_names.append(result)\n",
    "\n",
    "    consequent_names_str = ', '.join(consequent_names)\n",
    "\n",
    "    print(consequent_names_str)\n",
    "    print('consequents:', consequent_names_str, '\\n', '-' * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 1109', ' 2497']\n",
      "Enrolled Courses:\n",
      "- interactive word embedding use word vec plotly (ID: 0966)\n",
      "- nlp twitter sentiment analysis (ID: 1677)\n",
      "- sentiment analysis deep learning use bert (ID: 1109)\n",
      "Recommend Courses:\n",
      "- transfer learn nlp tensorflow hub (ID: 2497)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1429: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n"
     ]
    }
   ],
   "source": [
    "def recommend_courses(enrolled_courses, df_ar, num_recommendations=5):\n",
    "    # Tạo một danh sách để lưu trữ các khoá học được đề xuất\n",
    "    recommended_courses = []\n",
    "\n",
    "    # Duyệt qua từng tập luật kết hợp trong df_ar\n",
    "    for index, row in df_ar.iterrows():\n",
    "        antecedents = row['antecedents']\n",
    "        consequents = row['consequents']\n",
    "\n",
    "        # Chuyển các ID trong antecedents thành các ID không có khoảng cách\n",
    "        antecedents_cleaned = [course_id.replace(\" \", \"\") for course_id in antecedents]\n",
    "\n",
    "        # Kiểm tra nếu có ít nhất một khoá học từ antecedents có trong danh sách enrolled_courses\n",
    "        if any(course_id in antecedents_cleaned for course_id in enrolled_courses):\n",
    "            # Lấy danh sách các khoá học trong consequents\n",
    "            recommended_courses.extend(consequents)\n",
    "\n",
    "    # Loại bỏ các khoá học đã đăng ký và lặp lại\n",
    "    recommended_courses = list(set(recommended_courses) - set(enrolled_courses))\n",
    "    print(recommended_courses)\n",
    "\n",
    "    # Chọn một số lượng giới hạn của khoá học để đề xuất\n",
    "    if len(recommended_courses) > num_recommendations:\n",
    "        recommended_courses = recommended_courses[:num_recommendations]\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "# Danh sách các khoá học mà người dùng đã đăng ký\n",
    "enrolled_courses = [\"0966\", \"1677\", \"1109\"]\n",
    "\n",
    "# Đề xuất các khoá học dựa trên danh sách đã đăng ký và bảng df_ar\n",
    "recommended_courses = recommend_courses(enrolled_courses, df_ar)\n",
    "course_dict = dict(zip(course['Course_id'], course['Course Name']))\n",
    "# In ra các khoá học được đề xuất\n",
    "print(\"Enrolled Courses:\")\n",
    "for course_id in enrolled_courses:\n",
    "    clean_course_id = course_id.replace(\" \", \"\")  # Loại bỏ dấu cách\n",
    "    course_name = course_dict.get(int(clean_course_id), 'Not Found')  # Lấy tên khoá học từ từ điển course_dict\n",
    "    print(f\"- {course_name} (ID: {clean_course_id})\")\n",
    "    \n",
    "# In ra các khoá học được đề xuất, nhưng chỉ hiển thị những khoá học chưa được đăng ký\n",
    "print(\"Recommend Courses:\")\n",
    "for course_id in recommended_courses:\n",
    "    clean_course_id = course_id.replace(\" \", \"\")  # Loại bỏ dấu cách\n",
    "    course_name = course_dict.get(int(clean_course_id), 'Not Found')  # Lấy tên khoá học từ từ điển course_dict\n",
    "    \n",
    "    # Kiểm tra xem khoá học đã được đăng ký hay chưa\n",
    "    if clean_course_id not in enrolled_courses:\n",
    "        print(f\"- {course_name} (ID: {clean_course_id})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"11\"></a>\n",
    "\n",
    "### 2. Similar Courses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.corpus import stopwords\n",
    "import en_core_web_sm\n",
    "spc_en = en_core_web_sm.load()\n",
    "import re\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess text\n",
    "# spc_en = en_core_web_sm.load()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stopwords_eng = stopwords.words(\"english\")\n",
    "    text = text.lower()\n",
    "    text = text.replace(\",\", \"\").replace(\".\", \"\").replace(\"!\", \"\").replace(\"?\", \"\")\n",
    "    text = re.sub(r\"[\\W\\d_]+\", \" \", text)\n",
    "    text = [pal for pal in text.split() if pal not in stopwords_eng]\n",
    "    spc_text = spc_en(\" \".join(text))\n",
    "    tokens = [word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in spc_text]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    # Chuyển đổi các vector cột thành vector hàng\n",
    "    vector1 = vector1.reshape(1, -1)\n",
    "    vector2 = vector2.reshape(1, -1)\n",
    "    \n",
    "    dot_product = np.dot(vector1, vector2.T)  # Sử dụng vector thứ hai chuyển vị\n",
    "    norm1 = np.linalg.norm(vector1)\n",
    "    norm2 = np.linalg.norm(vector2)\n",
    "    similarity = dot_product / (norm1 * norm2)\n",
    "    return similarity[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vector1, vector2):\n",
    "    # Tính hiệu của hai vector\n",
    "    diff = vector1 - vector2\n",
    "    \n",
    "    # Tính khoảng cách Euclidean bằng cách tính norm của hiệu\n",
    "    distance = np.linalg.norm(diff)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description:\n",
      "\n",
      "Linear regression, build an application\n",
      "\n",
      "\n",
      "Nearest Course:\n",
      "Neighbor 1: practical machine learning -- Specialized: Data Science\n",
      "Neighbor 2: practical machine learning -- Specialized: Data Science\n",
      "Neighbor 3: introduction programming swift -- Specialized: Computer Science\n",
      "Neighbor 4: apply social network analysis python -- Specialized: Data Science\n",
      "Neighbor 5: advance linear model datum science least square -- Specialized: Data Science\n"
     ]
    }
   ],
   "source": [
    "# Hàm mã hoá câu với BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def encode_sequence_with_bert(sequence):\n",
    "    # Tiền xử lý và mã hoá câu\n",
    "    input_ids = tokenizer(preprocess_text(sequence), return_tensors=\"pt\").input_ids\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        hidden_states = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "def nearest_bert(new_sentence):\n",
    "    # Tiền xử lý và mã hoá câu mới với BERT\n",
    "    encoded_new_sentence = encode_sequence_with_bert(new_sentence)\n",
    "\n",
    "    # Chuyển đổi chuỗi số thực từ cột BERT_Encoded thành ma trận 2D\n",
    "    bert_encoded_matrix = course['BERT_Encoded'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "    bert_encoded_matrix = np.vstack(bert_encoded_matrix)\n",
    "\n",
    "    # Tạo mô hình KNN với k=5 (tìm 5 câu giống nhất)\n",
    "    knn_model = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "    knn_model.fit(bert_encoded_matrix)\n",
    "\n",
    "    # Tìm 5 câu giống với câu mới nhất trong BERT_Encoded\n",
    "    nearest_neighbors = knn_model.kneighbors([encoded_new_sentence], n_neighbors=5)\n",
    "\n",
    "    print(\"Description:\")\n",
    "    print(new_sentence)\n",
    "    print(\"\\nNearest Course:\")\n",
    "    for i, neighbor_index in enumerate(nearest_neighbors[1][0]):\n",
    "        neighbor_sequence = course.at[neighbor_index, \"Course Name\"]\n",
    "        neighbor_specialized = course.at[neighbor_index, \"Specialized\"]\n",
    "\n",
    "        print(f\"Neighbor {i + 1}: {neighbor_sequence} -- Specialized: {neighbor_specialized}\")\n",
    "\n",
    "# Chuỗi câu mới\n",
    "new_sentence = \"\"\"\n",
    "Linear regression, build an application\n",
    "\"\"\"\n",
    "nearest_bert(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\algorithms.py:525: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  common = np.find_common_type([values.dtype, comps_array.dtype], [])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1482, 2733, 1630, 2012, 807, 615, 132, 2338, 1280]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1 = course[course[\"Course_id\"] == 4][\"IDF\"]\n",
    "vec1 = course[course[\"Course_id\"] == 4][\"IDF\"]\n",
    "\n",
    "# cosine_similarityfrom sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Đọc dữ liệu khoá học\n",
    "course = pd.read_csv(\"../data/course.csv\", index_col=0)\n",
    "\n",
    "# Load cột IDF đã mã hoá\n",
    "idf_encoded_matrix = course[\"BERT_Encoded\"].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "idf_encoded_matrix = np.vstack(idf_encoded_matrix)\n",
    "\n",
    "# Tạo mô hình KNN với k=5 (tìm 5 khoá gần nhất)\n",
    "knn_model = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "knn_model.fit(idf_encoded_matrix)\n",
    "\n",
    "def find_nearest_courses(course_id, num):\n",
    "    # Tìm vector IDF của khoá học cụ thể\n",
    "    course_index = course[course[\"Course_id\"] == course_id].index[0]\n",
    "    course_idf_vector = idf_encoded_matrix[course_index]\n",
    "\n",
    "    # Tìm 5 khoá gần nhất với khoá học cụ thể\n",
    "    nearest_neighbors = knn_model.kneighbors([course_idf_vector], n_neighbors=num)\n",
    "\n",
    "    # print(f\"Course ID: {course_id}\")\n",
    "    # print(\"\\nNearest Courses:\")\n",
    "    \n",
    "    recommended_course_ids = []\n",
    "    for i, neighbor_index in enumerate(nearest_neighbors[1][0]):\n",
    "        neighbor_course_id = course.at[neighbor_index, \"Course_id\"]\n",
    "        recommended_course_ids.append(neighbor_course_id)\n",
    "        neighbor_course_name = course.at[neighbor_index, \"Course Name\"]\n",
    "        # print(f\"Neighbor {i + 1}: Course ID {neighbor_course_id} -- {neighbor_course_name}\")\n",
    "\n",
    "    return recommended_course_ids\n",
    "find_nearest_courses(1,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1429: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\Sysme Hue\\AppData\\Local\\Temp\\ipykernel_16968\\3869517720.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">40</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: 'C:\\\\Users\\\\Sysme </span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">Hue\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_16968\\\\3869517720.py'</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\Sysme Hue\\AppData\\Local\\Temp\\ipykernel_16968\\3869517720.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">20</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">recommend_courses</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: 'C:\\\\Users\\\\Sysme </span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">Hue\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_16968\\\\3869517720.py'</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>unsupported operand <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">type</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span> for -: <span style=\"color: #008000; text-decoration-color: #008000\">'set'</span> and <span style=\"color: #008000; text-decoration-color: #008000\">'list'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\Sysme Hue\\AppData\\Local\\Temp\\ipykernel_16968\\3869517720.py\u001b[0m:\u001b[94m40\u001b[0m in \u001b[92m<module>\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: 'C:\\\\Users\\\\Sysme \u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31mHue\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_16968\\\\3869517720.py'\u001b[0m                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\Sysme Hue\\AppData\\Local\\Temp\\ipykernel_16968\\3869517720.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92mrecommend_courses\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: 'C:\\\\Users\\\\Sysme \u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31mHue\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_16968\\\\3869517720.py'\u001b[0m                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0munsupported operand \u001b[1;35mtype\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m for -: \u001b[32m'set'\u001b[0m and \u001b[32m'list'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def recommend_courses(enrolled_courses, df_ar, num_recommendations=5):\n",
    "    # Tạo một danh sách để lưu trữ các khoá học được đề xuất\n",
    "    recommended_courses = []\n",
    "\n",
    "    # Duyệt qua từng tập luật kết hợp trong df_ar\n",
    "\n",
    "    for index, row in df_ar.iterrows():\n",
    "        antecedents = row['antecedents']\n",
    "        consequents = row['consequents']\n",
    "\n",
    "        # Chuyển các ID trong antecedents thành các ID không có khoảng cách và thành chuỗi\n",
    "        antecedents_cleaned = [str(course_id).replace(\" \", \"\") for course_id in antecedents]\n",
    "\n",
    "        # Kiểm tra nếu có ít nhất một khoá học từ antecedents có trong danh sách enrolled_courses\n",
    "        if any(course_id in antecedents_cleaned for course_id in enrolled_courses):\n",
    "            # Lấy danh sách các khoá học trong consequents\n",
    "            recommended_courses.extend(consequents)\n",
    "\n",
    "    # Loại bỏ các khoá học đã đăng ký và lặp lại\n",
    "    recommended_courses = list(set(recommended_courses) - set(enrolled_courses))\n",
    "\n",
    "    # Kiểm tra số lượng khoá học được đề xuất, nếu ít hơn 5, tìm thêm các khoá học gần nhất\n",
    "    if len(recommended_courses) < num_recommendations and  len(recommended_courses) != 0 :\n",
    "\n",
    "        num_to_find = (num_recommendations - len(recommended_courses)) / len(recommended_courses)\n",
    "        \n",
    "        for course_id in enrolled_courses:\n",
    "            recommended_courses.extend(find_nearest_courses(int(course_id), int(num_to_find)))\n",
    "        \n",
    "    # Loại bỏ các khoá học trùng lặp (nếu có)\n",
    "    recommended_courses = list(set(recommended_courses))\n",
    "    # Chọn một số lượng giới hạn của khoá học để đề xuất\n",
    "    if len(recommended_courses) > num_recommendations:\n",
    "        recommended_courses = recommended_courses[:num_recommendations]\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "\n",
    "# Sử dụng hàm để đề xuất khoá học\n",
    "enrolled_courses = [\"0966\", \"1677\", \"1109\"]\n",
    "recommended_courses = recommend_courses(enrolled_courses, df_ar)\n",
    "\n",
    "course_dict = dict(zip(course['Course_id'], course['Course Name']))\n",
    "# In ra các khoá học đã đăng ký\n",
    "print(\"Enrolled Courses:\")\n",
    "for course_id in enrolled_courses:\n",
    "    clean_course_id = course_id.replace(\" \", \"\")\n",
    "    course_name = course_dict.get(int(clean_course_id), 'Not Found')\n",
    "    print(f\"- {course_name} (ID: {clean_course_id})\")\n",
    "\n",
    "# In ra các khoá học được đề xuất\n",
    "print(\"Recommended Courses:\")\n",
    "for course_id in recommended_courses:\n",
    "    clean_course_id = str(course_id).replace(\" \", \"\")\n",
    "    course_name = course_dict.get(int(clean_course_id), 'Not Found')\n",
    "    print(f\"- {course_name} (ID: {clean_course_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enrolled Courses:\n",
      "- interactive word embedding use word vec plotly (ID: 0966)\n",
      "- nlp twitter sentiment analysis (ID: 1677)\n",
      "- sentiment analysis deep learning use bert (ID: 1109)\n",
      "Recommended Courses:\n",
      "- transfer learn nlp tensorflow hub (ID: 2497)\n",
      "- optimize tensorflow model deployment tensorrt (ID: 520)\n",
      "- simple recurrent neural network kera (ID: 817)\n",
      "- neural network scratch tensorflow (ID: 3370)\n",
      "- neural network visualizer web app python (ID: 2546)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sysme Hue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1429: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n"
     ]
    }
   ],
   "source": [
    "def recommend_courses(enrolled_courses, df_ar, num_recommendations=5):\n",
    "\n",
    "    recommended_courses = []\n",
    "\n",
    "    for index, row in df_ar.iterrows():\n",
    "        antecedents = row['antecedents']\n",
    "        consequents = row['consequents']\n",
    "\n",
    "        antecedents_cleaned = [str(course_id).replace(\" \", \"\") for course_id in antecedents]\n",
    "\n",
    "        if any(course_id in antecedents_cleaned for course_id in enrolled_courses):\n",
    "            recommended_courses.extend(consequents)\n",
    "\n",
    "    recommended_courses = [str(course_id).replace(\" \", \"\") for course_id in recommended_courses]\n",
    "    recommended_courses = list(set(recommended_courses) - set(enrolled_courses))\n",
    "\n",
    "    if len(recommended_courses) > num_recommendations:\n",
    "        recommended_courses = recommended_courses[:num_recommendations]\n",
    "    if len(recommended_courses) < num_recommendations:\n",
    "        \n",
    "        num_to_find = (num_recommendations - len(recommended_courses)) / (len(recommended_courses)-0.1)\n",
    "        re = []\n",
    "        for course_id in recommended_courses:\n",
    "            nearest_courses = find_nearest_courses(int(course_id), 5)\n",
    "            for i,course in enumerate(nearest_courses):\n",
    "                if course not in enrolled_courses and course not in recommended_courses and int(course) != int(course_id):\n",
    "                    re.append(course)\n",
    "                \n",
    "                if len(re) + 1 >= round(num_to_find)*(i+1):\n",
    "                    break\n",
    "\n",
    "    # recommended_courses = list(set(recommended_courses) - set(enrolled_courses) - set(re))\n",
    "    [recommended_courses.append(i)for i in set(re)]\n",
    "\n",
    "    # if len(recommended_courses) > num_recommendations:\n",
    "    #     recommended_courses = recommended_courses[:num_recommendations]\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "# Sử dụng hàm để đề xuất khoá học\n",
    "enrolled_courses = [\"0966\", \"1677\", \"1109\"]\n",
    "recommended_courses = recommend_courses(enrolled_courses, df_ar)\n",
    "\n",
    "course_dict = dict(zip(course['Course_id'], course['Course Name']))\n",
    "\n",
    "print(\"Enrolled Courses:\")\n",
    "for course_id in enrolled_courses:\n",
    "    clean_course_id = course_id.replace(\" \", \"\")\n",
    "    course_name = course_dict.get(int(clean_course_id), 'Not Found')\n",
    "    print(f\"- {course_name} (ID: {clean_course_id})\")\n",
    "\n",
    "print(\"Recommended Courses:\")\n",
    "for course_id in recommended_courses:\n",
    "    clean_course_id = str(course_id).replace(\" \", \"\")\n",
    "    course_name = course_dict.get(int(clean_course_id), 'Not Found')\n",
    "    print(f\"- {course_name} (ID: {clean_course_id})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1.5789473684210527)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
